Will train until validation_0-rmse hasn't improved in 10 rounds.
0	validation_0-rmse:34.38540
1	validation_0-rmse:34.28400
2	validation_0-rmse:34.19199
3	validation_0-rmse:34.09919
4	validation_0-rmse:34.01232
5	validation_0-rmse:33.92428
6	validation_0-rmse:33.84188
7	validation_0-rmse:33.75843
8	validation_0-rmse:33.68066
9	validation_0-rmse:33.60122
10	validation_0-rmse:33.52799
11	validation_0-rmse:33.45756
12	validation_0-rmse:33.38486
13	validation_0-rmse:33.31856
14	validation_0-rmse:33.25023
15	validation_0-rmse:33.18752
16	validation_0-rmse:33.12247
17	validation_0-rmse:33.06012
18	validation_0-rmse:33.00254
19	validation_0-rmse:32.94403
20	validation_0-rmse:32.88702
21	validation_0-rmse:32.83246
22	validation_0-rmse:32.78051
23	validation_0-rmse:32.72940
24	validation_0-rmse:32.67799
25	validation_0-rmse:32.63149
26	validation_0-rmse:32.58599
27	validation_0-rmse:32.53943
28	validation_0-rmse:32.49748
29	validation_0-rmse:32.45915
30	validation_0-rmse:32.41734
31	validation_0-rmse:32.38065
32	validation_0-rmse:32.34719
33	validation_0-rmse:32.30830
34	validation_0-rmse:32.27330
35	validation_0-rmse:32.23529
36	validation_0-rmse:32.20151
37	validation_0-rmse:32.16605
38	validation_0-rmse:32.13392
39	validation_0-rmse:32.10813
40	validation_0-rmse:32.07876
41	validation_0-rmse:32.05209
42	validation_0-rmse:32.02651
43	validation_0-rmse:32.00057
44	validation_0-rmse:31.97440
45	validation_0-rmse:31.95210
46	validation_0-rmse:31.92955
47	validation_0-rmse:31.91068
48	validation_0-rmse:31.89299
49	validation_0-rmse:31.87504
50	validation_0-rmse:31.85961
51	validation_0-rmse:31.84388
52	validation_0-rmse:31.81785
53	validation_0-rmse:31.80522
54	validation_0-rmse:31.79362
55	validation_0-rmse:31.78337
56	validation_0-rmse:31.75966
57	validation_0-rmse:31.75249
58	validation_0-rmse:31.74023
59	validation_0-rmse:31.73371
60	validation_0-rmse:31.72188
61	validation_0-rmse:31.70143
62	validation_0-rmse:31.70197
63	validation_0-rmse:31.68152
64	validation_0-rmse:31.67250
65	validation_0-rmse:31.67017
66	validation_0-rmse:31.66978
67	validation_0-rmse:31.67370
68	validation_0-rmse:31.67350
69	validation_0-rmse:31.66450
70	validation_0-rmse:31.67043
71	validation_0-rmse:31.66800
72	validation_0-rmse:31.65355
73	validation_0-rmse:31.66163
74	validation_0-rmse:31.65665
75	validation_0-rmse:31.66075
76	validation_0-rmse:31.65614
77	validation_0-rmse:31.66580
78	validation_0-rmse:31.67264
79	validation_0-rmse:31.66851
80	validation_0-rmse:31.67972
81	validation_0-rmse:31.68790
Stopping. Best iteration:
[72]	validation_0-rmse:31.65355
====================2 model trained====================
==================== fold_2 ====================
rmse: 31.653547269556327
/var/folders/9l/qzgwttq17ps8lhfhrtgpdkx00000gn/T/ipykernel_39008/538381536.py:25: UserWarning:
wandb_callback will be deprecated in favor of WandbCallback. Please use WandbCallback for more features.
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning:
Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning:
ntree_limit is deprecated, use `iteration_range` or model slicing instead.