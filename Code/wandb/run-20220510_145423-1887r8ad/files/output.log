==================== fold_0 Training ====================
Epoch 1/100
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d5c85ee0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d5c85ee0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - ETA: 0s - loss: 14.4351 - val_loss: 14.5333
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
110/110 [==============================] - ETA: 0s - loss: 14.4351 - val_loss: 14.5333WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d5c76670> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d5c76670> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 2s 12ms/step - loss: 14.4351 - val_loss: 11.2105 - val_val_loss: 11.1835 - _timestamp: 1652165669.0000 - _runtime: 6.0000
Epoch 2/100
110/110 [==============================] - 1s 8ms/step - loss: 13.1113 - val_loss: 13.0427 - _timestamp: 1652165670.0000 - _runtime: 7.0000
Epoch 3/100
110/110 [==============================] - 1s 7ms/step - loss: 13.0159 - val_loss: 12.9421 - _timestamp: 1652165671.0000 - _runtime: 8.0000
Epoch 4/100
 15/110 [===>..........................] - ETA: 0s - loss: 14.3319 - val_loss: 14.3319
110/110 [==============================] - 1s 8ms/step - loss: 12.9181 - val_loss: 12.9177 - _timestamp: 1652165671.0000 - _runtime: 8.0000
Epoch 5/100
110/110 [==============================] - 1s 7ms/step - loss: 12.8055 - val_loss: 12.7110 - _timestamp: 1652165672.0000 - _runtime: 9.0000
Epoch 6/100
110/110 [==============================] - 1s 7ms/step - loss: 12.7364 - val_loss: 12.6474 - _timestamp: 1652165673.0000 - _runtime: 10.0000
Epoch 7/100
110/110 [==============================] - 1s 8ms/step - loss: 12.6026 - val_loss: 12.6675 - _timestamp: 1652165674.0000 - _runtime: 11.0000
Epoch 8/100
110/110 [==============================] - 1s 8ms/step - loss: 12.5434 - val_loss: 12.4794 - _timestamp: 1652165675.0000 - _runtime: 12.0000
Epoch 9/100
110/110 [==============================] - 1s 7ms/step - loss: 12.4855 - val_loss: 12.5585 - _timestamp: 1652165676.0000 - _runtime: 13.0000
Epoch 10/100
110/110 [==============================] - 1s 8ms/step - loss: 12.4530 - val_loss: 12.4038 - _timestamp: 1652165676.0000 - _runtime: 13.0000
Epoch 11/100
110/110 [==============================] - 1s 8ms/step - loss: 12.4553 - val_loss: 12.3978 - _timestamp: 1652165677.0000 - _runtime: 14.0000
Epoch 12/100
110/110 [==============================] - 1s 8ms/step - loss: 12.5168 - val_loss: 12.4155 - _timestamp: 1652165678.0000 - _runtime: 15.0000
Epoch 13/100
110/110 [==============================] - 1s 7ms/step - loss: 12.4216 - val_loss: 12.3316 - _timestamp: 1652165679.0000 - _runtime: 16.0000
Epoch 14/100
110/110 [==============================] - 1s 8ms/step - loss: 12.3736 - val_loss: 12.3081 - _timestamp: 1652165680.0000 - _runtime: 17.0000
Epoch 15/100
110/110 [==============================] - 1s 7ms/step - loss: 12.3695 - val_loss: 12.3305 - _timestamp: 1652165681.0000 - _runtime: 18.0000
Epoch 16/100
110/110 [==============================] - 1s 7ms/step - loss: 12.3282 - val_loss: 12.2532 - _timestamp: 1652165681.0000 - _runtime: 18.0000
Epoch 17/100
110/110 [==============================] - 1s 8ms/step - loss: 12.3203 - val_loss: 12.2830 - _timestamp: 1652165682.0000 - _runtime: 19.0000
Epoch 18/100
110/110 [==============================] - 1s 8ms/step - loss: 12.3682 - val_loss: 12.3125 - _timestamp: 1652165683.0000 - _runtime: 20.0000
Epoch 19/100
110/110 [==============================] - 1s 7ms/step - loss: 12.2905 - val_loss: 12.2317 - _timestamp: 1652165684.0000 - _runtime: 21.0000
Epoch 20/100
110/110 [==============================] - 1s 7ms/step - loss: 12.2871 - val_loss: 14.0918 - _timestamp: 1652165685.0000 - _runtime: 22.0000
Epoch 21/100
110/110 [==============================] - 1s 8ms/step - loss: 12.2651 - val_loss: 12.1693 - _timestamp: 1652165686.0000 - _runtime: 23.0000
Epoch 22/100
110/110 [==============================] - 1s 8ms/step - loss: 12.2363 - val_loss: 12.1645 - _timestamp: 1652165686.0000 - _runtime: 23.0000
Epoch 23/100
110/110 [==============================] - 1s 7ms/step - loss: 12.1655 - val_loss: 12.1305 - _timestamp: 1652165687.0000 - _runtime: 24.0000
Epoch 24/100
110/110 [==============================] - 1s 8ms/step - loss: 12.1331 - val_loss: 12.2437 - _timestamp: 1652165688.0000 - _runtime: 25.0000
Epoch 25/100
110/110 [==============================] - 1s 8ms/step - loss: 12.1315 - val_loss: 12.1986 - _timestamp: 1652165689.0000 - _runtime: 26.0000
Epoch 26/100
110/110 [==============================] - 1s 8ms/step - loss: 12.1559 - val_loss: 12.1092 - _timestamp: 1652165690.0000 - _runtime: 27.0000
WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d5bfb550> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d5bfb550> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
==================== fold_0 score ====================
rmse: 32.684093585276294
2022-05-10 14:54:50.564030: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.