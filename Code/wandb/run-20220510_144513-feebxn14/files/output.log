==================== fold_0 Training ====================
Epoch 1/100
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf3d9ee0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf3d9ee0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 37/110 [=========>....................] - ETA: 1s - loss: 11.4612 - val_loss: 11.4612
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
110/110 [==============================] - ETA: 0s - loss: 12.0322 - val_loss: 13.4449WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cfd7f8b0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cfd7f8b0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 3s 19ms/step - loss: 12.0322 - val_loss: 11.2048 - val_val_loss: 11.1579 - _timestamp: 1652165119.0000 - _runtime: 6.0000
Epoch 2/100
 51/110 [============>.................] - ETA: 0s - loss: 11.7225 - val_loss: 11.7225
110/110 [==============================] - 1s 13ms/step - loss: 11.5845 - val_loss: 11.5149 - _timestamp: 1652165121.0000 - _runtime: 8.0000
Epoch 3/100
110/110 [==============================] - 1s 13ms/step - loss: 11.1806 - val_loss: 11.1340 - _timestamp: 1652165122.0000 - _runtime: 9.0000
Epoch 4/100
110/110 [==============================] - 1s 13ms/step - loss: 11.3448 - val_loss: 11.3547 - _timestamp: 1652165123.0000 - _runtime: 10.0000
Epoch 5/100
110/110 [==============================] - 1s 13ms/step - loss: 11.0935 - val_loss: 11.0609 - _timestamp: 1652165125.0000 - _runtime: 12.0000
Epoch 6/100
110/110 [==============================] - 1s 13ms/step - loss: 11.2982 - val_loss: 11.2059 - _timestamp: 1652165126.0000 - _runtime: 13.0000
Epoch 7/100
110/110 [==============================] - 1s 13ms/step - loss: 11.2336 - val_loss: 11.1347 - _timestamp: 1652165128.0000 - _runtime: 15.0000
Epoch 8/100
110/110 [==============================] - 1s 13ms/step - loss: 11.1545 - val_loss: 11.2008 - _timestamp: 1652165129.0000 - _runtime: 16.0000
Epoch 9/100
110/110 [==============================] - 2s 14ms/step - loss: 11.1041 - val_loss: 11.0178 - _timestamp: 1652165131.0000 - _runtime: 18.0000
Epoch 10/100
110/110 [==============================] - 1s 13ms/step - loss: 11.0718 - val_loss: 10.9833 - _timestamp: 1652165132.0000 - _runtime: 19.0000
Epoch 11/100
110/110 [==============================] - 1s 13ms/step - loss: 11.0501 - val_loss: 11.0316 - _timestamp: 1652165134.0000 - _runtime: 21.0000
Epoch 12/100
110/110 [==============================] - 1s 13ms/step - loss: 11.2419 - val_loss: 11.1596 - _timestamp: 1652165135.0000 - _runtime: 22.0000
Epoch 13/100
110/110 [==============================] - 1s 13ms/step - loss: 11.3094 - val_loss: 11.2267 - _timestamp: 1652165136.0000 - _runtime: 23.0000
Epoch 14/100
110/110 [==============================] - 1s 13ms/step - loss: 10.9026 - val_loss: 10.8125 - _timestamp: 1652165138.0000 - _runtime: 25.0000
Epoch 15/100
110/110 [==============================] - 1s 12ms/step - loss: 11.1861 - val_loss: 11.0947 - _timestamp: 1652165139.0000 - _runtime: 26.0000
Epoch 16/100
110/110 [==============================] - 1s 13ms/step - loss: 11.0693 - val_loss: 10.9788 - _timestamp: 1652165141.0000 - _runtime: 28.0000
Epoch 17/100
110/110 [==============================] - 1s 12ms/step - loss: 11.0669 - val_loss: 11.1098 - _timestamp: 1652165142.0000 - _runtime: 29.0000
Epoch 18/100
110/110 [==============================] - 1s 13ms/step - loss: 12.3500 - val_loss: 12.2645 - _timestamp: 1652165144.0000 - _runtime: 31.0000
Epoch 19/100
110/110 [==============================] - 1s 12ms/step - loss: 11.5463 - val_loss: 11.5135 - _timestamp: 1652165145.0000 - _runtime: 32.0000
Epoch 20/100
110/110 [==============================] - 1s 13ms/step - loss: 14.6377 - val_loss: 14.6407 - _timestamp: 1652165146.0000 - _runtime: 33.0000
Epoch 21/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0293 - val_loss: 14.1364 - _timestamp: 1652165148.0000 - _runtime: 35.0000
Epoch 22/100
110/110 [==============================] - 1s 13ms/step - loss: 13.9586 - val_loss: 13.8696 - _timestamp: 1652165149.0000 - _runtime: 36.0000
Epoch 23/100
110/110 [==============================] - 1s 12ms/step - loss: 13.7605 - val_loss: 13.6684 - _timestamp: 1652165150.0000 - _runtime: 37.0000
Epoch 24/100
110/110 [==============================] - 1s 12ms/step - loss: 15.2371 - val_loss: 15.1034 - _timestamp: 1652165152.0000 - _runtime: 39.0000
Epoch 25/100
110/110 [==============================] - 1s 13ms/step - loss: 14.0144 - val_loss: 13.9202 - _timestamp: 1652165153.0000 - _runtime: 40.0000
Epoch 26/100
110/110 [==============================] - 1s 13ms/step - loss: 14.0158 - val_loss: 13.9261 - _timestamp: 1652165155.0000 - _runtime: 42.0000
Epoch 27/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0110 - val_loss: 13.9225 - _timestamp: 1652165156.0000 - _runtime: 43.0000
Epoch 28/100
110/110 [==============================] - 1s 13ms/step - loss: 14.0137 - val_loss: 14.2880 - _timestamp: 1652165157.0000 - _runtime: 44.0000
Epoch 29/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0205 - val_loss: 13.9302 - _timestamp: 1652165159.0000 - _runtime: 46.0000
Epoch 30/100
110/110 [==============================] - 1s 13ms/step - loss: 14.0110 - val_loss: 13.8903 - _timestamp: 1652165160.0000 - _runtime: 47.0000
Epoch 31/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0171 - val_loss: 13.9185 - _timestamp: 1652165161.0000 - _runtime: 48.0000
Epoch 32/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0147 - val_loss: 13.9110 - _timestamp: 1652165163.0000 - _runtime: 50.0000
Epoch 33/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0272 - val_loss: 13.9869 - _timestamp: 1652165164.0000 - _runtime: 51.0000
Epoch 34/100
110/110 [==============================] - 1s 13ms/step - loss: 14.0098 - val_loss: 13.8991 - _timestamp: 1652165166.0000 - _runtime: 53.0000
Epoch 35/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0123 - val_loss: 14.2762 - _timestamp: 1652165167.0000 - _runtime: 54.0000
Epoch 36/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0145 - val_loss: 14.0264 - _timestamp: 1652165168.0000 - _runtime: 55.0000
Epoch 37/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0132 - val_loss: 13.9007 - _timestamp: 1652165170.0000 - _runtime: 57.0000
Epoch 38/100
110/110 [==============================] - 1s 13ms/step - loss: 14.0096 - val_loss: 14.1516 - _timestamp: 1652165171.0000 - _runtime: 58.0000
Epoch 39/100
110/110 [==============================] - 1s 12ms/step - loss: 14.0078 - val_loss: 14.2176 - _timestamp: 1652165172.0000 - _runtime: 59.0000
WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d867b820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d867b820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
==================== fold_0 score ====================
rmse: 31.38327410581271
2022-05-10 14:46:13.111638: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.