==================== fold_0 Training ====================
Epoch 1/50
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cff39040> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cff39040> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 41/110 [==========>...................] - ETA: 1s - loss: 12.3621 - val_loss: 12.3621
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
110/110 [==============================] - ETA: 0s - loss: 12.0447 - val_loss: 11.9439WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cf200160> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cf200160> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 3s 20ms/step - loss: 12.0447 - val_loss: 10.4381 - val_val_loss: 10.3973 - _timestamp: 1652165016.0000 - _runtime: 6.0000
Epoch 2/50
 51/110 [============>.................] - ETA: 0s - loss: 10.9603 - val_loss: 10.9603
110/110 [==============================] - 1s 13ms/step - loss: 10.9157 - val_loss: 10.8982 - _timestamp: 1652165017.0000 - _runtime: 7.0000
Epoch 3/50
110/110 [==============================] - 1s 13ms/step - loss: 10.6984 - val_loss: 10.6278 - _timestamp: 1652165019.0000 - _runtime: 9.0000
Epoch 4/50
110/110 [==============================] - 1s 13ms/step - loss: 10.6955 - val_loss: 10.7494 - _timestamp: 1652165020.0000 - _runtime: 10.0000
Epoch 5/50
110/110 [==============================] - 1s 12ms/step - loss: 10.5638 - val_loss: 10.5517 - _timestamp: 1652165022.0000 - _runtime: 12.0000
Epoch 6/50
110/110 [==============================] - 1s 13ms/step - loss: 10.6289 - val_loss: 10.5459 - _timestamp: 1652165023.0000 - _runtime: 13.0000
Epoch 7/50
110/110 [==============================] - 1s 13ms/step - loss: 10.5634 - val_loss: 10.7381 - _timestamp: 1652165024.0000 - _runtime: 14.0000
Epoch 8/50
110/110 [==============================] - 1s 12ms/step - loss: 10.5706 - val_loss: 10.5664 - _timestamp: 1652165026.0000 - _runtime: 16.0000
Epoch 9/50
110/110 [==============================] - 1s 13ms/step - loss: 10.6442 - val_loss: 10.5745 - _timestamp: 1652165027.0000 - _runtime: 17.0000
Epoch 10/50
110/110 [==============================] - 2s 14ms/step - loss: 10.2798 - val_loss: 10.2021 - _timestamp: 1652165029.0000 - _runtime: 19.0000
Epoch 11/50
110/110 [==============================] - 1s 12ms/step - loss: 10.4526 - val_loss: 10.3794 - _timestamp: 1652165030.0000 - _runtime: 20.0000
Epoch 12/50
110/110 [==============================] - 2s 14ms/step - loss: 10.2580 - val_loss: 10.1942 - _timestamp: 1652165031.0000 - _runtime: 21.0000
Epoch 13/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4869 - val_loss: 10.4169 - _timestamp: 1652165033.0000 - _runtime: 23.0000
Epoch 14/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4381 - val_loss: 10.3596 - _timestamp: 1652165034.0000 - _runtime: 24.0000
Epoch 15/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4252 - val_loss: 10.5807 - _timestamp: 1652165036.0000 - _runtime: 26.0000
Epoch 16/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4362 - val_loss: 10.3788 - _timestamp: 1652165037.0000 - _runtime: 27.0000
Epoch 17/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4503 - val_loss: 10.3728 - _timestamp: 1652165039.0000 - _runtime: 29.0000
Epoch 18/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4075 - val_loss: 10.3177 - _timestamp: 1652165040.0000 - _runtime: 30.0000
Epoch 19/50
110/110 [==============================] - 1s 12ms/step - loss: 10.4493 - val_loss: 10.3716 - _timestamp: 1652165041.0000 - _runtime: 31.0000
Epoch 20/50
110/110 [==============================] - 1s 13ms/step - loss: 10.3583 - val_loss: 10.2991 - _timestamp: 1652165043.0000 - _runtime: 33.0000
Epoch 21/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4134 - val_loss: 10.3239 - _timestamp: 1652165044.0000 - _runtime: 34.0000
Epoch 22/50
110/110 [==============================] - 1s 13ms/step - loss: 10.3502 - val_loss: 11.0594 - _timestamp: 1652165046.0000 - _runtime: 36.0000
Epoch 23/50
110/110 [==============================] - 1s 12ms/step - loss: 10.3068 - val_loss: 10.3738 - _timestamp: 1652165047.0000 - _runtime: 37.0000
Epoch 24/50
110/110 [==============================] - 1s 13ms/step - loss: 10.4181 - val_loss: 10.4690 - _timestamp: 1652165049.0000 - _runtime: 39.0000
WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a1c634c0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a1c634c0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
==================== fold_0 score ====================
rmse: 31.638864075338986
2022-05-10 14:44:09.258432: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.