/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
2022-05-10 14:30:30.397599: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
2022-05-10 14:30:31.295724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x360598310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x360598310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - ETA: 0s - loss: 13.6597 - val_loss: 13.5599WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d4503700> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d4503700> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 1s 9ms/step - loss: 13.6597 - val_loss: 11.0286 - val_val_loss: 10.9926 - _timestamp: 1652164231.0000 - _runtime: 6.0000
Epoch 2/200
110/110 [==============================] - 1s 7ms/step - loss: 12.8167 - val_loss: 12.7417 - _timestamp: 1652164232.0000 - _runtime: 7.0000
Epoch 3/200
110/110 [==============================] - 1s 7ms/step - loss: 12.4569 - val_loss: 12.3812 - _timestamp: 1652164233.0000 - _runtime: 8.0000
Epoch 4/200
110/110 [==============================] - 1s 7ms/step - loss: 12.1176 - val_loss: 12.0863 - _timestamp: 1652164233.0000 - _runtime: 8.0000
Epoch 5/200
110/110 [==============================] - 1s 6ms/step - loss: 11.5923 - val_loss: 11.5878 - _timestamp: 1652164234.0000 - _runtime: 9.0000
Epoch 6/200
110/110 [==============================] - 1s 6ms/step - loss: 11.1128 - val_loss: 11.1157 - _timestamp: 1652164235.0000 - _runtime: 10.0000
Epoch 7/200
110/110 [==============================] - 1s 6ms/step - loss: 11.2609 - val_loss: 11.2392 - _timestamp: 1652164235.0000 - _runtime: 10.0000
Epoch 8/200
110/110 [==============================] - 1s 6ms/step - loss: 11.3782 - val_loss: 11.3268 - _timestamp: 1652164236.0000 - _runtime: 11.0000
Epoch 9/200
110/110 [==============================] - 1s 6ms/step - loss: 11.2333 - val_loss: 11.1748 - _timestamp: 1652164237.0000 - _runtime: 12.0000
Epoch 10/200
110/110 [==============================] - 1s 6ms/step - loss: 11.3609 - val_loss: 11.3394 - _timestamp: 1652164237.0000 - _runtime: 12.0000
Epoch 11/200
110/110 [==============================] - 1s 6ms/step - loss: 11.0556 - val_loss: 10.9781 - _timestamp: 1652164238.0000 - _runtime: 13.0000
Epoch 12/200
110/110 [==============================] - 1s 6ms/step - loss: 11.2924 - val_loss: 11.2325 - _timestamp: 1652164239.0000 - _runtime: 14.0000
Epoch 13/200
110/110 [==============================] - 1s 6ms/step - loss: 11.1262 - val_loss: 11.0771 - _timestamp: 1652164239.0000 - _runtime: 14.0000
Epoch 14/200
110/110 [==============================] - 1s 6ms/step - loss: 11.1722 - val_loss: 11.1629 - _timestamp: 1652164240.0000 - _runtime: 15.0000
Epoch 15/200
110/110 [==============================] - 1s 6ms/step - loss: 11.0821 - val_loss: 11.0826 - _timestamp: 1652164241.0000 - _runtime: 16.0000
Epoch 16/200
110/110 [==============================] - 1s 6ms/step - loss: 11.2244 - val_loss: 11.2044 - _timestamp: 1652164241.0000 - _runtime: 16.0000
Epoch 17/200
110/110 [==============================] - 1s 7ms/step - loss: 11.1399 - val_loss: 11.1121 - _timestamp: 1652164242.0000 - _runtime: 17.0000
Epoch 18/200
110/110 [==============================] - 1s 7ms/step - loss: 11.1562 - val_loss: 11.0590 - _timestamp: 1652164243.0000 - _runtime: 18.0000
Epoch 19/200
110/110 [==============================] - 1s 7ms/step - loss: 11.3565 - val_loss: 11.3300 - _timestamp: 1652164244.0000 - _runtime: 19.0000
Epoch 20/200
110/110 [==============================] - 1s 6ms/step - loss: 10.9700 - val_loss: 10.9133 - _timestamp: 1652164244.0000 - _runtime: 19.0000
Epoch 21/200
110/110 [==============================] - 1s 6ms/step - loss: 11.1900 - val_loss: 11.1185 - _timestamp: 1652164245.0000 - _runtime: 20.0000
Epoch 22/200
110/110 [==============================] - 1s 6ms/step - loss: 11.3504 - val_loss: 11.3568 - _timestamp: 1652164246.0000 - _runtime: 21.0000
Epoch 23/200
110/110 [==============================] - 1s 6ms/step - loss: 11.1724 - val_loss: 11.1519 - _timestamp: 1652164246.0000 - _runtime: 21.0000
Epoch 24/200
110/110 [==============================] - 1s 6ms/step - loss: 11.1232 - val_loss: 11.0805 - _timestamp: 1652164247.0000 - _runtime: 22.0000
Epoch 25/200
110/110 [==============================] - 1s 6ms/step - loss: 10.9054 - val_loss: 10.8773 - _timestamp: 1652164248.0000 - _runtime: 23.0000
Epoch 26/200
110/110 [==============================] - 1s 6ms/step - loss: 10.9948 - val_loss: 10.9021 - _timestamp: 1652164248.0000 - _runtime: 23.0000
Epoch 27/200
110/110 [==============================] - 1s 6ms/step - loss: 10.9421 - val_loss: 10.9398 - _timestamp: 1652164249.0000 - _runtime: 24.0000
Epoch 28/200
110/110 [==============================] - 1s 7ms/step - loss: 11.2140 - val_loss: 11.1917 - _timestamp: 1652164250.0000 - _runtime: 25.0000
Epoch 29/200
110/110 [==============================] - 1s 6ms/step - loss: 10.8628 - val_loss: 13.0284 - _timestamp: 1652164250.0000 - _runtime: 25.0000
Epoch 30/200
110/110 [==============================] - 1s 7ms/step - loss: 10.8430 - val_loss: 10.8465 - _timestamp: 1652164251.0000 - _runtime: 26.0000
Epoch 31/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7208 - val_loss: 10.6608 - _timestamp: 1652164252.0000 - _runtime: 27.0000
Epoch 32/200
110/110 [==============================] - 1s 6ms/step - loss: 10.8029 - val_loss: 10.7818 - _timestamp: 1652164252.0000 - _runtime: 27.0000
Epoch 33/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6577 - val_loss: 10.6293 - _timestamp: 1652164253.0000 - _runtime: 28.0000
Epoch 34/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7720 - val_loss: 10.9197 - _timestamp: 1652164254.0000 - _runtime: 29.0000
Epoch 35/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7506 - val_loss: 10.7319 - _timestamp: 1652164254.0000 - _runtime: 29.0000
Epoch 36/200
110/110 [==============================] - 1s 6ms/step - loss: 10.8119 - val_loss: 10.7479 - _timestamp: 1652164255.0000 - _runtime: 30.0000
Epoch 37/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7699 - val_loss: 10.9301 - _timestamp: 1652164256.0000 - _runtime: 31.0000
Epoch 38/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6916 - val_loss: 10.6963 - _timestamp: 1652164256.0000 - _runtime: 31.0000
Epoch 39/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7703 - val_loss: 11.2164 - _timestamp: 1652164257.0000 - _runtime: 32.0000
Epoch 40/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7697 - val_loss: 10.7755 - _timestamp: 1652164258.0000 - _runtime: 33.0000
Epoch 41/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6799 - val_loss: 10.6483 - _timestamp: 1652164258.0000 - _runtime: 33.0000
Epoch 42/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6324 - val_loss: 10.6313 - _timestamp: 1652164259.0000 - _runtime: 34.0000
Epoch 43/200
Epoch 45/200===========================] - 1s 6ms/step - loss: 10.7538 - val_loss: 10.7608 - _timestamp: 1652164260.0000 - _runtime: 35.0000
Epoch 44/200
110/110 [==============================] - 1s 10ms/step - loss: 10.8049 - val_loss: 10.7687 - _timestamp: 1652164261.0000 - _runtime: 36.0000
Epoch 45/200===========================] - 1s 6ms/step - loss: 10.7538 - val_loss: 10.7608 - _timestamp: 1652164260.0000 - _runtime: 35.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.7231 - val_loss: 10.7180 - _timestamp: 1652164262.0000 - _runtime: 37.0000
Epoch 46/200
 11/110 [==>...........................] - ETA: 0s - loss: 8.3988 - val_loss: 8.39880.8274 - _timestamp: 1652164262.0000 - _runtime: 37.0000
Epoch 47/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6811 - val_loss: 10.6629 - _timestamp: 1652164263.0000 - _runtime: 38.0000
Epoch 48/200
110/110 [==============================] - 1s 6ms/step - loss: 10.8745 - val_loss: 10.8429 - _timestamp: 1652164264.0000 - _runtime: 39.0000
Epoch 49/200
 19/110 [====>.........................] - ETA: 0s - loss: 11.1630 - val_loss: 11.16306650 - _timestamp: 1652164264.0000 - _runtime: 39.0000
Epoch 50/200
110/110 [==============================] - 1s 6ms/step - loss: 10.5372 - val_loss: 10.5058 - _timestamp: 1652164265.0000 - _runtime: 40.0000
Epoch 51/200
110/110 [==============================] - 1s 6ms/step - loss: 10.5808 - val_loss: 10.5496 - _timestamp: 1652164265.0000 - _runtime: 40.0000
Epoch 52/200
 26/110 [======>.......................] - ETA: 0s - loss: 12.0897 - val_loss: 12.08977476 - _timestamp: 1652164266.0000 - _runtime: 41.0000
Epoch 53/200
110/110 [==============================] - 1s 6ms/step - loss: 10.5876 - val_loss: 10.5741 - _timestamp: 1652164267.0000 - _runtime: 42.0000
Epoch 54/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7826 - val_loss: 10.9368 - _timestamp: 1652164267.0000 - _runtime: 42.0000
Epoch 55/200
 36/110 [========>.....................] - ETA: 0s - loss: 10.5140 - val_loss: 10.51407656 - _timestamp: 1652164268.0000 - _runtime: 43.0000
Epoch 56/200
110/110 [==============================] - 1s 6ms/step - loss: 10.8040 - val_loss: 10.8093 - _timestamp: 1652164269.0000 - _runtime: 44.0000
Epoch 57/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7292 - val_loss: 10.6422 - _timestamp: 1652164269.0000 - _runtime: 44.0000
Epoch 58/200
 47/110 [===========>..................] - ETA: 0s - loss: 9.2590 - val_loss: 9.2590  4248 - _timestamp: 1652164270.0000 - _runtime: 45.0000
Epoch 59/200
110/110 [==============================] - 1s 6ms/step - loss: 10.8753 - val_loss: 10.8635 - _timestamp: 1652164271.0000 - _runtime: 46.0000
Epoch 60/200
110/110 [==============================] - 1s 6ms/step - loss: 10.5109 - val_loss: 10.6079 - _timestamp: 1652164271.0000 - _runtime: 46.0000
Epoch 61/200
 66/110 [=================>............] - ETA: 0s - loss: 11.9691 - val_loss: 11.96913046 - _timestamp: 1652164272.0000 - _runtime: 47.0000
Epoch 62/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7350 - val_loss: 10.7605 - _timestamp: 1652164273.0000 - _runtime: 48.0000
Epoch 63/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6413 - val_loss: 10.5483 - _timestamp: 1652164273.0000 - _runtime: 48.0000
Epoch 64/200
 76/110 [===================>..........] - ETA: 0s - loss: 11.0931 - val_loss: 11.09316436 - _timestamp: 1652164274.0000 - _runtime: 49.0000
Epoch 65/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7532 - val_loss: 10.6881 - _timestamp: 1652164275.0000 - _runtime: 50.0000
Epoch 66/200
110/110 [==============================] - 1s 6ms/step - loss: 10.7809 - val_loss: 10.7636 - _timestamp: 1652164275.0000 - _runtime: 50.0000
Epoch 67/200
 98/110 [=========================>....] - ETA: 0s - loss: 10.8318 - val_loss: 10.83185394 - _timestamp: 1652164276.0000 - _runtime: 51.0000
Epoch 68/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6833 - val_loss: 10.6750 - _timestamp: 1652164277.0000 - _runtime: 52.0000
Epoch 69/200
110/110 [==============================] - 1s 6ms/step - loss: 10.5087 - val_loss: 10.4846 - _timestamp: 1652164277.0000 - _runtime: 52.0000
Epoch 70/200
110/110 [==============================] - 1s 5ms/step - loss: 10.6136 - val_loss: 10.6010 - _timestamp: 1652164280.0000 - _runtime: 55.0000
Epoch 71/200
110/110 [==============================] - 1s 5ms/step - loss: 10.6677 - val_loss: 10.6561 - _timestamp: 1652164278.0000 - _runtime: 53.0000
Epoch 72/200
110/110 [==============================] - 1s 6ms/step - loss: 10.6344 - val_loss: 10.5938 - _timestamp: 1652164279.0000 - _runtime: 54.0000
Epoch 73/200
110/110 [==============================] - 1s 5ms/step - loss: 10.6136 - val_loss: 10.6010 - _timestamp: 1652164280.0000 - _runtime: 55.0000
Epoch 74/200
110/110 [==============================] - 1s 5ms/step - loss: 10.8446 - val_loss: 10.8153 - _timestamp: 1652164280.0000 - _runtime: 55.0000
Epoch 75/200
110/110 [==============================] - 1s 7ms/step - loss: 10.8395 - val_loss: 11.1142 - _timestamp: 1652164281.0000 - _runtime: 56.0000
Epoch 76/200
 28/110 [======>.......................] - ETA: 0s - loss: 8.9730 - val_loss: 8.9730
 84/110 [=====================>........] - ETA: 0s - loss: 10.5627 - val_loss: 10.56278153 - _timestamp: 1652164280.0000 - _runtime: 55.0000
110/110 [==============================] - 1s 7ms/step - loss: 10.4432 - val_loss: 10.4114 - _timestamp: 1652164283.0000 - _runtime: 58.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.6123 - val_loss: 10.6283 - _timestamp: 1652164286.0000 - _runtime: 61.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.5231 - val_loss: 10.4981 - _timestamp: 1652164289.0000 - _runtime: 64.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.7311 - val_loss: 10.7062 - _timestamp: 1652164291.0000 - _runtime: 66.0000
 31/110 [=======>......................] - ETA: 0s - loss: 10.3254 - val_loss: 10.32547062 - _timestamp: 1652164291.0000 - _runtime: 66.0000
110/110 [==============================] - 1s 5ms/step - loss: 10.5329 - val_loss: 10.5287 - _timestamp: 1652164294.0000 - _runtime: 69.0000
110/110 [==============================] - 1s 5ms/step - loss: 10.6906 - val_loss: 12.1634 - _timestamp: 1652164296.0000 - _runtime: 71.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.7529 - val_loss: 10.9315 - _timestamp: 1652164299.0000 - _runtime: 74.0000
 91/110 [=======================>......] - ETA: 0s - loss: 10.2876 - val_loss: 10.28769315 - _timestamp: 1652164299.0000 - _runtime: 74.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.4175 - val_loss: 10.4016 - _timestamp: 1652164302.0000 - _runtime: 77.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.5660 - val_loss: 10.5353 - _timestamp: 1652164304.0000 - _runtime: 79.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.7863 - val_loss: 10.6981 - _timestamp: 1652164307.0000 - _runtime: 82.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.7253 - val_loss: 10.6928 - _timestamp: 1652164309.0000 - _runtime: 84.0000
 61/110 [===============>..............] - ETA: 0s - loss: 10.5389 - val_loss: 10.53896928 - _timestamp: 1652164309.0000 - _runtime: 84.0000
110/110 [==============================] - 1s 5ms/step - loss: 10.5225 - val_loss: 10.5078 - _timestamp: 1652164312.0000 - _runtime: 87.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.4895 - val_loss: 10.4756 - _timestamp: 1652164315.0000 - _runtime: 90.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.6143 - val_loss: 10.9074 - _timestamp: 1652164317.0000 - _runtime: 92.0000
 46/110 [===========>..................] - ETA: 0s - loss: 9.7627 - val_loss: 9.7627  9074 - _timestamp: 1652164317.0000 - _runtime: 92.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.3698 - val_loss: 10.3103 - _timestamp: 1652164320.0000 - _runtime: 95.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.6120 - val_loss: 10.5318 - _timestamp: 1652164323.0000 - _runtime: 98.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.6140 - val_loss: 10.6399 - _timestamp: 1652164326.0000 - _runtime: 101.0000
 27/110 [======>.......................] - ETA: 0s - loss: 11.4093 - val_loss: 11.40936399 - _timestamp: 1652164326.0000 - _runtime: 101.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.4624 - val_loss: 10.4452 - _timestamp: 1652164328.0000 - _runtime: 103.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.5787 - val_loss: 10.5512 - _timestamp: 1652164331.0000 - _runtime: 106.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.6232 - val_loss: 10.6337 - _timestamp: 1652164333.0000 - _runtime: 108.0000
 63/110 [================>.............] - ETA: 0s - loss: 10.7868 - val_loss: 10.78686337 - _timestamp: 1652164333.0000 - _runtime: 108.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.8024 - val_loss: 10.7544 - _timestamp: 1652164336.0000 - _runtime: 111.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.5212 - val_loss: 10.5140 - _timestamp: 1652164339.0000 - _runtime: 114.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.6502 - val_loss: 10.7867 - _timestamp: 1652164341.0000 - _runtime: 116.0000
  1/110 [..............................] - ETA: 0s - loss: 3.9457 - val_loss: 3.94570.7867 - _timestamp: 1652164341.0000 - _runtime: 116.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.5589 - val_loss: 10.4772 - _timestamp: 1652164344.0000 - _runtime: 119.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.5707 - val_loss: 10.4845 - _timestamp: 1652164347.0000 - _runtime: 122.0000
110/110 [==============================] - 1s 6ms/step - loss: 10.5751 - val_loss: 10.5507 - _timestamp: 1652164349.0000 - _runtime: 124.0000
  1/110 [..............................] - ETA: 0s - loss: 2.7107 - val_loss: 2.71070.5507 - _timestamp: 1652164349.0000 - _runtime: 124.0000
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert_timestamp: 1652164352.0000 - _runtime: 127.0000
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert_timestamp: 1652164352.0000 - _runtime: 127.0000
2022-05-10 14:32:32.735892: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.