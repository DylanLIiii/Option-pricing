{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621cbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "import wandb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import gc \n",
    "from wandb.xgboost import wandb_callback\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eab32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edc74c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f1ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "class CONFIG:\n",
    "    nfold = 5\n",
    "    is_local = True\n",
    "    local_path = '/Users/dylan/DylanLi/XJTLU/期权项目/Data/Output_data/option/'\n",
    "    local_log_path = '/Users/dylan/DylanLi/XJTLU/期权项目/Code/model_nn/log/'\n",
    "    colab_path = '/content/'\n",
    "    seed = 1128\n",
    "    log_dir = local_log_path + datetime.datetime.now().strftime(r\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2ecf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "class CONFIG:\n",
    "    nfold = 5\n",
    "    is_local = True\n",
    "    local_path = '/Users/dylan/DylanLi/XJTLU/期权项目/Data/Output_data/option/'\n",
    "    local_log_path = '/Users/dylan/DylanLi/XJTLU/期权项目/Code/model_nn/log/'\n",
    "    colab_path = '/content/'\n",
    "    seed = 42\n",
    "    log_dir = local_log_path + datetime.datetime.now().strftime(r\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1112e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.is_local:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.local_path, '2020_whole.csv'))\n",
    "    \n",
    "else:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.colab_path, '2020_whole.csv'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666e2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.is_local:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.local_path, '2020_whole.csv'))\n",
    "    \n",
    "else:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.colab_path, '2020_whole.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95bdbc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "class CONFIG:\n",
    "    nfold = 5\n",
    "    is_local = True\n",
    "    local_path = '/Users/dylan/iCollections/桌面文件/Repo/Option_Pricing/Data/Output_data/option/'\n",
    "    local_log_path = '/Users/dylan/iCollections/桌面文件/Repo/Option_Pricing/Code/model_nn/log'\n",
    "    colab_path = '/content/'\n",
    "    seed = 42\n",
    "    log_dir = local_log_path + datetime.datetime.now().strftime(r\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd38ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.is_local:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.local_path, '2020_whole.csv'))\n",
    "    \n",
    "else:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.colab_path, '2020_whole.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a10048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用 SP 试试 \n",
    "data_C = data[data['call_put'] == 'C']]\n",
    "data_P = data[data['call_put' == 'P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb279af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用 SP 试试 \n",
    "data_C = data[data['call_put'] == 'C']\n",
    "data_P = data[data['call_put'] == 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f129df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Date, underlying, exchange, root_symbol, futures_symbol, fut_expiration_date, futures_close, opt_expiration_date, strike, call_put, style, bid, ask, settlement, volume, open_interest]\n",
      "Index: []"
     ]
    }
   ],
   "source": [
    "data_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314b7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用 SP 试试 \n",
    "data_C = data[data['call_put'] == 'c']\n",
    "data_P = data[data['call_put'] == 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cf1c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a932f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0        Date underlying exchange root_symbol  \\\n",
      "1                 1  2020-02-05         NG    NYMEX          LN   \n",
      "2                 2  2020-02-05         NG    NYMEX          LN   \n",
      "3                 3  2020-02-05         NG    NYMEX          LN   \n",
      "4                 4  2020-02-05         NG    NYMEX          LN   \n",
      "5                 5  2020-02-05         NG    NYMEX          LN   \n",
      "...             ...         ...        ...      ...         ...   \n",
      "2103136     2103136  2020-09-28        MNQ      CME         MQ1   \n",
      "2103137     2103137  2020-09-28        MNQ      CME         MQ1   \n",
      "2103138     2103138  2020-09-28        MNQ      CME         MQ1   \n",
      "2103139     2103139  2020-09-28        MNQ      CME         MQ1   \n",
      "2103140     2103140  2020-09-28        MNQ      CME         MQ1   \n",
      "\n",
      "        futures_symbol fut_expiration_date  futures_close opt_expiration_date  \\\n",
      "1            NG/20X.NX          10/28/2020          2.237          2020-10-27   \n",
      "2            NG/20X.NX          10/28/2020          2.237          2020-10-27   \n",
      "3            NG/20X.NX          10/28/2020          2.237          2020-10-27   \n",
      "4            NG/20X.NX          10/28/2020          2.237          2020-10-27   \n",
      "5            NG/20X.NX          10/28/2020          2.237          2020-10-27   \n",
      "...                ...                 ...            ...                 ...   \n",
      "2103136     MNQ/20Z.CM          12/18/2020      11394.750          2020-10-02   \n",
      "2103137     MNQ/20Z.CM          12/18/2020      11394.750          2020-10-02   \n",
      "2103138     MNQ/20Z.CM          12/18/2020      11394.750          2020-10-02   \n",
      "2103139     MNQ/20Z.CM          12/18/2020      11394.750          2020-10-02   \n",
      "2103140     MNQ/20Z.CM          12/18/2020      11394.750          2020-10-02   \n",
      "\n",
      "           strike call_put style  bid  ask  settlement  volume  open_interest  \n",
      "1            2.50        c     E  0.0  0.0      0.1068     575           1275  \n",
      "2            2.55        c     E  0.0  0.0      0.0941       5             24  \n",
      "3            2.75        c     E  0.0  0.0      0.0562     200           1973  \n",
      "4            3.00        c     E  0.0  0.0      0.0302      75           6765  \n",
      "5            3.50        c     E  0.0  0.0      0.0113     250           1766  \n",
      "...           ...      ...   ...  ...  ...         ...     ...            ...  \n",
      "2103136  11450.00        c     E  0.0  0.0    116.7500       4              1  \n",
      "2103137  11520.00        c     E  0.0  0.0     85.5000      16             16  \n",
      "2103138  11530.00        c     E  0.0  0.0     81.7500       2              2  \n",
      "2103139  11720.00        c     E  0.0  0.0     28.2500       1              1  \n",
      "2103140  11800.00        c     E  0.0  0.0     16.7500       1              4  \n",
      "\n",
      "[1005250 rows x 17 columns]"
     ]
    }
   ],
   "source": [
    "data_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a20c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cv(df, fold):\n",
    "    \"\"\"return a dataframe with fold index\n",
    "    Use two method here, one with PurgedGroupTimeSeriesSplit(self designed GoupKFold API in scikit-learn), one with TimeSeriesSplit\n",
    "    \n",
    "    To avoid the problem of differents fold with high homogeneity, should use small n_splits:\n",
    "    - If your purpose is performance estimation, you need models with low bias estimates (which means no systematic distortion of estimates). \n",
    "        You can achieve this by using a higher number of folds, usually between 10 and 20.\n",
    "    - If your aim is parameter tuning, you need a mix of bias and variance, so it is advisable to use a medium number of folds, usually between 5 and 7.\n",
    "    - If your purpose is just to apply variable selection and simplify your dataset, you need models with low variance estimates (or you will have disagreement). \n",
    "        Hence, a lower number of folds will suffice, usually between 3 and 5.\n",
    "    \n",
    "    To simulate the generation of time window, use large n_splits, the step can be numbers of sample(n) // n_splits\n",
    "    e.g. when n_splits = n, step = n // n = 1\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): dataframe with time window\n",
    "        fold (int): fold number\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataframe with fold index\n",
    "    \"\"\"\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    df.sort_values(by='Date', inplace = True)\n",
    "    df['fold'] = -1\n",
    "    if len(data.underlying.unique()) > 1:\n",
    "        cv = PurgedGroupTimeSeriesSplit(n_splits=fold, group_gap=0)\n",
    "        step = df.shape[0]//fold\n",
    "        print(f\"{'=' * 20} step : {step} {'=' * 20}\")\n",
    "        for index, (train_, val_) in enumerate(cv.split(df, groups=df.underlying)):\n",
    "            print(f\"{'=' * 20} val_{index} {'=' * 20}\")\n",
    "            print(val_)\n",
    "            df.iloc[val_, -1] = index\n",
    "    else :\n",
    "        cv = TimeSeriesSplit(n_splits=fold)\n",
    "        step = df.shape[0]//fold\n",
    "        print(f\"{'=' * 20} step : {step} {'=' * 20}\")\n",
    "        for index, (train_, val_) in enumerate(cv.split(df)):\n",
    "            print(f\"{'=' * 20} val_{index} {'=' * 20}\")\n",
    "            print(val_)\n",
    "            df.iloc[val_, -1] = index\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "490e25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C = setup_cv(data, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed189e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cv(df, fold):\n",
    "    \"\"\"return a dataframe with fold index\n",
    "    Use two method here, one with PurgedGroupTimeSeriesSplit(self designed GoupKFold API in scikit-learn), one with TimeSeriesSplit\n",
    "    \n",
    "    To avoid the problem of differents fold with high homogeneity, should use small n_splits:\n",
    "    - If your purpose is performance estimation, you need models with low bias estimates (which means no systematic distortion of estimates). \n",
    "        You can achieve this by using a higher number of folds, usually between 10 and 20.\n",
    "    - If your aim is parameter tuning, you need a mix of bias and variance, so it is advisable to use a medium number of folds, usually between 5 and 7.\n",
    "    - If your purpose is just to apply variable selection and simplify your dataset, you need models with low variance estimates (or you will have disagreement). \n",
    "        Hence, a lower number of folds will suffice, usually between 3 and 5.\n",
    "    \n",
    "    To simulate the generation of time window, use large n_splits, the step can be numbers of sample(n) // n_splits\n",
    "    e.g. when n_splits = n, step = n // n = 1\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): dataframe with time window\n",
    "        fold (int): fold number\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataframe with fold index\n",
    "    \"\"\"\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    df.sort_values(by='Date', inplace = True)\n",
    "    df['fold'] = -1\n",
    "    if len(data.underlying.unique()) > 1:\n",
    "        cv = PurgedGroupTimeSeriesSplit(n_splits=fold, group_gap=0)\n",
    "        step = df.shape[0]//fold\n",
    "        print(f\"{'=' * 20} step : {step} {'=' * 20}\")\n",
    "        for index, (train_, val_) in enumerate(cv.split(df, groups=df.underlying)):\n",
    "            print(f\"{'=' * 20} val_{index} {'=' * 20}\")\n",
    "            print(val_[-50:])\n",
    "            df.iloc[val_, -1] = index\n",
    "    else :\n",
    "        cv = TimeSeriesSplit(n_splits=fold)\n",
    "        step = df.shape[0]//fold\n",
    "        print(f\"{'=' * 20} step : {step} {'=' * 20}\")\n",
    "        for index, (train_, val_) in enumerate(cv.split(df)):\n",
    "            print(f\"{'=' * 20} val_{index} {'=' * 20}\")\n",
    "            print(val_[-50:])\n",
    "            df.iloc[val_, -1] = index\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d11eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C = setup_cv(data, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a4849b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-1,  0,  1,  2,  3,  4])"
     ]
    }
   ],
   "source": [
    "data.fold.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bafd3599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-1,  0,  1,  2,  3,  4])"
     ]
    }
   ],
   "source": [
    "data_C.fold.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "417ec9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Date', 'underlying', 'exchange', 'root_symbol',\n",
      "       'futures_symbol', 'fut_expiration_date', 'futures_close',\n",
      "       'opt_expiration_date', 'strike', 'call_put', 'style', 'bid', 'ask',\n",
      "       'settlement', 'volume', 'open_interest', 'fold'],\n",
      "      dtype='object')"
     ]
    }
   ],
   "source": [
    "data_C.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2961b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.is_local:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.local_path, '2020_whole.csv'))\n",
    "    \n",
    "else:\n",
    "    data = pd.read_csv(os.path.join(CONFIG.colab_path, '2020_whole.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9328eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用 SP 试试 \n",
    "data_C = data[data['call_put'] == 'c']\n",
    "data_P = data[data['call_put'] == 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddf61f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cv(df, fold):\n",
    "    \"\"\"return a dataframe with fold index\n",
    "    Use two method here, one with PurgedGroupTimeSeriesSplit(self designed GoupKFold API in scikit-learn), one with TimeSeriesSplit\n",
    "    \n",
    "    To avoid the problem of differents fold with high homogeneity, should use small n_splits:\n",
    "    - If your purpose is performance estimation, you need models with low bias estimates (which means no systematic distortion of estimates). \n",
    "        You can achieve this by using a higher number of folds, usually between 10 and 20.\n",
    "    - If your aim is parameter tuning, you need a mix of bias and variance, so it is advisable to use a medium number of folds, usually between 5 and 7.\n",
    "    - If your purpose is just to apply variable selection and simplify your dataset, you need models with low variance estimates (or you will have disagreement). \n",
    "        Hence, a lower number of folds will suffice, usually between 3 and 5.\n",
    "    \n",
    "    To simulate the generation of time window, use large n_splits, the step can be numbers of sample(n) // n_splits\n",
    "    e.g. when n_splits = n, step = n // n = 1\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): dataframe with time window\n",
    "        fold (int): fold number\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataframe with fold index\n",
    "    \"\"\"\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    df.sort_values(by='Date', inplace = True)\n",
    "    df['fold'] = -1\n",
    "    if len(data.underlying.unique()) > 1:\n",
    "        cv = PurgedGroupTimeSeriesSplit(n_splits=fold, group_gap=0)\n",
    "        step = df.shape[0]//fold\n",
    "        print(f\"{'=' * 20} step : {step} {'=' * 20}\")\n",
    "        for index, (train_, val_) in enumerate(cv.split(df, groups=df.underlying)):\n",
    "            print(f\"{'=' * 20} val_{index} {'=' * 20}\")\n",
    "            print(val_[-50:])\n",
    "            df.iloc[val_, -1] = index\n",
    "    else :\n",
    "        cv = TimeSeriesSplit(n_splits=fold)\n",
    "        step = df.shape[0]//fold\n",
    "        print(f\"{'=' * 20} step : {step} {'=' * 20}\")\n",
    "        for index, (train_, val_) in enumerate(cv.split(df)):\n",
    "            print(f\"{'=' * 20} val_{index} {'=' * 20}\")\n",
    "            print(val_[-50:])\n",
    "            df.iloc[val_, -1] = index\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aa14605",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C = setup_cv(data, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c024cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-1,  1,  2,  0,  3,  4])"
     ]
    }
   ],
   "source": [
    "data_C.fold.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87b92ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Date', 'underlying', 'exchange',\n",
      "       'root_symbol', 'futures_symbol', 'fut_expiration_date', 'futures_close',\n",
      "       'opt_expiration_date', 'strike', 'call_put', 'style', 'bid', 'ask',\n",
      "       'settlement', 'volume', 'open_interest', '1M', '3M', '6M', '12M',\n",
      "       'fold'],\n",
      "      dtype='object')"
     ]
    }
   ],
   "source": [
    "data_C.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32149b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个标的的RV\n",
    "def cal_rv(df):\n",
    "    \"\"\"return rv of each unerlying\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): df with rv\n",
    "    \"\"\"\n",
    "    df_time = df.drop_duplicates(subset=['Date'], keep='first', inplace=False)\n",
    "    df_time['sigma'] = np.std(df_time.futures_close.pct_change(1))\n",
    "    dict_ = df_time[['Date', 'sigma']].set_index('Date').to_dict()['sigma']\n",
    "    \n",
    "    df['sigma'] = df['Date'].map(dict_)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9143e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target\n",
    "\n",
    "data_C = data_C.groupby('underlying').apply(cal_rv)\n",
    "data_C['S_K'] = data_C.strike / data_C.futures_close\n",
    "data_C.Date = pd.to_datetime(data_C.Date)\n",
    "data_C.opt_expiration_date = pd.to_datetime(data_C.opt_expiration_date)\n",
    "data_C['days'] = (data_C.opt_expiration_date - data_C.Date).dt.days/365\n",
    "\n",
    "features = ['S_K', 'days', 'sigma']\n",
    "target = ['settlement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec19b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0.1  Unnamed: 0       Date underlying exchange root_symbol  \\\n",
      "1569641       1569641     1569641 2020-01-02         DA      CME          DA   \n",
      "1572300       1572300     1572300 2020-01-02         ES      CME          EW   \n",
      "1572299       1572299     1572299 2020-01-02         ES      CME          EW   \n",
      "1572298       1572298     1572298 2020-01-02         ES      CME          EW   \n",
      "1572297       1572297     1572297 2020-01-02         ES      CME          EW   \n",
      "...               ...         ...        ...        ...      ...         ...   \n",
      "1018733       1018733     1018733 2020-11-24         ES      CME         EW2   \n",
      "1018734       1018734     1018734 2020-11-24         ES      CME         EW2   \n",
      "1018735       1018735     1018735 2020-11-24         ES      CME         EW2   \n",
      "1018725       1018725     1018725 2020-11-24         ES      CME         EW2   \n",
      "1014297       1014297     1014297 2020-11-24         NG    NYMEX          NG   \n",
      "\n",
      "        futures_symbol fut_expiration_date  futures_close opt_expiration_date  \\\n",
      "1569641      DA/20F.CM          02/05/2020      17.059999          2020-02-05   \n",
      "1572300      ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "1572299      ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "1572298      ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "1572297      ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "...                ...                 ...            ...                 ...   \n",
      "1018733      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "1018734      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "1018735      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "1018725      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "1014297      NG/20Z.NX          11/25/2020       2.775000          2020-11-24   \n",
      "\n",
      "         ...  volume open_interest    1M    3M    6M   12M  fold     sigma  \\\n",
      "1569641  ...       2           413  1.50  1.51  1.53  1.44    -1  0.126342   \n",
      "1572300  ...       6            15  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "1572299  ...       7             8  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "1572298  ...      20           161  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "1572297  ...      13             7  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "...      ...     ...           ...   ...   ...   ...   ...   ...       ...   \n",
      "1018733  ...       3           223  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "1018734  ...     134          6061  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "1018735  ...      42           420  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "1018725  ...      39           242  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "1014297  ...       1           273  0.08  0.09  0.10  0.09     3  0.177440   \n",
      "\n",
      "              S_K      days  \n",
      "1569641  0.937866  0.093151  \n",
      "1572300  1.080086  0.326027  \n",
      "1572299  1.077017  0.326027  \n",
      "1572298  1.073949  0.326027  \n",
      "1572297  1.070881  0.326027  \n",
      "...           ...       ...  \n",
      "1018733  0.919414  0.046575  \n",
      "1018734  0.920790  0.046575  \n",
      "1018735  0.922166  0.046575  \n",
      "1018725  0.867112  0.046575  \n",
      "1014297  0.936937  0.000000  \n",
      "\n",
      "[2103142 rows x 26 columns]"
     ]
    }
   ],
   "source": [
    "data_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3c011ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afd3d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0.1  Unnamed: 0       Date underlying exchange root_symbol  \\\n",
      "0             1569641     1569641 2020-01-02         DA      CME          DA   \n",
      "1             1572300     1572300 2020-01-02         ES      CME          EW   \n",
      "2             1572299     1572299 2020-01-02         ES      CME          EW   \n",
      "3             1572298     1572298 2020-01-02         ES      CME          EW   \n",
      "4             1572297     1572297 2020-01-02         ES      CME          EW   \n",
      "...               ...         ...        ...        ...      ...         ...   \n",
      "2103137       1018733     1018733 2020-11-24         ES      CME         EW2   \n",
      "2103138       1018734     1018734 2020-11-24         ES      CME         EW2   \n",
      "2103139       1018735     1018735 2020-11-24         ES      CME         EW2   \n",
      "2103140       1018725     1018725 2020-11-24         ES      CME         EW2   \n",
      "2103141       1014297     1014297 2020-11-24         NG    NYMEX          NG   \n",
      "\n",
      "        futures_symbol fut_expiration_date  futures_close opt_expiration_date  \\\n",
      "0            DA/20F.CM          02/05/2020      17.059999          2020-02-05   \n",
      "1            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "2            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "3            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "4            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "...                ...                 ...            ...                 ...   \n",
      "2103137      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103138      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103139      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103140      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103141      NG/20Z.NX          11/25/2020       2.775000          2020-11-24   \n",
      "\n",
      "         ...  volume open_interest    1M    3M    6M   12M  fold     sigma  \\\n",
      "0        ...       2           413  1.50  1.51  1.53  1.44    -1  0.126342   \n",
      "1        ...       6            15  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "2        ...       7             8  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "3        ...      20           161  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "4        ...      13             7  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "...      ...     ...           ...   ...   ...   ...   ...   ...       ...   \n",
      "2103137  ...       3           223  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103138  ...     134          6061  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103139  ...      42           420  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103140  ...      39           242  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103141  ...       1           273  0.08  0.09  0.10  0.09     3  0.177440   \n",
      "\n",
      "              S_K      days  \n",
      "0        0.937866  0.093151  \n",
      "1        1.080086  0.326027  \n",
      "2        1.077017  0.326027  \n",
      "3        1.073949  0.326027  \n",
      "4        1.070881  0.326027  \n",
      "...           ...       ...  \n",
      "2103137  0.919414  0.046575  \n",
      "2103138  0.920790  0.046575  \n",
      "2103139  0.922166  0.046575  \n",
      "2103140  0.867112  0.046575  \n",
      "2103141  0.936937  0.000000  \n",
      "\n",
      "[2103142 rows x 26 columns]"
     ]
    }
   ],
   "source": [
    "data_C.reset_index(drop=True, inplace=True)\n",
    "data_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac23a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target\n",
    "\n",
    "data_C = data_C.groupby('underlying').apply(cal_rv)\n",
    "data_C['S_K'] = data_C.strike / data_C.futures_close\n",
    "data_C.Date = pd.to_datetime(data_C.Date)\n",
    "data_C.opt_expiration_date = pd.to_datetime(data_C.opt_expiration_date)\n",
    "data_C['days'] = (data_C.opt_expiration_date - data_C.Date).dt.days/365\n",
    "\n",
    "features = ['S_K', 'days', 'sigma', '12M']\n",
    "target = ['settlement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b003bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0.1  Unnamed: 0       Date underlying exchange root_symbol  \\\n",
      "0             1569641     1569641 2020-01-02         DA      CME          DA   \n",
      "1             1572300     1572300 2020-01-02         ES      CME          EW   \n",
      "2             1572299     1572299 2020-01-02         ES      CME          EW   \n",
      "3             1572298     1572298 2020-01-02         ES      CME          EW   \n",
      "4             1572297     1572297 2020-01-02         ES      CME          EW   \n",
      "...               ...         ...        ...        ...      ...         ...   \n",
      "2103137       1018733     1018733 2020-11-24         ES      CME         EW2   \n",
      "2103138       1018734     1018734 2020-11-24         ES      CME         EW2   \n",
      "2103139       1018735     1018735 2020-11-24         ES      CME         EW2   \n",
      "2103140       1018725     1018725 2020-11-24         ES      CME         EW2   \n",
      "2103141       1014297     1014297 2020-11-24         NG    NYMEX          NG   \n",
      "\n",
      "        futures_symbol fut_expiration_date  futures_close opt_expiration_date  \\\n",
      "0            DA/20F.CM          02/05/2020      17.059999          2020-02-05   \n",
      "1            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "2            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "3            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "4            ES/20M.CM          06/19/2020    3259.000000          2020-04-30   \n",
      "...                ...                 ...            ...                 ...   \n",
      "2103137      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103138      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103139      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103140      ES/20Z.CM          12/18/2020    3632.750000          2020-12-11   \n",
      "2103141      NG/20Z.NX          11/25/2020       2.775000          2020-11-24   \n",
      "\n",
      "         ...  volume open_interest    1M    3M    6M   12M  fold     sigma  \\\n",
      "0        ...       2           413  1.50  1.51  1.53  1.44    -1  0.126342   \n",
      "1        ...       6            15  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "2        ...       7             8  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "3        ...      20           161  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "4        ...      13             7  1.50  1.51  1.53  1.44    -1  0.022691   \n",
      "...      ...     ...           ...   ...   ...   ...   ...   ...       ...   \n",
      "2103137  ...       3           223  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103138  ...     134          6061  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103139  ...      42           420  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103140  ...      39           242  0.08  0.09  0.10  0.09     4  0.022691   \n",
      "2103141  ...       1           273  0.08  0.09  0.10  0.09     3  0.177440   \n",
      "\n",
      "              S_K      days  \n",
      "0        0.937866  0.093151  \n",
      "1        1.080086  0.326027  \n",
      "2        1.077017  0.326027  \n",
      "3        1.073949  0.326027  \n",
      "4        1.070881  0.326027  \n",
      "...           ...       ...  \n",
      "2103137  0.919414  0.046575  \n",
      "2103138  0.920790  0.046575  \n",
      "2103139  0.922166  0.046575  \n",
      "2103140  0.867112  0.046575  \n",
      "2103141  0.936937  0.000000  \n",
      "\n",
      "[2103142 rows x 26 columns]"
     ]
    }
   ],
   "source": [
    "data_C.reset_index(drop=True, inplace=True)\n",
    "data_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46692506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(data,nfold):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': 0.01,\n",
    "        'random_state': CONFIG.seed,\n",
    "    }\n",
    "    scores = []\n",
    "    models = []\n",
    "    \n",
    "    for f in range(nfold):\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        train_idx = data[data.fold != f].index\n",
    "        val_idx = data[data.fold == f].index\n",
    "        min_max_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        train_x = min_max_scaler.fit_transform(data.iloc[train_idx, :][features])\n",
    "        train_y = data.iloc[train_idx, :][target]\n",
    "        \n",
    "        val_x = min_max_scaler.fit_transform(data.iloc[val_idx, :][features])\n",
    "        val_y = data.iloc[val_idx, :]['settlement']\n",
    "        if data.underlying.value_counts().shape[0] == 1:\n",
    "            run = wandb.init(project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'xgb_single_{f}')\n",
    "        else : \n",
    "            run = wandb.init(project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'xgb_multi_{f}')\n",
    "        \n",
    "        callback = wandb_callback()\n",
    "        \n",
    "        model = xgb.XGBRegressor()\n",
    "        model.set_params(**params)\n",
    "        model.fit(train_x, train_y, early_stopping_rounds=10, eval_set=[(val_x, val_y)], callbacks=[callback])\n",
    "        model.save_model(f'/Users/dylan/DylanLi/XJTLU/期权项目/Code/model_xgb/model_{f}.json')\n",
    "        # plot\n",
    "        ax = xgb.plot_importance(model)\n",
    "        plt.show(ax)\n",
    "        wandb.log({\"chart\" : ax})\n",
    "\n",
    "        print(f\"{'='*20}{f} model trained{'='*20}\")\n",
    "        off_pred = model.predict(val_x, ntree_limit=model.best_ntree_limit)\n",
    "        off_score = np.sqrt(mean_squared_error(val_y, off_pred))\n",
    "        scores.append(off_score)\n",
    "        models.append(model)\n",
    "        print(f\"{'=' * 20} fold_{f} {'=' * 20}\")\n",
    "        print(f'rmse: {off_score}')\n",
    "        \n",
    "        \n",
    "        del train_x, train_y, val_x, val_y\n",
    "        gc.collect()\n",
    "        \n",
    "        run.finish()\n",
    "    \n",
    "    return scores, models\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22b7e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, models = xgb_model(data, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee08f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, models = xgb_model(data_C, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae0d0944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              S_K      days     sigma   12M\n",
      "0        0.937866  0.093151  0.126342  1.44\n",
      "1        1.080086  0.326027  0.022691  1.44\n",
      "2        1.077017  0.326027  0.022691  1.44\n",
      "3        1.073949  0.326027  0.022691  1.44\n",
      "4        1.070881  0.326027  0.022691  1.44\n",
      "...           ...       ...       ...   ...\n",
      "2103137  0.919414  0.046575  0.022691  0.09\n",
      "2103138  0.920790  0.046575  0.022691  0.09\n",
      "2103139  0.922166  0.046575  0.022691  0.09\n",
      "2103140  0.867112  0.046575  0.022691  0.09\n",
      "2103141  0.936937  0.000000  0.177440  0.09\n",
      "\n",
      "[2103142 rows x 4 columns]"
     ]
    }
   ],
   "source": [
    "data_C[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4839e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           S_K   days  sigma    12M\n",
      "0        False  False  False  False\n",
      "1        False  False  False  False\n",
      "2        False  False  False  False\n",
      "3        False  False  False  False\n",
      "4        False  False  False  False\n",
      "...        ...    ...    ...    ...\n",
      "2103137  False  False  False  False\n",
      "2103138  False  False  False  False\n",
      "2103139  False  False  False  False\n",
      "2103140  False  False  False  False\n",
      "2103141  False  False  False  False\n",
      "\n",
      "[2103142 rows x 4 columns]"
     ]
    }
   ],
   "source": [
    "data_C[features] == np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7144bf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0.1  Unnamed: 0 Date underlying exchange root_symbol  \\\n",
      "0                 NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "1                 NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "2                 NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "3                 NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "4                 NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "...               ...         ...  ...        ...      ...         ...   \n",
      "2103137           NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "2103138           NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "2103139           NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "2103140           NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "2103141           NaN         NaN  NaT        NaN      NaN         NaN   \n",
      "\n",
      "        futures_symbol fut_expiration_date  futures_close opt_expiration_date  \\\n",
      "0                  NaN                 NaN            NaN                 NaT   \n",
      "1                  NaN                 NaN            NaN                 NaT   \n",
      "2                  NaN                 NaN            NaN                 NaT   \n",
      "3                  NaN                 NaN            NaN                 NaT   \n",
      "4                  NaN                 NaN            NaN                 NaT   \n",
      "...                ...                 ...            ...                 ...   \n",
      "2103137            NaN                 NaN            NaN                 NaT   \n",
      "2103138            NaN                 NaN            NaN                 NaT   \n",
      "2103139            NaN                 NaN            NaN                 NaT   \n",
      "2103140            NaN                 NaN            NaN                 NaT   \n",
      "2103141            NaN                 NaN            NaN                 NaT   \n",
      "\n",
      "         ...  volume open_interest  1M  3M  6M  12M  fold  sigma  S_K  days  \n",
      "0        ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "1        ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "2        ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "3        ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "4        ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "...      ...     ...           ...  ..  ..  ..  ...   ...    ...  ...   ...  \n",
      "2103137  ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "2103138  ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "2103139  ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "2103140  ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "2103141  ...     NaN           NaN NaN NaN NaN  NaN   NaN    NaN  NaN   NaN  \n",
      "\n",
      "[2103142 rows x 26 columns]"
     ]
    }
   ],
   "source": [
    "data_C[data_C[features] == np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70197b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0.1           False\n",
      "Unnamed: 0             False\n",
      "Date                   False\n",
      "underlying             False\n",
      "exchange               False\n",
      "root_symbol            False\n",
      "futures_symbol         False\n",
      "fut_expiration_date    False\n",
      "futures_close          False\n",
      "opt_expiration_date    False\n",
      "strike                 False\n",
      "call_put               False\n",
      "style                  False\n",
      "bid                    False\n",
      "ask                    False\n",
      "settlement             False\n",
      "volume                 False\n",
      "open_interest          False\n",
      "1M                     False\n",
      "3M                     False\n",
      "6M                     False\n",
      "12M                    False\n",
      "fold                   False\n",
      "sigma                  False\n",
      "S_K                     True\n",
      "days                   False\n",
      "dtype: bool"
     ]
    }
   ],
   "source": [
    "data_C[data_C[features] == np.inf].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32441566",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C['S/K'] == np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bce46bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521164    inf\n",
      "564215    inf\n",
      "567445    inf\n",
      "Name: S_K, dtype: float64"
     ]
    }
   ],
   "source": [
    "data_C['S_K'][data_C['S_K'] == np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae259fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C.drop(data_C.S_K == np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c39b4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C.drop(data_C.S_K != np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7723c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0371c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6225458",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, models = xgb_model(data_C, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43327754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(data,nfold):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': 0.01,\n",
    "        'random_state': CONFIG.seed,\n",
    "    }\n",
    "    scores = []\n",
    "    models = []\n",
    "    \n",
    "    for f in range(nfold):\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        train_idx = data[data.fold != f].index\n",
    "        val_idx = data[data.fold == f].index\n",
    "        min_max_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        train_x = min_max_scaler.fit_transform(data.iloc[train_idx, :][features])\n",
    "        train_y = min_max_scaler.fit_transform(data.iloc[train_idx, :][target])\n",
    "        \n",
    "        val_x = min_max_scaler.fit_transform(data.iloc[val_idx, :][features])\n",
    "        val_y = min_max_scaler.fit_transform(data.iloc[val_idx, :]['settlement'])\n",
    "        if data.underlying.value_counts().shape[0] == 1:\n",
    "            run = wandb.init(project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'xgb_single_{f}')\n",
    "        else : \n",
    "            run = wandb.init(project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'xgb_multi_{f}')\n",
    "        \n",
    "        callback = wandb_callback()\n",
    "        \n",
    "        model = xgb.XGBRegressor()\n",
    "        model.set_params(**params)\n",
    "        model.fit(train_x, train_y, early_stopping_rounds=10, eval_set=[(val_x, val_y)], callbacks=[callback])\n",
    "        model.save_model(f'/Users/dylan/iCollections/桌面文件/Repo/Option_Pricing/Code/model_xgb/model_{f}.json')\n",
    "        # plot\n",
    "        ax = xgb.plot_importance(model)\n",
    "        plt.show(ax)\n",
    "        wandb.log({\"chart\" : ax})\n",
    "\n",
    "        print(f\"{'='*20}{f} model trained{'='*20}\")\n",
    "        off_pred = model.predict(val_x, ntree_limit=model.best_ntree_limit)\n",
    "        off_score = np.sqrt(mean_squared_error(val_y, off_pred))\n",
    "        scores.append(off_score)\n",
    "        models.append(model)\n",
    "        print(f\"{'=' * 20} fold_{f} {'=' * 20}\")\n",
    "        print(f'rmse: {off_score}')\n",
    "        \n",
    "        \n",
    "        del train_x, train_y, val_x, val_y\n",
    "        gc.collect()\n",
    "        \n",
    "        run.finish()\n",
    "    \n",
    "    return scores, models\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ce8d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, models = xgb_model(data_C, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d6ac073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(data,nfold):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': 0.01,\n",
    "        'random_state': CONFIG.seed,\n",
    "    }\n",
    "    scores = []\n",
    "    models = []\n",
    "    \n",
    "    for f in range(nfold):\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        train_idx = data[data.fold != f].index\n",
    "        val_idx = data[data.fold == f].index\n",
    "        min_max_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        train_x = min_max_scaler.fit_transform(data.iloc[train_idx, :][features])\n",
    "        train_y = min_max_scaler.fit_transform(data.iloc[train_idx, :][target])\n",
    "        \n",
    "        val_x = min_max_scaler.fit_transform(data.iloc[val_idx, :][features])\n",
    "        val_y = min_max_scaler.fit_transform(data.iloc[val_idx, :][target])\n",
    "        if data.underlying.value_counts().shape[0] == 1:\n",
    "            run = wandb.init(project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'xgb_single_{f}')\n",
    "        else : \n",
    "            run = wandb.init(project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'xgb_multi_{f}')\n",
    "        \n",
    "        callback = wandb_callback()\n",
    "        \n",
    "        model = xgb.XGBRegressor()\n",
    "        model.set_params(**params)\n",
    "        model.fit(train_x, train_y, early_stopping_rounds=10, eval_set=[(val_x, val_y)], callbacks=[callback])\n",
    "        model.save_model(f'/Users/dylan/iCollections/桌面文件/Repo/Option_Pricing/Code/model_xgb/model_{f}.json')\n",
    "        # plot\n",
    "        ax = xgb.plot_importance(model)\n",
    "        plt.show(ax)\n",
    "        wandb.log({\"chart\" : ax})\n",
    "\n",
    "        print(f\"{'='*20}{f} model trained{'='*20}\")\n",
    "        off_pred = model.predict(val_x, ntree_limit=model.best_ntree_limit)\n",
    "        off_score = np.sqrt(mean_squared_error(val_y, off_pred))\n",
    "        scores.append(off_score)\n",
    "        models.append(model)\n",
    "        print(f\"{'=' * 20} fold_{f} {'=' * 20}\")\n",
    "        print(f'rmse: {off_score}')\n",
    "        \n",
    "        \n",
    "        del train_x, train_y, val_x, val_y\n",
    "        gc.collect()\n",
    "        \n",
    "        run.finish()\n",
    "    \n",
    "    return scores, models\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e84e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, models = xgb_model(data_C, CONFIG.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cccb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method' : 'bayes',\n",
    "    'metric' : {\n",
    "        'name' : 'rmse',\n",
    "        'goal' : 'minimize',\n",
    "    },\n",
    "    'parameters' : {\n",
    "        'epochs': {\n",
    "            'values' : [50, 100 ,200]\n",
    "        },\n",
    "        'dropout' : {\n",
    "            'values': [0.8, 0.6 , 0.5, 0.4]\n",
    "        },\n",
    "        'learning_rate' : {\n",
    "            'values': [0.01, 0.001, 0.003, 0.1]\n",
    "        },\n",
    "        'optimizer' : {\n",
    "            'values' : ['adam',  'sgd']\n",
    "        },\n",
    "        'activation' : {\n",
    "            'values' : ['relu', 'swish', 'selu']\n",
    "        },\n",
    "        'nn_nodes': {\n",
    "            'values' : [[128,64],[128,64,32],[128,64,16],[128,32,16],[64,16], []]\n",
    "        },\n",
    "        'early_terminate' : {\n",
    "            'type': 'hyperband',\n",
    "            'max_iter' : 10,\n",
    "            's' : 2,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c8984a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new sweep\n",
    "# Arguments:\n",
    "#     – sweep_config: the sweep config dictionary defined above\n",
    "#     – entity: Set the username for the sweep\n",
    "#     – project: Set the project name for the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"dylanli\", project=\"Option-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35a0f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method' : 'bayes',\n",
    "    'metric' : {\n",
    "        'name' : 'rmse',\n",
    "        'goal' : 'minimize',\n",
    "    },\n",
    "    'parameters' : {\n",
    "        'epochs': {\n",
    "            'values' : [50, 100 ,200]\n",
    "        },\n",
    "        'dropout' : {\n",
    "            'values': [0.8, 0.6 , 0.5, 0.4]\n",
    "        },\n",
    "        'learning_rate' : {\n",
    "            'values': [0.01, 0.001, 0.003, 0.1]\n",
    "        },\n",
    "        'optimizer' : {\n",
    "            'values' : ['adam',  'sgd']\n",
    "        },\n",
    "        'activation' : {\n",
    "            'values' : ['relu', 'swish', 'selu']\n",
    "        },\n",
    "        'nn_nodes': {\n",
    "            'values' : [[128,64],[128,64,32],[128,64,16],[128,32,16],[64,16], []]\n",
    "        },\n",
    "        'early_terminate' : {\n",
    "            'type': 'hyperband',\n",
    "            'max_iter' : 10,\n",
    "            's' : 2,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ede7cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new sweep\n",
    "# Arguments:\n",
    "#     – sweep_config: the sweep config dictionary defined above\n",
    "#     – entity: Set the username for the sweep\n",
    "#     – project: Set the project name for the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"dylanli\", project=\"Option-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0110084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method' : 'bayes',\n",
    "    'metric' : {\n",
    "        'name' : 'rmse',\n",
    "        'goal' : 'minimize',\n",
    "    },\n",
    "    'parameters' : {\n",
    "        'epochs': {\n",
    "            'values' : [50, 100 ,200]\n",
    "        },\n",
    "        'dropout' : {\n",
    "            'values': [0.8, 0.6 , 0.5, 0.4]\n",
    "        },\n",
    "        'learning_rate' : {\n",
    "            'values': [0.01, 0.001, 0.003, 0.1]\n",
    "        },\n",
    "        'optimizer' : {\n",
    "            'values' : ['adam',  'sgd']\n",
    "        },\n",
    "        'activation' : {\n",
    "            'values' : ['relu', 'swish', 'selu']\n",
    "        },\n",
    "        'nn_nodes': {\n",
    "            'values' : [[128,64],[128,64,32],[128,64,16],[128,32,16],[64,16], []]\n",
    "        }\n",
    "    },\n",
    "    'early_terminate' : {\n",
    "        'type' : 'hyperband',\n",
    "        'max_iter' : 10,\n",
    "        's' : 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9cc48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new sweep\n",
    "# Arguments:\n",
    "#     – sweep_config: the sweep config dictionary defined above\n",
    "#     – entity: Set the username for the sweep\n",
    "#     – project: Set the project name for the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"dylanli\", project=\"Option-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e602f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_model(data=data, nfold=CONFIG.nfold,log=CONFIG.log_dir, is_train=True):\n",
    "    \n",
    "    config_defaults = {\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'dropout': 0.5,\n",
    "    'activation_type': 'swish',\n",
    "    'optimizer': 'adam',\n",
    "    'seed': 42,\n",
    "    'learning_rate': 1e-3,\n",
    "    'nn_nodes' : [32, 32, 32],\n",
    "    }\n",
    "\n",
    "    if data.underlying.value_counts().shape[0] == 1:\n",
    "        run = wandb.init(config=config_defaults, project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'nn_single')\n",
    "    else : \n",
    "        run = wandb.init(config=config_defaults, project=\"Option-project\", entity=\"dylanli\", reinit=True, name=f'nn_multi')\n",
    "    \n",
    "    \n",
    "    config = wandb.config\n",
    "    \n",
    "    # Deine the optimizer\n",
    "    if config.optimizer=='sgd':\n",
    "        optimizer = keras.optimizers.SGD(lr=config.learning_rate, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "    elif config.optimizer=='adam':\n",
    "        optimizer = keras.optimizers.Adam(lr=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "        \n",
    "    def dense_block(x, n_nodes, p=0.5, activation='swish'):\n",
    "        x = layers.Dropout(p)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "        return x\n",
    "    \n",
    "    def get_nn(dense_blocks, optimizer, dropout):\n",
    "        input_ = layers.Input(shape=(len(features),))\n",
    "        x = layers.BatchNormalization()(input_)\n",
    "        x = layers.Dense(256, activation='swish')(x)\n",
    "        \n",
    "        #TODO 多层连接\n",
    "        if len(dense_blocks) >= 1:\n",
    "            p = dropout\n",
    "            for units in dense_blocks:\n",
    "                x = dense_block(x, units, p)\n",
    "                p -= 0.05\n",
    "            \n",
    "        output = layers.Dense(1)(x)\n",
    "        \n",
    "        model = keras.Model(input_, output)\n",
    "            \n",
    "        model.compile(optimizer, loss=tf.keras.losses.MeanAbsoluteError(name='val_loss'), metrics=[tf.keras.losses.MeanAbsoluteError(name='val_loss')])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    models = []\n",
    "    scores = []\n",
    "    if is_train:\n",
    "        for f in range(nfold):\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_idx = data[data.fold != f].index\n",
    "            val_idx = data[data.fold == f].index\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            \n",
    "            train_x = scaler.fit_transform(data.iloc[train_idx, :][features])\n",
    "            train_y = data.iloc[train_idx, :]['settlement']\n",
    "            \n",
    "            val_x = scaler.fit_transform(data.iloc[val_idx, :][features])\n",
    "            val_y = np.array(data.iloc[val_idx, :]['settlement'])\n",
    "            \n",
    "            checkpoint = keras.callbacks.ModelCheckpoint(filepath=f'/Users/dylan/DylanLi/XJTLU/期权项目/Code/model_nn/model_nn_{f}.hdf5', save_best_only=True)\n",
    "            early_stop = keras.callbacks.EarlyStopping(patience=config.epochs//4)\n",
    "            tesnsorboard = keras.callbacks.TensorBoard(log, histogram_freq=1)\n",
    "            model = get_nn(config.nn_nodes, optimizer, config.dropout)\n",
    "            print(f\"{'=' * 20} fold_{f} Training {'=' * 20}\")\n",
    "            history = model.fit(train_x, train_y, epochs=config.epochs, batch_size=config.batch_size, validation_data=(val_x, val_y), callbacks=[checkpoint, early_stop, WandbCallback()], validation_freq=[1,1,1])\n",
    "            \n",
    "            pd.DataFrame(history.history, columns=['loss', 'val_loss']).plot()\n",
    "            plt.title(\"MSE\")\n",
    "            plt.show()\n",
    "            #loss, mae, mse = model.evaluate(val_x, val_y)\n",
    "            #print(f\"evaluation: loss: {loss}, mae: {mae}, mse: {mse}\")\n",
    "            \n",
    "            model.load_weights(f'/Users/dylan/DylanLi/XJTLU/期权项目/Code/model_nn/model_nn_{f}.hdf5')\n",
    "            off_pred = model.predict(val_x)\n",
    "            off_score = np.sqrt(mean_squared_error(val_y, off_pred))\n",
    "            #off_score = 0\n",
    "            \n",
    "            scores.append(off_score)\n",
    "            print(f\"{'=' * 20} fold_{f} score {'=' * 20}\")\n",
    "            print(f\"rmse: {off_score}\")\n",
    "            \n",
    "            del train_x, train_y, val_x, val_y\n",
    "            gc.collect()\n",
    "            \n",
    "            run.finish()\n",
    "            break\n",
    "        \n",
    "    #wandb.log({\"Fold scores\": scores})\n",
    "    #return scores, models\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6436d97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dylan/iCollections/桌面文件/Repo/Option_Pricing/Code/wandb/run-20220523_150550-oce8byez</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dylanli/Option-project/runs/oce8byez\" target=\"_blank\">nn_multi</a></strong> to <a href=\"https://wandb.ai/dylanli/Option-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/dylanli/Option-project/sweeps/q505v42d\" target=\"_blank\">https://wandb.ai/dylanli/Option-project/sweeps/q505v42d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=nn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
