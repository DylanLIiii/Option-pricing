/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
2022-05-10 15:32:45.856089: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf16cc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf16cc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

 82/110 [=====================>........] - ETA: 0s - loss: 12.5062 - val_loss: 12.5062
110/110 [==============================] - ETA: 0s - loss: 11.9512 - val_loss: 11.8776WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x35f9909d0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x35f9909d0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 5s 30ms/step - loss: 11.9512 - val_loss: 11.4272 - val_val_loss: 11.3915 - _timestamp: 1652167969.0000 - _runtime: 8.0000
Epoch 2/200
110/110 [==============================] - 2s 22ms/step - loss: 10.5683 - val_loss: 10.6781 - _timestamp: 1652167972.0000 - _runtime: 11.0000
Epoch 3/200
110/110 [==============================] - 2s 20ms/step - loss: 10.3565 - val_loss: 11.3900 - _timestamp: 1652167974.0000 - _runtime: 13.0000
Epoch 4/200
110/110 [==============================] - 2s 20ms/step - loss: 10.3678 - val_loss: 10.4212 - _timestamp: 1652167976.0000 - _runtime: 15.0000
Epoch 5/200
110/110 [==============================] - 2s 20ms/step - loss: 10.4336 - val_loss: 10.5544 - _timestamp: 1652167978.0000 - _runtime: 17.0000
Epoch 6/200

110/110 [==============================] - 2s 20ms/step - loss: 10.2784 - val_loss: 10.2951 - _timestamp: 1652167981.0000 - _runtime: 20.0000
Epoch 7/200
110/110 [==============================] - 2s 21ms/step - loss: 10.3359 - val_loss: 10.2740 - _timestamp: 1652167983.0000 - _runtime: 22.0000
Epoch 8/200
110/110 [==============================] - 2s 19ms/step - loss: 10.5565 - val_loss: 10.4850 - _timestamp: 1652167985.0000 - _runtime: 24.0000
Epoch 9/200
110/110 [==============================] - 2s 20ms/step - loss: 10.4092 - val_loss: 10.3938 - _timestamp: 1652167987.0000 - _runtime: 26.0000
Epoch 10/200
110/110 [==============================] - 2s 20ms/step - loss: 10.2873 - val_loss: 10.1995 - _timestamp: 1652167989.0000 - _runtime: 28.0000
Epoch 11/200
110/110 [==============================] - 2s 20ms/step - loss: 10.3530 - val_loss: 10.3793 - _timestamp: 1652167992.0000 - _runtime: 31.0000
Epoch 12/200
110/110 [==============================] - 2s 19ms/step - loss: 10.3309 - val_loss: 10.2576 - _timestamp: 1652167994.0000 - _runtime: 33.0000
Epoch 13/200
110/110 [==============================] - 2s 20ms/step - loss: 10.1818 - val_loss: 10.5489 - _timestamp: 1652167996.0000 - _runtime: 35.0000
Epoch 14/200
110/110 [==============================] - 2s 20ms/step - loss: 10.2655 - val_loss: 10.6140 - _timestamp: 1652167998.0000 - _runtime: 37.0000
Epoch 15/200
110/110 [==============================] - 2s 19ms/step - loss: 10.2328 - val_loss: 10.2018 - _timestamp: 1652168000.0000 - _runtime: 39.0000
Epoch 16/200
110/110 [==============================] - 2s 19ms/step - loss: 10.3584 - val_loss: 10.2927 - _timestamp: 1652168002.0000 - _runtime: 41.0000
Epoch 17/200
110/110 [==============================] - 2s 19ms/step - loss: 10.2007 - val_loss: 10.2077 - _timestamp: 1652168005.0000 - _runtime: 44.0000
Epoch 18/200
110/110 [==============================] - 2s 19ms/step - loss: 10.3305 - val_loss: 11.8507 - _timestamp: 1652168007.0000 - _runtime: 46.0000
Epoch 19/200

110/110 [==============================] - 2s 20ms/step - loss: 10.2111 - val_loss: 10.1348 - _timestamp: 1652168009.0000 - _runtime: 48.0000
Epoch 20/200
110/110 [==============================] - 2s 19ms/step - loss: 10.2151 - val_loss: 10.2159 - _timestamp: 1652168011.0000 - _runtime: 50.0000
Epoch 21/200
110/110 [==============================] - 2s 20ms/step - loss: 10.1693 - val_loss: 10.0884 - _timestamp: 1652168013.0000 - _runtime: 52.0000
Epoch 22/200
110/110 [==============================] - 2s 19ms/step - loss: 10.2813 - val_loss: 10.2535 - _timestamp: 1652168015.0000 - _runtime: 54.0000
Epoch 23/200
110/110 [==============================] - 2s 19ms/step - loss: 10.1887 - val_loss: 10.2142 - _timestamp: 1652168018.0000 - _runtime: 57.0000
Epoch 24/200
110/110 [==============================] - 2s 19ms/step - loss: 10.2344 - val_loss: 10.3384 - _timestamp: 1652168020.0000 - _runtime: 59.0000
Epoch 25/200
110/110 [==============================] - 2s 20ms/step - loss: 10.2198 - val_loss: 10.1480 - _timestamp: 1652168022.0000 - _runtime: 61.0000
Epoch 26/200
110/110 [==============================] - 2s 20ms/step - loss: 10.2661 - val_loss: 10.1920 - _timestamp: 1652168024.0000 - _runtime: 63.0000
Epoch 27/200
110/110 [==============================] - 2s 20ms/step - loss: 10.2107 - val_loss: 10.1403 - _timestamp: 1652168026.0000 - _runtime: 65.0000
Epoch 28/200
110/110 [==============================] - 2s 19ms/step - loss: 10.3296 - val_loss: 10.3023 - _timestamp: 1652168028.0000 - _runtime: 67.0000
Epoch 29/200
110/110 [==============================] - 2s 19ms/step - loss: 10.1836 - val_loss: 10.2018 - _timestamp: 1652168030.0000 - _runtime: 69.0000
Epoch 30/200
110/110 [==============================] - 2s 19ms/step - loss: 10.3033 - val_loss: 10.3511 - _timestamp: 1652168033.0000 - _runtime: 72.0000
Epoch 31/200

110/110 [==============================] - 2s 21ms/step - loss: 10.1075 - val_loss: 10.0328 - _timestamp: 1652168035.0000 - _runtime: 74.0000
Epoch 32/200
110/110 [==============================] - 2s 20ms/step - loss: 10.2608 - val_loss: 10.1872 - _timestamp: 1652168037.0000 - _runtime: 76.0000
Epoch 33/200
110/110 [==============================] - 2s 20ms/step - loss: 10.1488 - val_loss: 10.0765 - _timestamp: 1652168039.0000 - _runtime: 78.0000
Epoch 34/200
110/110 [==============================] - 2s 20ms/step - loss: 10.1733 - val_loss: 10.2307 - _timestamp: 1652168041.0000 - _runtime: 80.0000
Epoch 35/200
110/110 [==============================] - 2s 20ms/step - loss: 10.0212 - val_loss: 10.0041 - _timestamp: 1652168044.0000 - _runtime: 83.0000
Epoch 36/200
110/110 [==============================] - 2s 20ms/step - loss: 10.2186 - val_loss: 10.1547 - _timestamp: 1652168046.0000 - _runtime: 85.0000
Epoch 37/200
110/110 [==============================] - 2s 19ms/step - loss: 9.9757 - val_loss: 10.8744 - _timestamp: 1652168048.0000 - _runtime: 87.0000
Epoch 38/200
110/110 [==============================] - 2s 20ms/step - loss: 10.1894 - val_loss: 10.1124 - _timestamp: 1652168050.0000 - _runtime: 89.0000
Epoch 39/200
110/110 [==============================] - 2s 20ms/step - loss: 10.1547 - val_loss: 10.2458 - _timestamp: 1652168052.0000 - _runtime: 91.0000
Epoch 40/200
110/110 [==============================] - 2s 20ms/step - loss: 10.4180 - val_loss: 10.3771 - _timestamp: 1652168054.0000 - _runtime: 93.0000
Epoch 41/200
110/110 [==============================] - 2s 20ms/step - loss: 10.1716 - val_loss: 10.1264 - _timestamp: 1652168057.0000 - _runtime: 96.0000
Epoch 42/200
100/110 [==========================>...] - ETA: 0s - loss: 10.5213 - val_loss: 10.5213
 97/110 [=========================>....] - ETA: 0s - loss: 9.8276 - val_loss: 9.8276  .5170 - _timestamp: 1652168059.0000 - _runtime: 98.0000
Epoch 44/200===========================] - 2s 20ms/step - loss: 10.0766 - val_loss: 9.9915 - _timestamp: 1652168061.0000 - _runtime: 100.0000
Epoch 44/200===========================] - 2s 20ms/step - loss: 10.0766 - val_loss: 9.9915 - _timestamp: 1652168061.0000 - _runtime: 100.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.2493 - val_loss: 10.2493.0819 - _timestamp: 1652168063.0000 - _runtime: 102.0000
Epoch 45/200
 67/110 [=================>............] - ETA: 0s - loss: 10.1982 - val_loss: 10.19829679 - _timestamp: 1652168065.0000 - _runtime: 104.00000
Epoch 46/200
 52/110 [=============>................] - ETA: 1s - loss: 10.5656 - val_loss: 10.5656259 - _timestamp: 1652168068.0000 - _runtime: 107.000000
Epoch 47/200
 46/110 [===========>..................] - ETA: 1s - loss: 9.3007 - val_loss: 9.3007  .1239 - _timestamp: 1652168070.0000 - _runtime: 109.0000
Epoch 48/200
 34/110 [========>.....................] - ETA: 1s - loss: 10.6263 - val_loss: 10.6263144 - _timestamp: 1652168072.0000 - _runtime: 111.000000
Epoch 49/200
 28/110 [======>.......................] - ETA: 1s - loss: 10.9448 - val_loss: 10.9448.1642 - _timestamp: 1652168074.0000 - _runtime: 113.0000
Epoch 50/200
 19/110 [====>.........................] - ETA: 1s - loss: 9.6540 - val_loss: 9.654010.3341 - _timestamp: 1652168076.0000 - _runtime: 115.0000
Epoch 51/200
 13/110 [==>...........................] - ETA: 1s - loss: 11.0775 - val_loss: 11.0775.2206 - _timestamp: 1652168079.0000 - _runtime: 118.0000
Epoch 52/200
  7/110 [>.............................] - ETA: 2s - loss: 7.6953 - val_loss: 7.6953  .1552 - _timestamp: 1652168081.0000 - _runtime: 120.0000
Epoch 53/200
  1/110 [..............................] - ETA: 2s - loss: 12.2877 - val_loss: 12.2877.1012 - _timestamp: 1652168083.0000 - _runtime: 122.0000
Epoch 54/200
106/110 [===========================>..] - ETA: 0s - loss: 9.9594 - val_loss: 9.9594  .1012 - _timestamp: 1652168083.0000 - _runtime: 122.0000
100/110 [==========================>...] - ETA: 0s - loss: 10.3567 - val_loss: 10.35679572 - _timestamp: 1652168085.0000 - _runtime: 124.00000
 91/110 [=======================>......] - ETA: 0s - loss: 10.2819 - val_loss: 10.2819.4108 - _timestamp: 1652168087.0000 - _runtime: 126.0000
 79/110 [====================>.........] - ETA: 0s - loss: 10.7023 - val_loss: 10.7023.2290 - _timestamp: 1652168089.0000 - _runtime: 128.0000
 70/110 [==================>...........] - ETA: 0s - loss: 10.4602 - val_loss: 10.4602.1711 - _timestamp: 1652168092.0000 - _runtime: 131.0000
 63/110 [================>.............] - ETA: 0s - loss: 10.0787 - val_loss: 10.07871742 - _timestamp: 1652168094.0000 - _runtime: 133.00000
 55/110 [==============>...............] - ETA: 1s - loss: 9.5254 - val_loss: 9.5254  .1217 - _timestamp: 1652168096.0000 - _runtime: 135.0000
 46/110 [===========>..................] - ETA: 1s - loss: 10.5665 - val_loss: 10.56659623 - _timestamp: 1652168098.0000 - _runtime: 137.00000
 37/110 [=========>....................] - ETA: 1s - loss: 9.4504 - val_loss: 9.450410.2455 - _timestamp: 1652168100.0000 - _runtime: 139.0000
 28/110 [======>.......................] - ETA: 1s - loss: 10.6625 - val_loss: 10.6625.1544 - _timestamp: 1652168103.0000 - _runtime: 142.0000
109/110 [============================>.] - ETA: 0s - loss: 10.0749 - val_loss: 10.0749.1544 - _timestamp: 1652168103.0000 - _runtime: 142.0000
103/110 [===========================>..] - ETA: 0s - loss: 10.2176 - val_loss: 10.21769883 - _timestamp: 1652168105.0000 - _runtime: 144.00000
 94/110 [========================>.....] - ETA: 0s - loss: 10.0212 - val_loss: 10.0212.0712 - _timestamp: 1652168107.0000 - _runtime: 146.0000
 85/110 [======================>.......] - ETA: 0s - loss: 9.6930 - val_loss: 9.6930  .0594 - _timestamp: 1652168109.0000 - _runtime: 148.0000
 78/110 [====================>.........] - ETA: 0s - loss: 9.8911 - val_loss: 9.8911  671 - _timestamp: 1652168111.0000 - _runtime: 150.000000
 70/110 [==================>...........] - ETA: 0s - loss: 10.4351 - val_loss: 10.4351.2201 - _timestamp: 1652168113.0000 - _runtime: 152.0000
 64/110 [================>.............] - ETA: 0s - loss: 10.6279 - val_loss: 10.6279.4551 - _timestamp: 1652168116.0000 - _runtime: 155.0000
 58/110 [==============>...............] - ETA: 1s - loss: 11.1743 - val_loss: 11.1743.0648 - _timestamp: 1652168118.0000 - _runtime: 157.0000
 49/110 [============>.................] - ETA: 1s - loss: 11.1831 - val_loss: 11.1831.4839 - _timestamp: 1652168120.0000 - _runtime: 159.0000
 43/110 [==========>...................] - ETA: 1s - loss: 9.3149 - val_loss: 9.314910.1551 - _timestamp: 1652168122.0000 - _runtime: 161.0000
 37/110 [=========>....................] - ETA: 1s - loss: 11.0373 - val_loss: 11.0373.6175 - _timestamp: 1652168124.0000 - _runtime: 163.0000
 31/110 [=======>......................] - ETA: 1s - loss: 10.0139 - val_loss: 10.0139.2852 - _timestamp: 1652168126.0000 - _runtime: 165.0000
 22/110 [=====>........................] - ETA: 1s - loss: 9.6095 - val_loss: 9.6095  .0728 - _timestamp: 1652168129.0000 - _runtime: 168.0000
 13/110 [==>...........................] - ETA: 1s - loss: 11.4592 - val_loss: 11.4592.6195 - _timestamp: 1652168131.0000 - _runtime: 170.0000
  7/110 [>.............................] - ETA: 1s - loss: 10.1007 - val_loss: 10.1007.1666 - _timestamp: 1652168133.0000 - _runtime: 172.0000
108/110 [============================>.] - ETA: 0s - loss: 10.0586 - val_loss: 10.0586.1666 - _timestamp: 1652168133.0000 - _runtime: 172.0000
103/110 [===========================>..] - ETA: 0s - loss: 10.1443 - val_loss: 10.14439878 - _timestamp: 1652168135.0000 - _runtime: 174.00000
 94/110 [========================>.....] - ETA: 0s - loss: 10.1977 - val_loss: 10.1977.2759 - _timestamp: 1652168137.0000 - _runtime: 176.0000
 84/110 [=====================>........] - ETA: 0s - loss: 10.2147 - val_loss: 10.2147.2861 - _timestamp: 1652168139.0000 - _runtime: 178.0000
 75/110 [===================>..........] - ETA: 0s - loss: 9.9625 - val_loss: 9.9625  .0916 - _timestamp: 1652168142.0000 - _runtime: 181.0000
 67/110 [=================>............] - ETA: 0s - loss: 9.9993 - val_loss: 9.9993  .2198 - _timestamp: 1652168144.0000 - _runtime: 183.0000
 34/110 [========>.....................] - ETA: 1s - loss: 9.5248 - val_loss: 9.524812.3447 - _timestamp: 1652168146.0000 - _runtime: 185.0000
 28/110 [======>.......................] - ETA: 1s - loss: 11.1176 - val_loss: 11.1176403 - _timestamp: 1652168148.0000 - _runtime: 187.000000
 19/110 [====>.........................] - ETA: 1s - loss: 8.3106 - val_loss: 8.3106  .0560 - _timestamp: 1652168150.0000 - _runtime: 189.0000
 16/110 [===>..........................] - ETA: 1s - loss: 12.7142 - val_loss: 12.7142.1427 - _timestamp: 1652168153.0000 - _runtime: 192.0000
  7/110 [>.............................] - ETA: 2s - loss: 11.2632 - val_loss: 11.2632.1750 - _timestamp: 1652168155.0000 - _runtime: 194.0000
  4/110 [>.............................] - ETA: 2s - loss: 9.2299 - val_loss: 9.229910.0358 - _timestamp: 1652168157.0000 - _runtime: 196.0000
106/110 [===========================>..] - ETA: 0s - loss: 10.1121 - val_loss: 10.1121.0358 - _timestamp: 1652168157.0000 - _runtime: 196.0000
100/110 [==========================>...] - ETA: 0s - loss: 9.9817 - val_loss: 9.9817  .0920 - _timestamp: 1652168159.0000 - _runtime: 198.0000
 91/110 [=======================>......] - ETA: 0s - loss: 10.3323 - val_loss: 10.33239985 - _timestamp: 1652168161.0000 - _runtime: 200.00000
 85/110 [======================>.......] - ETA: 0s - loss: 10.3396 - val_loss: 10.3396.1995 - _timestamp: 1652168163.0000 - _runtime: 202.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.1482 - val_loss: 10.1482.1876 - _timestamp: 1652168166.0000 - _runtime: 205.0000
 67/110 [=================>............] - ETA: 0s - loss: 9.8653 - val_loss: 9.8653  .0845 - _timestamp: 1652168168.0000 - _runtime: 207.0000
 61/110 [===============>..............] - ETA: 0s - loss: 10.0208 - val_loss: 10.0208.0284 - _timestamp: 1652168170.0000 - _runtime: 209.0000
 52/110 [=============>................] - ETA: 1s - loss: 9.7647 - val_loss: 9.7647  .4972 - _timestamp: 1652168172.0000 - _runtime: 211.0000
 43/110 [==========>...................] - ETA: 1s - loss: 9.1793 - val_loss: 9.179310.0127 - _timestamp: 1652168174.0000 - _runtime: 213.0000
 34/110 [========>.....................] - ETA: 1s - loss: 9.2619 - val_loss: 9.2619  .0048 - _timestamp: 1652168176.0000 - _runtime: 215.0000
 28/110 [======>.......................] - ETA: 1s - loss: 10.9805 - val_loss: 10.9805.0277 - _timestamp: 1652168179.0000 - _runtime: 218.0000
 28/110 [======>.......................] - ETA: 1s - loss: 10.9805 - val_loss: 10.9805.0277 - _timestamp: 1652168179.0000 - _runtime: 218.0000
106/110 [===========================>..] - ETA: 0s - loss: 9.9048 - val_loss: 9.9048  .0277 - _timestamp: 1652168179.0000 - _runtime: 218.0000
rmse: 31.313027462109563 decorate the function with @tf.autograph.experimental.do_not_convertinux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
rmse: 31.313027462109563 decorate the function with @tf.autograph.experimental.do_not_convertinux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.