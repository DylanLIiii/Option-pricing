==================== fold_0 Training ====================
Epoch 1/100
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x175d1d430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x175d1d430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 13/110 [==>...........................] - ETA: 2s - loss: 15.6139 - val_loss: 15.6139
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.

110/110 [==============================] - ETA: 0s - loss: 14.5502 - val_loss: 14.4339WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2eab383a0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2eab383a0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 5s 36ms/step - loss: 14.5502 - val_loss: 11.8195 - val_val_loss: 11.7963 - _timestamp: 1652158493.0000 - _runtime: 9.0000
Epoch 2/100
 22/110 [=====>........................] - ETA: 1s - loss: 11.4671 - val_loss: 11.4671
110/110 [==============================] - 2s 20ms/step - loss: 12.7318 - val_loss: 12.6535 - _timestamp: 1652158495.0000 - _runtime: 11.0000
Epoch 3/100
110/110 [==============================] - 2s 20ms/step - loss: 11.5458 - val_loss: 11.4787 - _timestamp: 1652158497.0000 - _runtime: 13.0000
Epoch 4/100

110/110 [==============================] - 2s 19ms/step - loss: 11.1641 - val_loss: 13.4644 - _timestamp: 1652158499.0000 - _runtime: 15.0000
Epoch 5/100
110/110 [==============================] - 2s 20ms/step - loss: 10.8656 - val_loss: 10.7775 - _timestamp: 1652158502.0000 - _runtime: 18.0000
Epoch 6/100
110/110 [==============================] - 2s 19ms/step - loss: 10.9589 - val_loss: 11.0162 - _timestamp: 1652158504.0000 - _runtime: 20.0000
Epoch 7/100
110/110 [==============================] - 2s 20ms/step - loss: 10.8607 - val_loss: 10.7819 - _timestamp: 1652158506.0000 - _runtime: 22.0000
Epoch 8/100
110/110 [==============================] - 2s 19ms/step - loss: 10.7350 - val_loss: 11.5814 - _timestamp: 1652158508.0000 - _runtime: 24.0000
Epoch 9/100
110/110 [==============================] - 2s 21ms/step - loss: 10.7686 - val_loss: 10.7071 - _timestamp: 1652158510.0000 - _runtime: 26.0000
Epoch 10/100
110/110 [==============================] - 2s 21ms/step - loss: 10.6927 - val_loss: 10.6334 - _timestamp: 1652158513.0000 - _runtime: 29.0000
Epoch 11/100
110/110 [==============================] - 2s 22ms/step - loss: 10.6021 - val_loss: 10.5433 - _timestamp: 1652158515.0000 - _runtime: 31.0000
Epoch 12/100

110/110 [==============================] - 3s 24ms/step - loss: 10.5652 - val_loss: 10.5037 - _timestamp: 1652158518.0000 - _runtime: 34.0000
Epoch 13/100
110/110 [==============================] - 2s 23ms/step - loss: 10.4714 - val_loss: 10.5956 - _timestamp: 1652158520.0000 - _runtime: 36.0000
Epoch 14/100
110/110 [==============================] - 2s 21ms/step - loss: 10.6272 - val_loss: 10.5660 - _timestamp: 1652158523.0000 - _runtime: 39.0000
Epoch 15/100
110/110 [==============================] - 2s 21ms/step - loss: 10.4228 - val_loss: 12.4742 - _timestamp: 1652158525.0000 - _runtime: 41.0000
Epoch 16/100
110/110 [==============================] - 2s 21ms/step - loss: 10.5563 - val_loss: 10.9883 - _timestamp: 1652158527.0000 - _runtime: 43.0000
Epoch 17/100
110/110 [==============================] - 2s 20ms/step - loss: 10.5951 - val_loss: 10.6057 - _timestamp: 1652158529.0000 - _runtime: 45.0000
Epoch 18/100

110/110 [==============================] - 2s 22ms/step - loss: 10.4505 - val_loss: 10.3638 - _timestamp: 1652158532.0000 - _runtime: 48.0000
Epoch 19/100
110/110 [==============================] - 2s 21ms/step - loss: 10.6919 - val_loss: 10.6356 - _timestamp: 1652158534.0000 - _runtime: 50.0000
Epoch 20/100
110/110 [==============================] - 2s 19ms/step - loss: 10.6574 - val_loss: 10.5971 - _timestamp: 1652158536.0000 - _runtime: 52.0000
Epoch 21/100
110/110 [==============================] - 2s 20ms/step - loss: 10.6072 - val_loss: 10.6411 - _timestamp: 1652158539.0000 - _runtime: 55.0000
Epoch 22/100
110/110 [==============================] - 2s 20ms/step - loss: 10.5776 - val_loss: 10.5750 - _timestamp: 1652158541.0000 - _runtime: 57.0000
Epoch 23/100
110/110 [==============================] - 2s 21ms/step - loss: 10.4618 - val_loss: 10.4306 - _timestamp: 1652158543.0000 - _runtime: 59.0000
Epoch 24/100
110/110 [==============================] - 2s 21ms/step - loss: 10.6207 - val_loss: 10.5747 - _timestamp: 1652158545.0000 - _runtime: 61.0000
Epoch 25/100

110/110 [==============================] - 3s 24ms/step - loss: 10.3927 - val_loss: 10.3366 - _timestamp: 1652158548.0000 - _runtime: 64.0000
Epoch 26/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4839 - val_loss: 10.5293 - _timestamp: 1652158550.0000 - _runtime: 66.0000
Epoch 27/100
110/110 [==============================] - 2s 21ms/step - loss: 10.3214 - val_loss: 10.2388 - _timestamp: 1652158553.0000 - _runtime: 69.0000
Epoch 28/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4876 - val_loss: 10.4286 - _timestamp: 1652158555.0000 - _runtime: 71.0000
Epoch 29/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4149 - val_loss: 10.3589 - _timestamp: 1652158557.0000 - _runtime: 73.0000
Epoch 30/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4507 - val_loss: 10.3613 - _timestamp: 1652158559.0000 - _runtime: 75.0000
Epoch 31/100

110/110 [==============================] - 2s 20ms/step - loss: 10.4475 - val_loss: 10.3804 - _timestamp: 1652158561.0000 - _runtime: 77.0000
Epoch 32/100
110/110 [==============================] - 2s 20ms/step - loss: 10.5704 - val_loss: 10.5054 - _timestamp: 1652158564.0000 - _runtime: 80.0000
Epoch 33/100
110/110 [==============================] - 2s 20ms/step - loss: 10.3998 - val_loss: 10.3425 - _timestamp: 1652158566.0000 - _runtime: 82.0000
Epoch 34/100
110/110 [==============================] - 2s 19ms/step - loss: 10.4434 - val_loss: 10.3751 - _timestamp: 1652158568.0000 - _runtime: 84.0000
Epoch 35/100
110/110 [==============================] - 2s 19ms/step - loss: 10.3110 - val_loss: 10.2435 - _timestamp: 1652158570.0000 - _runtime: 86.0000
Epoch 36/100
110/110 [==============================] - 2s 19ms/step - loss: 10.4186 - val_loss: 10.3507 - _timestamp: 1652158572.0000 - _runtime: 88.0000
Epoch 37/100
110/110 [==============================] - 2s 19ms/step - loss: 10.4210 - val_loss: 10.3365 - _timestamp: 1652158574.0000 - _runtime: 90.0000
Epoch 38/100
110/110 [==============================] - 2s 21ms/step - loss: 10.4066 - val_loss: 10.8674 - _timestamp: 1652158577.0000 - _runtime: 93.0000
Epoch 39/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4028 - val_loss: 10.3218 - _timestamp: 1652158579.0000 - _runtime: 95.0000
Epoch 40/100
110/110 [==============================] - 2s 19ms/step - loss: 10.4088 - val_loss: 10.3427 - _timestamp: 1652158581.0000 - _runtime: 97.0000
Epoch 41/100
110/110 [==============================] - 2s 21ms/step - loss: 10.3147 - val_loss: 10.2286 - _timestamp: 1652158583.0000 - _runtime: 99.0000
Epoch 42/100
Epoch 43/100===========================] - 2s 22ms/step - loss: 10.5102 - val_loss: 10.4454 - _timestamp: 1652158586.0000 - _runtime: 102.0000
Epoch 43/100===========================] - 2s 22ms/step - loss: 10.5102 - val_loss: 10.4454 - _timestamp: 1652158586.0000 - _runtime: 102.0000
103/110 [===========================>..] - ETA: 0s - loss: 10.5626 - val_loss: 10.5626
 92/110 [========================>.....] - ETA: 0s - loss: 10.1525 - val_loss: 10.1525.4041 - _timestamp: 1652158588.0000 - _runtime: 104.0000
 88/110 [=======================>......] - ETA: 0s - loss: 10.4689 - val_loss: 10.4689.3587 - _timestamp: 1652158590.0000 - _runtime: 106.0000
 78/110 [====================>.........] - ETA: 0s - loss: 10.6999 - val_loss: 10.6999.4171 - _timestamp: 1652158592.0000 - _runtime: 108.0000
 70/110 [==================>...........] - ETA: 0s - loss: 10.1458 - val_loss: 10.1458.2968 - _timestamp: 1652158594.0000 - _runtime: 110.0000
 43/110 [==========>...................] - ETA: 1s - loss: 8.9602 - val_loss: 8.9602  .5380 - _timestamp: 1652158597.0000 - _runtime: 113.0000
 22/110 [=====>........................] - ETA: 1s - loss: 11.0760 - val_loss: 11.0760.3020 - _timestamp: 1652158599.0000 - _runtime: 115.0000
  1/110 [..............................] - ETA: 3s - loss: 11.5614 - val_loss: 11.5614.3313 - _timestamp: 1652158602.0000 - _runtime: 118.0000
 99/110 [==========================>...] - ETA: 0s - loss: 10.5894 - val_loss: 10.5894.3313 - _timestamp: 1652158602.0000 - _runtime: 118.0000
 84/110 [=====================>........] - ETA: 0s - loss: 10.0866 - val_loss: 10.0866.2831 - _timestamp: 1652158604.0000 - _runtime: 120.0000
 82/110 [=====================>........] - ETA: 0s - loss: 10.1422 - val_loss: 10.1422.2401 - _timestamp: 1652158606.0000 - _runtime: 122.0000
 73/110 [==================>...........] - ETA: 0s - loss: 9.6049 - val_loss: 9.6049  .2749 - _timestamp: 1652158608.0000 - _runtime: 124.0000
 64/110 [================>.............] - ETA: 0s - loss: 10.6208 - val_loss: 10.6208.5824 - _timestamp: 1652158611.0000 - _runtime: 127.0000
 61/110 [===============>..............] - ETA: 0s - loss: 10.5644 - val_loss: 10.5644.2644 - _timestamp: 1652158613.0000 - _runtime: 129.0000
 49/110 [============>.................] - ETA: 1s - loss: 10.7448 - val_loss: 10.7448.1219 - _timestamp: 1652158615.0000 - _runtime: 131.0000
 46/110 [===========>..................] - ETA: 1s - loss: 9.9523 - val_loss: 9.9523  .3496 - _timestamp: 1652158617.0000 - _runtime: 133.0000
 15/110 [===>..........................] - ETA: 1s - loss: 9.2726 - val_loss: 9.2726  .2918 - _timestamp: 1652158619.0000 - _runtime: 135.0000
  7/110 [>.............................] - ETA: 1s - loss: 12.1637 - val_loss: 12.1637.1964 - _timestamp: 1652158621.0000 - _runtime: 137.0000
  1/110 [..............................] - ETA: 2s - loss: 6.6634 - val_loss: 6.663410.1414 - _timestamp: 1652158623.0000 - _runtime: 139.0000
110/110 [==============================] - 2s 19ms/step - loss: 10.2451 - val_loss: 11.0194 - _timestamp: 1652158626.0000 - _runtime: 142.0000
  1/110 [..............................] - ETA: 2s - loss: 9.4851 - val_loss: 9.485110.3818 - _timestamp: 1652158627.0000 - _runtime: 143.0000
110/110 [==============================] - 2s 19ms/step - loss: 10.3131 - val_loss: 10.2313 - _timestamp: 1652158630.0000 - _runtime: 146.0000
110/110 [==============================] - 2s 18ms/step - loss: 10.2219 - val_loss: 10.1534 - _timestamp: 1652158632.0000 - _runtime: 148.0000
110/110 [==============================] - 2s 18ms/step - loss: 10.2433 - val_loss: 10.3479 - _timestamp: 1652158634.0000 - _runtime: 150.0000
  1/110 [..............................] - ETA: 2s - loss: 23.5516 - val_loss: 23.5516.2267 - _timestamp: 1652158636.0000 - _runtime: 152.0000
104/110 [===========================>..] - ETA: 0s - loss: 10.3026 - val_loss: 10.3026.2267 - _timestamp: 1652158636.0000 - _runtime: 152.0000
 89/110 [=======================>......] - ETA: 0s - loss: 10.1273 - val_loss: 10.1273.2311 - _timestamp: 1652158638.0000 - _runtime: 154.0000
 91/110 [=======================>......] - ETA: 0s - loss: 10.5280 - val_loss: 10.5280.3236 - _timestamp: 1652158640.0000 - _runtime: 156.0000
 91/110 [=======================>......] - ETA: 0s - loss: 10.6408 - val_loss: 10.6408.4832 - _timestamp: 1652158642.0000 - _runtime: 158.0000
 87/110 [======================>.......] - ETA: 0s - loss: 10.4913 - val_loss: 10.4913.4376 - _timestamp: 1652158644.0000 - _runtime: 160.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.5074 - val_loss: 10.5074.1451 - _timestamp: 1652158646.0000 - _runtime: 162.0000
 64/110 [================>.............] - ETA: 0s - loss: 9.7204 - val_loss: 9.7204  .3490 - _timestamp: 1652158648.0000 - _runtime: 164.0000
 52/110 [=============>................] - ETA: 1s - loss: 8.9776 - val_loss: 8.9776  .2467 - _timestamp: 1652158651.0000 - _runtime: 167.0000
 37/110 [=========>....................] - ETA: 1s - loss: 9.1871 - val_loss: 9.1871  .2036 - _timestamp: 1652158653.0000 - _runtime: 169.0000
 34/110 [========>.....................] - ETA: 1s - loss: 10.2810 - val_loss: 10.2810.6614 - _timestamp: 1652158655.0000 - _runtime: 171.0000
 31/110 [=======>......................] - ETA: 1s - loss: 9.5116 - val_loss: 9.5116  .1831 - _timestamp: 1652158657.0000 - _runtime: 173.0000
 31/110 [=======>......................] - ETA: 1s - loss: 10.1568 - val_loss: 10.1568.2548 - _timestamp: 1652158659.0000 - _runtime: 175.0000
 31/110 [=======>......................] - ETA: 1s - loss: 11.6155 - val_loss: 11.6155.4592 - _timestamp: 1652158661.0000 - _runtime: 177.0000
 28/110 [======>.......................] - ETA: 1s - loss: 10.1378 - val_loss: 10.1378.2593 - _timestamp: 1652158663.0000 - _runtime: 179.0000
 16/110 [===>..........................] - ETA: 1s - loss: 8.8366 - val_loss: 8.8366  .2655 - _timestamp: 1652158666.0000 - _runtime: 182.0000
  4/110 [>.............................] - ETA: 2s - loss: 13.7713 - val_loss: 13.7713.2289 - _timestamp: 1652158668.0000 - _runtime: 184.0000
106/110 [===========================>..] - ETA: 0s - loss: 10.3410 - val_loss: 10.3410.2289 - _timestamp: 1652158668.0000 - _runtime: 184.0000
103/110 [===========================>..] - ETA: 0s - loss: 10.3072 - val_loss: 10.3072.1707 - _timestamp: 1652158670.0000 - _runtime: 186.0000
 92/110 [========================>.....] - ETA: 0s - loss: 10.1629 - val_loss: 10.1629.2257 - _timestamp: 1652158672.0000 - _runtime: 188.0000
 82/110 [=====================>........] - ETA: 0s - loss: 10.5171 - val_loss: 10.5171.4641 - _timestamp: 1652158674.0000 - _runtime: 190.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.3572 - val_loss: 10.3572.0864 - _timestamp: 1652158677.0000 - _runtime: 193.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.3215 - val_loss: 10.3215.3621 - _timestamp: 1652158679.0000 - _runtime: 195.0000
 49/110 [============>.................] - ETA: 1s - loss: 10.1829 - val_loss: 10.1829.3013 - _timestamp: 1652158681.0000 - _runtime: 197.0000
 48/110 [============>.................] - ETA: 1s - loss: 10.6132 - val_loss: 10.6132.1887 - _timestamp: 1652158683.0000 - _runtime: 199.0000
 43/110 [==========>...................] - ETA: 1s - loss: 10.6860 - val_loss: 10.6860.1422 - _timestamp: 1652158685.0000 - _runtime: 201.0000
 34/110 [========>.....................] - ETA: 1s - loss: 9.6797 - val_loss: 9.6797  .1285 - _timestamp: 1652158687.0000 - _runtime: 203.0000
 28/110 [======>.......................] - ETA: 1s - loss: 8.9936 - val_loss: 8.9936  .1743 - _timestamp: 1652158689.0000 - _runtime: 205.0000
 28/110 [======>.......................] - ETA: 1s - loss: 9.6811 - val_loss: 9.6811  .5772 - _timestamp: 1652158691.0000 - _runtime: 207.0000
 12/110 [==>...........................] - ETA: 2s - loss: 10.1547 - val_loss: 10.1547.2051 - _timestamp: 1652158693.0000 - _runtime: 209.0000
  7/110 [>.............................] - ETA: 2s - loss: 15.6836 - val_loss: 15.6836.3247 - _timestamp: 1652158696.0000 - _runtime: 212.0000
  4/110 [>.............................] - ETA: 1s - loss: 17.0108 - val_loss: 17.0108.2973 - _timestamp: 1652158698.0000 - _runtime: 214.0000
108/110 [============================>.] - ETA: 0s - loss: 10.2910 - val_loss: 10.2910.2973 - _timestamp: 1652158698.0000 - _runtime: 214.0000
 97/110 [=========================>....] - ETA: 0s - loss: 10.0282 - val_loss: 10.0282.1687 - _timestamp: 1652158700.0000 - _runtime: 216.0000
 80/110 [====================>.........] - ETA: 0s - loss: 10.2220 - val_loss: 10.2220.0838 - _timestamp: 1652158702.0000 - _runtime: 218.0000
 66/110 [=================>............] - ETA: 0s - loss: 10.3387 - val_loss: 10.3387.1689 - _timestamp: 1652158704.0000 - _runtime: 220.0000
 52/110 [=============>................] - ETA: 1s - loss: 10.0236 - val_loss: 10.0236.1397 - _timestamp: 1652158707.0000 - _runtime: 223.0000
 38/110 [=========>....................] - ETA: 1s - loss: 11.0711 - val_loss: 11.0711.1960 - _timestamp: 1652158709.0000 - _runtime: 225.0000
 38/110 [=========>....................] - ETA: 1s - loss: 11.0711 - val_loss: 11.0711.1960 - _timestamp: 1652158709.0000 - _runtime: 225.0000
rmse: 31.32854482662858, decorate the function with @tf.autograph.experimental.do_not_convertinux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
rmse: 31.32854482662858, decorate the function with @tf.autograph.experimental.do_not_convertinux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.