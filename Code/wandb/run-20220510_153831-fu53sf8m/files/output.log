==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf12b160> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf12b160> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
  2/110 [..............................] - ETA: 9s - loss: 13.5219 - val_loss: 13.5219
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.

110/110 [==============================] - ETA: 0s - loss: 14.0413 - val_loss: 13.9729WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d54d7b80> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d54d7b80> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 5s 31ms/step - loss: 14.0413 - val_loss: 11.6900 - val_val_loss: 11.6642 - _timestamp: 1652168320.0000 - _runtime: 8.0000
Epoch 2/200
 40/110 [=========>....................] - ETA: 1s - loss: 13.3367 - val_loss: 13.3367
110/110 [==============================] - 2s 20ms/step - loss: 13.6644 - val_loss: 14.1675 - _timestamp: 1652168323.0000 - _runtime: 11.0000
Epoch 3/200
110/110 [==============================] - 2s 20ms/step - loss: 13.5180 - val_loss: 13.4180 - _timestamp: 1652168325.0000 - _runtime: 13.0000
Epoch 4/200
110/110 [==============================] - 2s 21ms/step - loss: 13.1352 - val_loss: 13.0305 - _timestamp: 1652168327.0000 - _runtime: 15.0000
Epoch 5/200

110/110 [==============================] - 2s 20ms/step - loss: 13.2833 - val_loss: 13.1745 - _timestamp: 1652168329.0000 - _runtime: 17.0000
Epoch 6/200
110/110 [==============================] - 2s 20ms/step - loss: 13.1070 - val_loss: 13.0590 - _timestamp: 1652168331.0000 - _runtime: 19.0000
Epoch 7/200
110/110 [==============================] - 2s 20ms/step - loss: 12.8727 - val_loss: 15.0378 - _timestamp: 1652168334.0000 - _runtime: 22.0000
Epoch 8/200
110/110 [==============================] - 2s 20ms/step - loss: 12.6454 - val_loss: 12.5455 - _timestamp: 1652168336.0000 - _runtime: 24.0000
Epoch 9/200
110/110 [==============================] - 2s 20ms/step - loss: 12.3071 - val_loss: 12.2082 - _timestamp: 1652168338.0000 - _runtime: 26.0000
Epoch 10/200
110/110 [==============================] - 2s 20ms/step - loss: 12.7279 - val_loss: 12.6894 - _timestamp: 1652168340.0000 - _runtime: 28.0000
Epoch 11/200
110/110 [==============================] - 2s 20ms/step - loss: 12.5920 - val_loss: 12.5006 - _timestamp: 1652168342.0000 - _runtime: 30.0000
Epoch 12/200
110/110 [==============================] - 2s 19ms/step - loss: 12.4836 - val_loss: 12.3992 - _timestamp: 1652168345.0000 - _runtime: 33.0000
Epoch 13/200
110/110 [==============================] - 2s 20ms/step - loss: 12.6219 - val_loss: 12.8483 - _timestamp: 1652168347.0000 - _runtime: 35.0000
Epoch 14/200
110/110 [==============================] - 2s 20ms/step - loss: 12.1242 - val_loss: 12.0648 - _timestamp: 1652168349.0000 - _runtime: 37.0000
Epoch 15/200
110/110 [==============================] - 2s 20ms/step - loss: 12.0712 - val_loss: 12.0675 - _timestamp: 1652168351.0000 - _runtime: 39.0000
Epoch 16/200
110/110 [==============================] - 2s 20ms/step - loss: 11.9764 - val_loss: 11.8958 - _timestamp: 1652168353.0000 - _runtime: 41.0000
Epoch 17/200

110/110 [==============================] - 2s 20ms/step - loss: 11.9705 - val_loss: 11.8962 - _timestamp: 1652168356.0000 - _runtime: 44.0000
Epoch 18/200
110/110 [==============================] - 2s 20ms/step - loss: 11.8561 - val_loss: 11.7753 - _timestamp: 1652168358.0000 - _runtime: 46.0000
Epoch 19/200
110/110 [==============================] - 2s 20ms/step - loss: 11.8702 - val_loss: 11.7807 - _timestamp: 1652168360.0000 - _runtime: 48.0000
Epoch 20/200
110/110 [==============================] - 2s 21ms/step - loss: 11.7475 - val_loss: 11.6890 - _timestamp: 1652168362.0000 - _runtime: 50.0000
Epoch 21/200
110/110 [==============================] - 2s 20ms/step - loss: 11.9874 - val_loss: 11.9015 - _timestamp: 1652168364.0000 - _runtime: 52.0000
Epoch 22/200
110/110 [==============================] - 2s 20ms/step - loss: 11.9819 - val_loss: 11.9535 - _timestamp: 1652168367.0000 - _runtime: 55.0000
Epoch 23/200
110/110 [==============================] - 2s 20ms/step - loss: 12.2146 - val_loss: 12.1220 - _timestamp: 1652168369.0000 - _runtime: 57.0000
Epoch 24/200
110/110 [==============================] - 2s 20ms/step - loss: 12.3232 - val_loss: 12.2424 - _timestamp: 1652168371.0000 - _runtime: 59.0000
Epoch 25/200
110/110 [==============================] - 2s 20ms/step - loss: 12.7963 - val_loss: 12.8414 - _timestamp: 1652168373.0000 - _runtime: 61.0000
Epoch 26/200
110/110 [==============================] - 2s 20ms/step - loss: 12.3215 - val_loss: 12.2316 - _timestamp: 1652168375.0000 - _runtime: 63.0000
Epoch 27/200
110/110 [==============================] - 2s 20ms/step - loss: 12.4620 - val_loss: 12.3680 - _timestamp: 1652168378.0000 - _runtime: 66.0000
Epoch 28/200
110/110 [==============================] - 2s 20ms/step - loss: 12.5291 - val_loss: 12.4918 - _timestamp: 1652168380.0000 - _runtime: 68.0000
Epoch 29/200
110/110 [==============================] - 2s 20ms/step - loss: 12.0673 - val_loss: 12.2936 - _timestamp: 1652168382.0000 - _runtime: 70.0000
Epoch 30/200
110/110 [==============================] - 2s 20ms/step - loss: 12.9965 - val_loss: 12.8979 - _timestamp: 1652168384.0000 - _runtime: 72.0000
Epoch 31/200
110/110 [==============================] - 2s 20ms/step - loss: 12.5131 - val_loss: 12.5990 - _timestamp: 1652168386.0000 - _runtime: 74.0000
Epoch 32/200
110/110 [==============================] - 2s 20ms/step - loss: 12.3888 - val_loss: 12.4877 - _timestamp: 1652168389.0000 - _runtime: 77.0000
Epoch 33/200
110/110 [==============================] - 2s 20ms/step - loss: 12.2987 - val_loss: 12.1911 - _timestamp: 1652168391.0000 - _runtime: 79.0000
Epoch 34/200
110/110 [==============================] - 2s 20ms/step - loss: 12.1326 - val_loss: 12.5910 - _timestamp: 1652168393.0000 - _runtime: 81.0000
Epoch 35/200
110/110 [==============================] - 2s 20ms/step - loss: 12.1657 - val_loss: 12.0611 - _timestamp: 1652168395.0000 - _runtime: 83.0000
Epoch 36/200
110/110 [==============================] - 2s 20ms/step - loss: 12.1623 - val_loss: 12.0619 - _timestamp: 1652168397.0000 - _runtime: 85.0000
Epoch 37/200

110/110 [==============================] - 2s 20ms/step - loss: 11.9495 - val_loss: 11.8728 - _timestamp: 1652168400.0000 - _runtime: 88.0000
Epoch 38/200
110/110 [==============================] - 2s 20ms/step - loss: 11.8294 - val_loss: 11.9064 - _timestamp: 1652168402.0000 - _runtime: 90.0000
Epoch 39/200
110/110 [==============================] - 2s 21ms/step - loss: 11.9462 - val_loss: 12.0292 - _timestamp: 1652168404.0000 - _runtime: 92.0000
Epoch 40/200
110/110 [==============================] - 2s 21ms/step - loss: 11.7966 - val_loss: 11.6960 - _timestamp: 1652168406.0000 - _runtime: 94.0000
Epoch 41/200
110/110 [==============================] - 2s 20ms/step - loss: 12.4019 - val_loss: 12.2990 - _timestamp: 1652168409.0000 - _runtime: 97.0000
Epoch 42/200
Epoch 43/200===========================] - 2s 20ms/step - loss: 12.4335 - val_loss: 12.3329 - _timestamp: 1652168411.0000 - _runtime: 99.0000
Epoch 43/200===========================] - 2s 20ms/step - loss: 12.4335 - val_loss: 12.3329 - _timestamp: 1652168411.0000 - _runtime: 99.0000
 34/110 [========>.....................] - ETA: 1s - loss: 10.5876 - val_loss: 10.5876.9893 - _timestamp: 1652168413.0000 - _runtime: 101.0000
Epoch 44/200
 28/110 [======>.......................] - ETA: 1s - loss: 12.6883 - val_loss: 12.6883.2488 - _timestamp: 1652168415.0000 - _runtime: 103.0000
Epoch 45/200
 19/110 [====>.........................] - ETA: 1s - loss: 12.1312 - val_loss: 12.1312.1395 - _timestamp: 1652168417.0000 - _runtime: 105.0000
Epoch 46/200
 99/110 [==========================>...] - ETA: 0s - loss: 11.8917 - val_loss: 11.8917.1395 - _timestamp: 1652168417.0000 - _runtime: 105.0000
 91/110 [=======================>......] - ETA: 0s - loss: 12.1600 - val_loss: 12.1600.1925 - _timestamp: 1652168420.0000 - _runtime: 108.0000
 82/110 [=====================>........] - ETA: 0s - loss: 11.9077 - val_loss: 11.9077.8704 - _timestamp: 1652168422.0000 - _runtime: 110.0000
 73/110 [==================>...........] - ETA: 0s - loss: 11.2595 - val_loss: 11.2595.9552 - _timestamp: 1652168424.0000 - _runtime: 112.0000
 64/110 [================>.............] - ETA: 0s - loss: 12.6056 - val_loss: 12.6056.1520 - _timestamp: 1652168426.0000 - _runtime: 114.0000
 55/110 [==============>...............] - ETA: 1s - loss: 12.8703 - val_loss: 12.8703.0892 - _timestamp: 1652168428.0000 - _runtime: 116.0000
 46/110 [===========>..................] - ETA: 1s - loss: 11.5476 - val_loss: 11.5476.0488 - _timestamp: 1652168431.0000 - _runtime: 119.0000
 40/110 [=========>....................] - ETA: 1s - loss: 10.6326 - val_loss: 10.6326.8713 - _timestamp: 1652168433.0000 - _runtime: 121.0000
 28/110 [======>.......................] - ETA: 1s - loss: 13.9639 - val_loss: 13.9639.8437 - _timestamp: 1652168435.0000 - _runtime: 123.0000
 22/110 [=====>........................] - ETA: 1s - loss: 10.9614 - val_loss: 10.9614.9654 - _timestamp: 1652168437.0000 - _runtime: 125.0000
 16/110 [===>..........................] - ETA: 1s - loss: 10.4912 - val_loss: 10.4912.8718 - _timestamp: 1652168439.0000 - _runtime: 127.0000
  7/110 [>.............................] - ETA: 1s - loss: 13.6350 - val_loss: 13.6350.1415 - _timestamp: 1652168441.0000 - _runtime: 129.0000
109/110 [============================>.] - ETA: 0s - loss: 11.8589 - val_loss: 11.8589.1415 - _timestamp: 1652168441.0000 - _runtime: 129.0000
100/110 [==========================>...] - ETA: 0s - loss: 11.8181 - val_loss: 11.8181.9321 - _timestamp: 1652168444.0000 - _runtime: 132.0000
 88/110 [=======================>......] - ETA: 0s - loss: 11.9054 - val_loss: 11.9054.7483 - _timestamp: 1652168446.0000 - _runtime: 134.0000
 73/110 [==================>...........] - ETA: 0s - loss: 12.2202 - val_loss: 12.2202.6214 - _timestamp: 1652168448.0000 - _runtime: 136.0000
 66/110 [=================>............] - ETA: 0s - loss: 11.3482 - val_loss: 11.3482.3466 - _timestamp: 1652168450.0000 - _runtime: 138.0000
 58/110 [==============>...............] - ETA: 1s - loss: 12.8375 - val_loss: 12.8375.7628 - _timestamp: 1652168453.0000 - _runtime: 141.0000
 46/110 [===========>..................] - ETA: 1s - loss: 12.5037 - val_loss: 12.5037.5846 - _timestamp: 1652168455.0000 - _runtime: 143.0000
 34/110 [========>.....................] - ETA: 1s - loss: 11.2579 - val_loss: 11.2579.4995 - _timestamp: 1652168457.0000 - _runtime: 145.0000
 25/110 [=====>........................] - ETA: 1s - loss: 9.8335 - val_loss: 9.8335  .0126 - _timestamp: 1652168459.0000 - _runtime: 147.0000
 16/110 [===>..........................] - ETA: 1s - loss: 13.6267 - val_loss: 13.6267.7910 - _timestamp: 1652168462.0000 - _runtime: 150.0000
 10/110 [=>............................] - ETA: 1s - loss: 11.1789 - val_loss: 11.1789.7952 - _timestamp: 1652168464.0000 - _runtime: 152.0000
 85/110 [======================>.......] - ETA: 0s - loss: 11.6954 - val_loss: 11.6954.7952 - _timestamp: 1652168464.0000 - _runtime: 152.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.6135 - val_loss: 10.6135.5939 - _timestamp: 1652168466.0000 - _runtime: 154.0000
 67/110 [=================>............] - ETA: 0s - loss: 11.5513 - val_loss: 11.5513.5749 - _timestamp: 1652168468.0000 - _runtime: 156.0000
 58/110 [==============>...............] - ETA: 1s - loss: 11.0377 - val_loss: 11.0377.6166 - _timestamp: 1652168470.0000 - _runtime: 158.0000
 49/110 [============>.................] - ETA: 1s - loss: 11.5075 - val_loss: 11.5075.7728 - _timestamp: 1652168473.0000 - _runtime: 161.0000
 37/110 [=========>....................] - ETA: 1s - loss: 11.2390 - val_loss: 11.2390.8846 - _timestamp: 1652168475.0000 - _runtime: 163.0000
 28/110 [======>.......................] - ETA: 1s - loss: 12.8603 - val_loss: 12.8603.6346 - _timestamp: 1652168477.0000 - _runtime: 165.0000
 19/110 [====>.........................] - ETA: 1s - loss: 12.4324 - val_loss: 12.4324.0174 - _timestamp: 1652168479.0000 - _runtime: 167.0000
 13/110 [==>...........................] - ETA: 1s - loss: 11.5914 - val_loss: 11.5914.0160 - _timestamp: 1652168481.0000 - _runtime: 169.0000
  7/110 [>.............................] - ETA: 2s - loss: 10.9605 - val_loss: 10.9605.8503 - _timestamp: 1652168484.0000 - _runtime: 172.0000
109/110 [============================>.] - ETA: 0s - loss: 11.8435 - val_loss: 11.8435.8503 - _timestamp: 1652168484.0000 - _runtime: 172.0000
100/110 [==========================>...] - ETA: 0s - loss: 11.7028 - val_loss: 11.7028.7421 - _timestamp: 1652168486.0000 - _runtime: 174.0000
 94/110 [========================>.....] - ETA: 0s - loss: 11.6457 - val_loss: 11.6457.6683 - _timestamp: 1652168488.0000 - _runtime: 176.0000
 82/110 [=====================>........] - ETA: 0s - loss: 11.8580 - val_loss: 11.8580.9337 - _timestamp: 1652168490.0000 - _runtime: 178.0000
 70/110 [==================>...........] - ETA: 0s - loss: 12.2782 - val_loss: 12.2782.7648 - _timestamp: 1652168492.0000 - _runtime: 180.0000
 64/110 [================>.............] - ETA: 0s - loss: 10.5594 - val_loss: 10.5594.5155 - _timestamp: 1652168495.0000 - _runtime: 183.0000
 52/110 [=============>................] - ETA: 1s - loss: 12.1004 - val_loss: 12.1004.4376 - _timestamp: 1652168497.0000 - _runtime: 185.0000
 43/110 [==========>...................] - ETA: 1s - loss: 10.6669 - val_loss: 10.6669.5526 - _timestamp: 1652168499.0000 - _runtime: 187.0000
 31/110 [=======>......................] - ETA: 1s - loss: 11.5469 - val_loss: 11.5469.6148 - _timestamp: 1652168501.0000 - _runtime: 189.0000
 22/110 [=====>........................] - ETA: 1s - loss: 11.6042 - val_loss: 11.6042.5502 - _timestamp: 1652168503.0000 - _runtime: 191.0000
 13/110 [==>...........................] - ETA: 1s - loss: 12.2120 - val_loss: 12.2120.5888 - _timestamp: 1652168506.0000 - _runtime: 194.0000
  7/110 [>.............................] - ETA: 1s - loss: 11.1617 - val_loss: 11.1617.7805 - _timestamp: 1652168508.0000 - _runtime: 196.0000
 10/110 [=>............................] - ETA: 1s - loss: 7.3559 - val_loss: 7.355911.6485 - _timestamp: 1652168510.0000 - _runtime: 198.0000
 16/110 [===>..........................] - ETA: 1s - loss: 10.4818 - val_loss: 10.4818.5450 - _timestamp: 1652168512.0000 - _runtime: 200.0000
 16/110 [===>..........................] - ETA: 1s - loss: 11.9868 - val_loss: 11.9868.6097 - _timestamp: 1652168514.0000 - _runtime: 202.0000
 22/110 [=====>........................] - ETA: 1s - loss: 13.1726 - val_loss: 13.1726.9285 - _timestamp: 1652168516.0000 - _runtime: 204.0000
  1/110 [..............................] - ETA: 2s - loss: 9.4605 - val_loss: 9.460511.7433 - _timestamp: 1652168518.0000 - _runtime: 206.0000
  8/110 [=>............................] - ETA: 1s - loss: 12.3343 - val_loss: 12.3343.2688 - _timestamp: 1652168519.0000 - _runtime: 207.0000
 13/110 [==>...........................] - ETA: 1s - loss: 10.5615 - val_loss: 10.5615.3472 - _timestamp: 1652168521.0000 - _runtime: 209.0000
 23/110 [=====>........................] - ETA: 1s - loss: 11.8386 - val_loss: 11.8386.7484 - _timestamp: 1652168523.0000 - _runtime: 211.0000
 26/110 [======>.......................] - ETA: 1s - loss: 11.1797 - val_loss: 11.1797.6387 - _timestamp: 1652168525.0000 - _runtime: 213.0000
 34/110 [========>.....................] - ETA: 1s - loss: 12.6680 - val_loss: 12.6680.7803 - _timestamp: 1652168527.0000 - _runtime: 215.0000
 40/110 [=========>....................] - ETA: 1s - loss: 12.2086 - val_loss: 12.2086.4029 - _timestamp: 1652168529.0000 - _runtime: 217.0000
 48/110 [============>.................] - ETA: 1s - loss: 12.1575 - val_loss: 12.1575.5790 - _timestamp: 1652168531.0000 - _runtime: 219.0000
 51/110 [============>.................] - ETA: 1s - loss: 12.0325 - val_loss: 12.0325.6576 - _timestamp: 1652168533.0000 - _runtime: 221.0000
 62/110 [===============>..............] - ETA: 0s - loss: 11.9713 - val_loss: 11.9713.7407 - _timestamp: 1652168535.0000 - _runtime: 223.0000
 73/110 [==================>...........] - ETA: 0s - loss: 12.0250 - val_loss: 12.0250.9029 - _timestamp: 1652168537.0000 - _runtime: 225.0000
 78/110 [====================>.........] - ETA: 0s - loss: 11.5165 - val_loss: 11.5165.2307 - _timestamp: 1652168538.0000 - _runtime: 226.0000
 84/110 [=====================>........] - ETA: 0s - loss: 11.6484 - val_loss: 11.6484.6016 - _timestamp: 1652168540.0000 - _runtime: 228.0000
 96/110 [=========================>....] - ETA: 0s - loss: 11.4745 - val_loss: 11.4745.6135 - _timestamp: 1652168542.0000 - _runtime: 230.0000
105/110 [===========================>..] - ETA: 0s - loss: 11.6960 - val_loss: 11.6960.8079 - _timestamp: 1652168544.0000 - _runtime: 232.0000
110/110 [==============================] - 2s 18ms/step - loss: 11.6128 - val_loss: 11.5964 - _timestamp: 1652168548.0000 - _runtime: 236.0000
  1/110 [..............................] - ETA: 2s - loss: 16.2676 - val_loss: 16.2676.0502 - _timestamp: 1652168550.0000 - _runtime: 238.0000
 10/110 [=>............................] - ETA: 1s - loss: 10.6391 - val_loss: 10.6391.6026 - _timestamp: 1652168552.0000 - _runtime: 240.0000
 19/110 [====>.........................] - ETA: 1s - loss: 11.9573 - val_loss: 11.9573.8205 - _timestamp: 1652168554.0000 - _runtime: 242.0000
 28/110 [======>.......................] - ETA: 1s - loss: 11.9782 - val_loss: 11.9782.8255 - _timestamp: 1652168556.0000 - _runtime: 244.0000
 34/110 [========>.....................] - ETA: 1s - loss: 12.3225 - val_loss: 12.3225.7359 - _timestamp: 1652168557.0000 - _runtime: 245.0000
 45/110 [===========>..................] - ETA: 1s - loss: 11.7866 - val_loss: 11.7866.0167 - _timestamp: 1652168559.0000 - _runtime: 247.0000
 53/110 [=============>................] - ETA: 0s - loss: 11.8233 - val_loss: 11.8233.7182 - _timestamp: 1652168561.0000 - _runtime: 249.0000
 59/110 [===============>..............] - ETA: 0s - loss: 11.6817 - val_loss: 11.6817.7028 - _timestamp: 1652168563.0000 - _runtime: 251.0000
 65/110 [================>.............] - ETA: 0s - loss: 11.3389 - val_loss: 11.3389.8561 - _timestamp: 1652168565.0000 - _runtime: 253.0000
 42/110 [==========>...................] - ETA: 1s - loss: 12.4847 - val_loss: 12.4847.7473 - _timestamp: 1652168567.0000 - _runtime: 255.0000
 46/110 [===========>..................] - ETA: 1s - loss: 12.3389 - val_loss: 12.3389.6228 - _timestamp: 1652168569.0000 - _runtime: 257.0000
 53/110 [=============>................] - ETA: 0s - loss: 12.2355 - val_loss: 12.2355.7173 - _timestamp: 1652168571.0000 - _runtime: 259.0000
 52/110 [=============>................] - ETA: 1s - loss: 12.2266 - val_loss: 12.2266.5093 - _timestamp: 1652168573.0000 - _runtime: 261.0000
 54/110 [=============>................] - ETA: 0s - loss: 11.9169 - val_loss: 11.9169.5750 - _timestamp: 1652168575.0000 - _runtime: 263.0000
 61/110 [===============>..............] - ETA: 0s - loss: 12.3745 - val_loss: 12.3745.6178 - _timestamp: 1652168577.0000 - _runtime: 265.0000
 64/110 [================>.............] - ETA: 0s - loss: 10.9612 - val_loss: 10.9612.3535 - _timestamp: 1652168579.0000 - _runtime: 267.0000
 71/110 [==================>...........] - ETA: 0s - loss: 12.2604 - val_loss: 12.2604.5563 - _timestamp: 1652168581.0000 - _runtime: 269.0000
 76/110 [===================>..........] - ETA: 0s - loss: 11.4141 - val_loss: 11.4141.5907 - _timestamp: 1652168583.0000 - _runtime: 271.0000
 84/110 [=====================>........] - ETA: 0s - loss: 11.3815 - val_loss: 11.3815.4764 - _timestamp: 1652168584.0000 - _runtime: 272.0000
 92/110 [========================>.....] - ETA: 0s - loss: 11.6723 - val_loss: 11.6723.4218 - _timestamp: 1652168586.0000 - _runtime: 274.0000
 97/110 [=========================>....] - ETA: 0s - loss: 11.8220 - val_loss: 11.8220.4136 - _timestamp: 1652168588.0000 - _runtime: 276.0000
106/110 [===========================>..] - ETA: 0s - loss: 11.5813 - val_loss: 11.5813.6757 - _timestamp: 1652168590.0000 - _runtime: 278.0000
  7/110 [>.............................] - ETA: 1s - loss: 10.2038 - val_loss: 10.2038.5043 - _timestamp: 1652168594.0000 - _runtime: 282.0000
 10/110 [=>............................] - ETA: 1s - loss: 14.7088 - val_loss: 14.7088.7248 - _timestamp: 1652168596.0000 - _runtime: 284.0000
 16/110 [===>..........................] - ETA: 1s - loss: 11.0957 - val_loss: 11.0957.8938 - _timestamp: 1652168598.0000 - _runtime: 286.0000
 28/110 [======>.......................] - ETA: 1s - loss: 11.3586 - val_loss: 11.3586.5719 - _timestamp: 1652168600.0000 - _runtime: 288.0000
 34/110 [========>.....................] - ETA: 1s - loss: 10.9077 - val_loss: 10.9077.5193 - _timestamp: 1652168602.0000 - _runtime: 290.0000
 37/110 [=========>....................] - ETA: 1s - loss: 12.1697 - val_loss: 12.1697.6546 - _timestamp: 1652168603.0000 - _runtime: 291.0000
 43/110 [==========>...................] - ETA: 1s - loss: 12.1450 - val_loss: 12.1450.6673 - _timestamp: 1652168605.0000 - _runtime: 293.0000
 48/110 [============>.................] - ETA: 1s - loss: 11.6067 - val_loss: 11.6067.4877 - _timestamp: 1652168607.0000 - _runtime: 295.0000
 54/110 [=============>................] - ETA: 0s - loss: 12.0862 - val_loss: 12.0862.7774 - _timestamp: 1652168609.0000 - _runtime: 297.0000
 59/110 [===============>..............] - ETA: 0s - loss: 12.2679 - val_loss: 12.2679.7948 - _timestamp: 1652168611.0000 - _runtime: 299.0000
 39/110 [=========>....................] - ETA: 1s - loss: 11.6663 - val_loss: 11.6663.8224 - _timestamp: 1652168613.0000 - _runtime: 301.0000
 44/110 [===========>..................] - ETA: 1s - loss: 11.9462 - val_loss: 11.9462.8087 - _timestamp: 1652168615.0000 - _runtime: 303.0000
 51/110 [============>.................] - ETA: 1s - loss: 12.9231 - val_loss: 12.9231.4069 - _timestamp: 1652168617.0000 - _runtime: 305.0000
 58/110 [==============>...............] - ETA: 0s - loss: 10.6977 - val_loss: 10.6977.8003 - _timestamp: 1652168619.0000 - _runtime: 307.0000
 63/110 [================>.............] - ETA: 0s - loss: 11.9045 - val_loss: 11.9045.5554 - _timestamp: 1652168621.0000 - _runtime: 309.0000
 65/110 [================>.............] - ETA: 0s - loss: 11.0702 - val_loss: 11.0702.3383 - _timestamp: 1652168623.0000 - _runtime: 311.0000
 72/110 [==================>...........] - ETA: 0s - loss: 11.5852 - val_loss: 11.5852.8481 - _timestamp: 1652168625.0000 - _runtime: 313.0000
 79/110 [====================>.........] - ETA: 0s - loss: 12.0660 - val_loss: 12.0660.6039 - _timestamp: 1652168627.0000 - _runtime: 315.0000
 89/110 [=======================>......] - ETA: 0s - loss: 11.9020 - val_loss: 11.9020.7720 - _timestamp: 1652168628.0000 - _runtime: 316.0000
 88/110 [=======================>......] - ETA: 0s - loss: 11.6630 - val_loss: 11.6630.5952 - _timestamp: 1652168630.0000 - _runtime: 318.0000
 93/110 [========================>.....] - ETA: 0s - loss: 11.4390 - val_loss: 11.4390.6518 - _timestamp: 1652168632.0000 - _runtime: 320.0000
 99/110 [==========================>...] - ETA: 0s - loss: 11.5691 - val_loss: 11.5691.4083 - _timestamp: 1652168634.0000 - _runtime: 322.0000
105/110 [===========================>..] - ETA: 0s - loss: 11.5506 - val_loss: 11.5506.0791 - _timestamp: 1652168636.0000 - _runtime: 324.0000
  1/110 [..............................] - ETA: 2s - loss: 10.8245 - val_loss: 10.8245.7389 - _timestamp: 1652168640.0000 - _runtime: 328.0000
  4/110 [>.............................] - ETA: 1s - loss: 10.2575 - val_loss: 10.2575.6084 - _timestamp: 1652168642.0000 - _runtime: 330.0000
  4/110 [>.............................] - ETA: 1s - loss: 12.9205 - val_loss: 12.9205.4681 - _timestamp: 1652168644.0000 - _runtime: 332.0000
  7/110 [>.............................] - ETA: 1s - loss: 10.5215 - val_loss: 10.5215.8458 - _timestamp: 1652168646.0000 - _runtime: 334.0000
 10/110 [=>............................] - ETA: 1s - loss: 9.8381 - val_loss: 9.838111.4980 - _timestamp: 1652168648.0000 - _runtime: 336.0000
 13/110 [==>...........................] - ETA: 1s - loss: 11.1254 - val_loss: 11.1254.5293 - _timestamp: 1652168650.0000 - _runtime: 338.0000
 18/110 [===>..........................] - ETA: 1s - loss: 12.3670 - val_loss: 12.3670.4869 - _timestamp: 1652168652.0000 - _runtime: 340.0000
 22/110 [=====>........................] - ETA: 1s - loss: 11.8272 - val_loss: 11.8272.2928 - _timestamp: 1652168654.0000 - _runtime: 342.0000
 25/110 [=====>........................] - ETA: 1s - loss: 12.1709 - val_loss: 12.1709.5598 - _timestamp: 1652168656.0000 - _runtime: 344.0000
 34/110 [========>.....................] - ETA: 1s - loss: 11.9252 - val_loss: 11.9252.4577 - _timestamp: 1652168658.0000 - _runtime: 346.0000
 32/110 [=======>......................] - ETA: 1s - loss: 12.3371 - val_loss: 12.3371.2732 - _timestamp: 1652168660.0000 - _runtime: 348.0000
 40/110 [=========>....................] - ETA: 1s - loss: 11.9990 - val_loss: 11.9990.5138 - _timestamp: 1652168662.0000 - _runtime: 350.0000
 50/110 [============>.................] - ETA: 1s - loss: 10.6771 - val_loss: 10.6771.2771 - _timestamp: 1652168663.0000 - _runtime: 351.0000
 54/110 [=============>................] - ETA: 0s - loss: 11.1732 - val_loss: 11.1732.9040 - _timestamp: 1652168665.0000 - _runtime: 353.0000
 32/110 [=======>......................] - ETA: 1s - loss: 12.9352 - val_loss: 12.9352.4010 - _timestamp: 1652168667.0000 - _runtime: 355.0000
 39/110 [=========>....................] - ETA: 1s - loss: 10.9579 - val_loss: 10.9579.4477 - _timestamp: 1652168669.0000 - _runtime: 357.0000
 43/110 [==========>...................] - ETA: 1s - loss: 10.4484 - val_loss: 10.4484.4494 - _timestamp: 1652168671.0000 - _runtime: 359.0000
 49/110 [============>.................] - ETA: 1s - loss: 11.5225 - val_loss: 11.5225.6001 - _timestamp: 1652168673.0000 - _runtime: 361.0000
 50/110 [============>.................] - ETA: 1s - loss: 11.3396 - val_loss: 11.3396.6788 - _timestamp: 1652168675.0000 - _runtime: 363.0000
 57/110 [==============>...............] - ETA: 0s - loss: 11.2142 - val_loss: 11.2142.6345 - _timestamp: 1652168677.0000 - _runtime: 365.0000
 64/110 [================>.............] - ETA: 0s - loss: 11.0071 - val_loss: 11.0071.6538 - _timestamp: 1652168679.0000 - _runtime: 367.0000
 67/110 [=================>............] - ETA: 0s - loss: 11.5741 - val_loss: 11.5741.2025 - _timestamp: 1652168681.0000 - _runtime: 369.0000
 73/110 [==================>...........] - ETA: 0s - loss: 11.6075 - val_loss: 11.6075.5199 - _timestamp: 1652168683.0000 - _runtime: 371.0000
 83/110 [=====================>........] - ETA: 0s - loss: 11.7291 - val_loss: 11.7291.4328 - _timestamp: 1652168685.0000 - _runtime: 373.0000
 85/110 [======================>.......] - ETA: 0s - loss: 11.4817 - val_loss: 11.4817.3407 - _timestamp: 1652168687.0000 - _runtime: 375.0000
 85/110 [======================>.......] - ETA: 0s - loss: 11.4016 - val_loss: 11.4016.4204 - _timestamp: 1652168689.0000 - _runtime: 377.0000
 85/110 [======================>.......] - ETA: 0s - loss: 11.0856 - val_loss: 11.0856.3640 - _timestamp: 1652168691.0000 - _runtime: 379.0000
 85/110 [======================>.......] - ETA: 0s - loss: 11.7656 - val_loss: 11.7656.2557 - _timestamp: 1652168693.0000 - _runtime: 381.0000
 92/110 [========================>.....] - ETA: 0s - loss: 11.3284 - val_loss: 11.3284.2487 - _timestamp: 1652168695.0000 - _runtime: 383.0000
 99/110 [==========================>...] - ETA: 0s - loss: 11.6436 - val_loss: 11.6436.5551 - _timestamp: 1652168697.0000 - _runtime: 385.0000
105/110 [===========================>..] - ETA: 0s - loss: 11.5483 - val_loss: 11.5483.6679 - _timestamp: 1652168698.0000 - _runtime: 386.0000
  1/110 [..............................] - ETA: 2s - loss: 5.4546 - val_loss: 5.454611.3875 - _timestamp: 1652168702.0000 - _runtime: 390.0000
Epoch 188/200
  7/110 [>.............................] - ETA: 1s - loss: 13.0798 - val_loss: 13.0798.4118 - _timestamp: 1652168704.0000 - _runtime: 392.0000
Epoch 189/200
 11/110 [==>...........................] - ETA: 1s - loss: 8.9091 - val_loss: 8.909111.5736 - _timestamp: 1652168706.0000 - _runtime: 394.0000
Epoch 190/200
 16/110 [===>..........................] - ETA: 1s - loss: 11.4476 - val_loss: 11.4476.3351 - _timestamp: 1652168708.0000 - _runtime: 396.0000
Epoch 191/200
 24/110 [=====>........................] - ETA: 1s - loss: 11.3523 - val_loss: 11.3523.5101 - _timestamp: 1652168710.0000 - _runtime: 398.0000
Epoch 192/200
 28/110 [======>.......................] - ETA: 1s - loss: 11.8543 - val_loss: 11.8543.0745 - _timestamp: 1652168712.0000 - _runtime: 400.0000
Epoch 193/200
 36/110 [========>.....................] - ETA: 1s - loss: 11.0080 - val_loss: 11.0080.1190 - _timestamp: 1652168714.0000 - _runtime: 402.0000
Epoch 194/200
 45/110 [===========>..................] - ETA: 1s - loss: 11.6544 - val_loss: 11.6544.3431 - _timestamp: 1652168716.0000 - _runtime: 404.0000
Epoch 195/200
 49/110 [============>.................] - ETA: 1s - loss: 11.6763 - val_loss: 11.6763.3659 - _timestamp: 1652168718.0000 - _runtime: 406.0000
Epoch 196/200
 58/110 [==============>...............] - ETA: 0s - loss: 12.2177 - val_loss: 12.2177.4971 - _timestamp: 1652168719.0000 - _runtime: 407.0000
Epoch 197/200
 65/110 [================>.............] - ETA: 0s - loss: 12.0369 - val_loss: 12.0369.3706 - _timestamp: 1652168721.0000 - _runtime: 409.0000
Epoch 198/200
 69/110 [=================>............] - ETA: 0s - loss: 10.9650 - val_loss: 10.9650.3123 - _timestamp: 1652168723.0000 - _runtime: 411.0000
Epoch 199/200
 48/110 [============>.................] - ETA: 1s - loss: 11.1964 - val_loss: 11.1964.6203 - _timestamp: 1652168725.0000 - _runtime: 413.0000
Epoch 200/200
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d5cea430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d5cea430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
==================== fold_0 score ====================
rmse: 32.02427367810651
2022-05-10 15:45:27.822803: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.