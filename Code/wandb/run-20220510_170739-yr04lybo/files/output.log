==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x40c272430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x40c272430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 13/110 [==>...........................] - ETA: 3s - loss: 12.6692 - val_loss: 12.6692
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
110/110 [==============================] - ETA: 0s - loss: 11.9609 - val_loss: 11.8564WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x4082ac670> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x4082ac670> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 3s 23ms/step - loss: 11.9609 - val_loss: 11.5484 - val_val_loss: 11.5089 - _timestamp: 1652173668.0000 - _runtime: 9.0000
Epoch 2/200
  1/110 [..............................] - ETA: 1s - loss: 5.5858 - val_loss: 5.5858
110/110 [==============================] - 2s 14ms/step - loss: 10.8028 - val_loss: 11.8741 - _timestamp: 1652173670.0000 - _runtime: 11.0000
Epoch 3/200
110/110 [==============================] - 2s 14ms/step - loss: 10.5777 - val_loss: 10.4939 - _timestamp: 1652173671.0000 - _runtime: 12.0000
Epoch 4/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4741 - val_loss: 10.4011 - _timestamp: 1652173673.0000 - _runtime: 14.0000
Epoch 5/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4696 - val_loss: 10.3917 - _timestamp: 1652173674.0000 - _runtime: 15.0000
Epoch 6/200
110/110 [==============================] - 1s 13ms/step - loss: 10.6685 - val_loss: 10.7273 - _timestamp: 1652173676.0000 - _runtime: 17.0000
Epoch 7/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3851 - val_loss: 10.3345 - _timestamp: 1652173677.0000 - _runtime: 18.0000
Epoch 8/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3721 - val_loss: 10.2865 - _timestamp: 1652173679.0000 - _runtime: 20.0000
Epoch 9/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4992 - val_loss: 10.4348 - _timestamp: 1652173680.0000 - _runtime: 21.0000
Epoch 10/200
110/110 [==============================] - 1s 13ms/step - loss: 10.6262 - val_loss: 10.7876 - _timestamp: 1652173681.0000 - _runtime: 22.0000
Epoch 11/200
110/110 [==============================] - 1s 13ms/step - loss: 10.5504 - val_loss: 10.4780 - _timestamp: 1652173683.0000 - _runtime: 24.0000
Epoch 12/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3331 - val_loss: 10.6007 - _timestamp: 1652173684.0000 - _runtime: 25.0000
Epoch 13/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3833 - val_loss: 10.8062 - _timestamp: 1652173686.0000 - _runtime: 27.0000
Epoch 14/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4278 - val_loss: 10.5979 - _timestamp: 1652173687.0000 - _runtime: 28.0000
Epoch 15/200
110/110 [==============================] - 2s 14ms/step - loss: 10.1782 - val_loss: 10.1009 - _timestamp: 1652173689.0000 - _runtime: 30.0000
Epoch 16/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3144 - val_loss: 10.2821 - _timestamp: 1652173690.0000 - _runtime: 31.0000
Epoch 17/200
110/110 [==============================] - 2s 14ms/step - loss: 10.1933 - val_loss: 10.1128 - _timestamp: 1652173692.0000 - _runtime: 33.0000
Epoch 18/200
110/110 [==============================] - 1s 13ms/step - loss: 10.5177 - val_loss: 10.4385 - _timestamp: 1652173693.0000 - _runtime: 34.0000
Epoch 19/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2400 - val_loss: 10.1539 - _timestamp: 1652173695.0000 - _runtime: 36.0000
Epoch 20/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2447 - val_loss: 10.2919 - _timestamp: 1652173696.0000 - _runtime: 37.0000
Epoch 21/200
110/110 [==============================] - 1s 12ms/step - loss: 10.4511 - val_loss: 10.3741 - _timestamp: 1652173697.0000 - _runtime: 38.0000
Epoch 22/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3327 - val_loss: 10.2574 - _timestamp: 1652173699.0000 - _runtime: 40.0000
Epoch 23/200
110/110 [==============================] - 1s 14ms/step - loss: 10.4703 - val_loss: 10.3952 - _timestamp: 1652173700.0000 - _runtime: 41.0000
Epoch 24/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2958 - val_loss: 10.9339 - _timestamp: 1652173702.0000 - _runtime: 43.0000
Epoch 25/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3721 - val_loss: 10.3230 - _timestamp: 1652173703.0000 - _runtime: 44.0000
Epoch 26/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2477 - val_loss: 10.1639 - _timestamp: 1652173705.0000 - _runtime: 46.0000
Epoch 27/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3086 - val_loss: 10.4422 - _timestamp: 1652173706.0000 - _runtime: 47.0000
Epoch 28/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3412 - val_loss: 10.2735 - _timestamp: 1652173708.0000 - _runtime: 49.0000
Epoch 29/200
110/110 [==============================] - 1s 14ms/step - loss: 10.2820 - val_loss: 10.2763 - _timestamp: 1652173709.0000 - _runtime: 50.0000
Epoch 30/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3019 - val_loss: 10.2444 - _timestamp: 1652173711.0000 - _runtime: 52.0000
Epoch 31/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2655 - val_loss: 10.1999 - _timestamp: 1652173712.0000 - _runtime: 53.0000
Epoch 32/200
110/110 [==============================] - 1s 14ms/step - loss: 10.3813 - val_loss: 10.3118 - _timestamp: 1652173714.0000 - _runtime: 55.0000
Epoch 33/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4061 - val_loss: 10.3659 - _timestamp: 1652173715.0000 - _runtime: 56.0000
Epoch 34/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3794 - val_loss: 10.3953 - _timestamp: 1652173716.0000 - _runtime: 57.0000
Epoch 35/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3730 - val_loss: 10.3084 - _timestamp: 1652173718.0000 - _runtime: 59.0000
Epoch 36/200
110/110 [==============================] - 1s 13ms/step - loss: 10.1227 - val_loss: 10.0499 - _timestamp: 1652173719.0000 - _runtime: 60.0000
Epoch 37/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4695 - val_loss: 10.4003 - _timestamp: 1652173721.0000 - _runtime: 62.0000
Epoch 38/200
110/110 [==============================] - 2s 14ms/step - loss: 10.1712 - val_loss: 10.2665 - _timestamp: 1652173722.0000 - _runtime: 63.0000
Epoch 39/200
110/110 [==============================] - 2s 14ms/step - loss: 10.2628 - val_loss: 10.1953 - _timestamp: 1652173724.0000 - _runtime: 65.0000
Epoch 40/200
110/110 [==============================] - 1s 14ms/step - loss: 10.4138 - val_loss: 10.3663 - _timestamp: 1652173725.0000 - _runtime: 66.0000
Epoch 41/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3083 - val_loss: 10.2548 - _timestamp: 1652173727.0000 - _runtime: 68.0000
Epoch 42/200
110/110 [==============================] - 1s 14ms/step - loss: 10.1089 - val_loss: 10.0524 - _timestamp: 1652173728.0000 - _runtime: 69.0000
Epoch 43/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2091 - val_loss: 10.1259 - _timestamp: 1652173730.0000 - _runtime: 71.0000
Epoch 44/200
 66/110 [=================>............] - ETA: 0s - loss: 9.4951 - val_loss: 9.4951
Epoch 45/200===========================] - 1s 13ms/step - loss: 10.2091 - val_loss: 10.1259 - _timestamp: 1652173730.0000 - _runtime: 71.0000
 72/110 [==================>...........] - ETA: 0s - loss: 11.1078 - val_loss: 11.1078
110/110 [==============================] - 1s 13ms/step - loss: 10.3008 - val_loss: 10.2219 - _timestamp: 1652173734.0000 - _runtime: 75.0000
Epoch 47/200
 10/110 [=>............................] - ETA: 1s - loss: 10.4194 - val_loss: 10.4194
Epoch 48/200===========================] - 1s 13ms/step - loss: 10.3008 - val_loss: 10.2219 - _timestamp: 1652173734.0000 - _runtime: 75.0000
 51/110 [============>.................] - ETA: 0s - loss: 9.8691 - val_loss: 9.8691
 98/110 [=========================>....] - ETA: 0s - loss: 10.0995 - val_loss: 10.0995.2219 - _timestamp: 1652173734.0000 - _runtime: 75.0000
Epoch 51/200===========================] - 1s 12ms/step - loss: 10.2127 - val_loss: 10.1244 - _timestamp: 1652173738.0000 - _runtime: 79.0000
 39/110 [=========>....................] - ETA: 0s - loss: 9.7929 - val_loss: 9.7929
 83/110 [=====================>........] - ETA: 0s - loss: 10.1823 - val_loss: 10.1823.1244 - _timestamp: 1652173738.0000 - _runtime: 79.0000
Epoch 54/200===========================] - 1s 13ms/step - loss: 10.4715 - val_loss: 10.4381 - _timestamp: 1652173742.0000 - _runtime: 83.0000
 21/110 [====>.........................] - ETA: 1s - loss: 9.3140 - val_loss: 9.3140
 65/110 [================>.............] - ETA: 0s - loss: 10.7209 - val_loss: 10.7209.4381 - _timestamp: 1652173742.0000 - _runtime: 83.0000
110/110 [==============================] - 1s 14ms/step - loss: 10.0530 - val_loss: 9.9850 - _timestamp: 1652173747.0000 - _runtime: 88.00000
 33/110 [========>.....................] - ETA: 1s - loss: 11.0854 - val_loss: 11.08549850 - _timestamp: 1652173747.0000 - _runtime: 88.00000
110/110 [==============================] - 2s 14ms/step - loss: 10.3789 - val_loss: 10.4734 - _timestamp: 1652173751.0000 - _runtime: 92.0000
Epoch 60/200===========================] - 2s 14ms/step - loss: 10.3789 - val_loss: 10.4734 - _timestamp: 1652173751.0000 - _runtime: 92.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3859 - val_loss: 10.6347 - _timestamp: 1652173756.0000 - _runtime: 97.0000
Epoch 63/200===========================] - 2s 14ms/step - loss: 10.3859 - val_loss: 10.6347 - _timestamp: 1652173756.0000 - _runtime: 97.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.3265 - val_loss: 10.2639 - _timestamp: 1652173760.0000 - _runtime: 101.0000
Epoch 66/200===========================] - 1s 13ms/step - loss: 10.3265 - val_loss: 10.2639 - _timestamp: 1652173760.0000 - _runtime: 101.0000
 87/110 [======================>.......] - ETA: 0s - loss: 10.6324 - val_loss: 10.6324.2639 - _timestamp: 1652173760.0000 - _runtime: 101.0000
Epoch 69/200===========================] - 2s 14ms/step - loss: 10.1105 - val_loss: 10.2623 - _timestamp: 1652173765.0000 - _runtime: 106.0000
Epoch 69/200===========================] - 2s 14ms/step - loss: 10.1105 - val_loss: 10.2623 - _timestamp: 1652173765.0000 - _runtime: 106.0000
Epoch 69/200===========================] - 2s 14ms/step - loss: 10.1105 - val_loss: 10.2623 - _timestamp: 1652173765.0000 - _runtime: 106.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1318 - val_loss: 10.0700 - _timestamp: 1652173774.0000 - _runtime: 115.0000
Epoch 72/200===========================] - 1s 13ms/step - loss: 10.1318 - val_loss: 10.0700 - _timestamp: 1652173774.0000 - _runtime: 115.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0898 - val_loss: 10.0328 - _timestamp: 1652173778.0000 - _runtime: 119.0000
Epoch 75/200===========================] - 1s 13ms/step - loss: 10.0898 - val_loss: 10.0328 - _timestamp: 1652173778.0000 - _runtime: 119.0000
106/110 [===========================>..] - ETA: 0s - loss: 10.3366 - val_loss: 10.3366.0328 - _timestamp: 1652173778.0000 - _runtime: 119.0000
Epoch 78/200===========================] - 1s 12ms/step - loss: 10.1755 - val_loss: 10.0894 - _timestamp: 1652173783.0000 - _runtime: 124.0000
 90/110 [=======================>......] - ETA: 0s - loss: 9.6655 - val_loss: 9.6655  .0894 - _timestamp: 1652173783.0000 - _runtime: 124.0000
Epoch 81/200===========================] - 1s 13ms/step - loss: 9.9826 - val_loss: 11.3567 - _timestamp: 1652173787.0000 - _runtime: 128.00000
 75/110 [===================>..........] - ETA: 0s - loss: 10.1205 - val_loss: 10.12053567 - _timestamp: 1652173787.0000 - _runtime: 128.00000
Epoch 84/200===========================] - 1s 13ms/step - loss: 10.1534 - val_loss: 10.1801 - _timestamp: 1652173791.0000 - _runtime: 132.0000
 59/110 [===============>..............] - ETA: 0s - loss: 10.9799 - val_loss: 10.9799.1801 - _timestamp: 1652173791.0000 - _runtime: 132.0000
Epoch 87/200===========================] - 1s 13ms/step - loss: 10.2884 - val_loss: 10.4105 - _timestamp: 1652173795.0000 - _runtime: 136.0000
 48/110 [============>.................] - ETA: 0s - loss: 11.1080 - val_loss: 11.1080.4105 - _timestamp: 1652173795.0000 - _runtime: 136.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2531 - val_loss: 10.1773 - _timestamp: 1652173799.0000 - _runtime: 140.0000
 30/110 [=======>......................] - ETA: 1s - loss: 8.0541 - val_loss: 8.0541  .1773 - _timestamp: 1652173799.0000 - _runtime: 140.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0218 - val_loss: 10.1719 - _timestamp: 1652173804.0000 - _runtime: 145.0000
 18/110 [===>..........................] - ETA: 1s - loss: 10.1950 - val_loss: 10.1950.1719 - _timestamp: 1652173804.0000 - _runtime: 145.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.3193 - val_loss: 10.3193 - _timestamp: 1652173808.0000 - _runtime: 149.0000
Epoch 96/200===========================] - 1s 13ms/step - loss: 10.3193 - val_loss: 10.3193 - _timestamp: 1652173808.0000 - _runtime: 149.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0981 - val_loss: 10.0360 - _timestamp: 1652173812.0000 - _runtime: 153.0000
Epoch 99/200===========================] - 1s 13ms/step - loss: 10.0981 - val_loss: 10.0360 - _timestamp: 1652173812.0000 - _runtime: 153.0000
 89/110 [=======================>......] - ETA: 0s - loss: 10.5896 - val_loss: 10.5896.0360 - _timestamp: 1652173812.0000 - _runtime: 153.0000
Epoch 102/200==========================] - 1s 13ms/step - loss: 10.2907 - val_loss: 10.2275 - _timestamp: 1652173817.0000 - _runtime: 158.0000
 59/110 [===============>..............] - ETA: 0s - loss: 9.4611 - val_loss: 9.4611  .2275 - _timestamp: 1652173817.0000 - _runtime: 158.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1476 - val_loss: 10.0827 - _timestamp: 1652173821.0000 - _runtime: 162.0000
 37/110 [=========>....................] - ETA: 0s - loss: 9.7286 - val_loss: 9.7286  .0827 - _timestamp: 1652173821.0000 - _runtime: 162.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1147 - val_loss: 10.1560 - _timestamp: 1652173825.0000 - _runtime: 166.0000
  5/110 [>.............................] - ETA: 1s - loss: 8.5855 - val_loss: 8.5855  .1560 - _timestamp: 1652173825.0000 - _runtime: 166.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2775 - val_loss: 10.3230 - _timestamp: 1652173830.0000 - _runtime: 171.0000
Epoch 111/200==========================] - 1s 13ms/step - loss: 10.2775 - val_loss: 10.3230 - _timestamp: 1652173830.0000 - _runtime: 171.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1425 - val_loss: 10.5575 - _timestamp: 1652173834.0000 - _runtime: 175.0000
Epoch 114/200==========================] - 2s 14ms/step - loss: 10.1425 - val_loss: 10.5575 - _timestamp: 1652173834.0000 - _runtime: 175.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.0910 - val_loss: 10.1318 - _timestamp: 1652173838.0000 - _runtime: 179.0000
Epoch 117/200==========================] - 1s 12ms/step - loss: 10.0910 - val_loss: 10.1318 - _timestamp: 1652173838.0000 - _runtime: 179.0000
108/110 [============================>.] - ETA: 0s - loss: 10.1825 - val_loss: 10.1825.1318 - _timestamp: 1652173838.0000 - _runtime: 179.0000
Epoch 120/200==========================] - 1s 13ms/step - loss: 10.1191 - val_loss: 10.1665 - _timestamp: 1652173843.0000 - _runtime: 184.0000
 91/110 [=======================>......] - ETA: 0s - loss: 10.4056 - val_loss: 10.4056.1665 - _timestamp: 1652173843.0000 - _runtime: 184.0000
Epoch 123/200==========================] - 1s 13ms/step - loss: 10.4538 - val_loss: 10.5133 - _timestamp: 1652173847.0000 - _runtime: 188.0000
 55/110 [==============>...............] - ETA: 0s - loss: 10.5637 - val_loss: 10.5637.5133 - _timestamp: 1652173847.0000 - _runtime: 188.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1219 - val_loss: 10.0578 - _timestamp: 1652173851.0000 - _runtime: 192.0000
 33/110 [========>.....................] - ETA: 0s - loss: 10.9463 - val_loss: 10.9463.0578 - _timestamp: 1652173851.0000 - _runtime: 192.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1864 - val_loss: 10.1357 - _timestamp: 1652173856.0000 - _runtime: 197.0000
 25/110 [=====>........................] - ETA: 1s - loss: 8.9092 - val_loss: 8.9092  .1357 - _timestamp: 1652173856.0000 - _runtime: 197.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1825 - val_loss: 10.1108 - _timestamp: 1652173860.0000 - _runtime: 201.0000
 14/110 [==>...........................] - ETA: 1s - loss: 10.9190 - val_loss: 10.9190.1108 - _timestamp: 1652173860.0000 - _runtime: 201.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.1989 - val_loss: 10.1273 - _timestamp: 1652173864.0000 - _runtime: 205.0000
Epoch 135/200==========================] - 1s 12ms/step - loss: 10.1989 - val_loss: 10.1273 - _timestamp: 1652173864.0000 - _runtime: 205.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1570 - val_loss: 10.0761 - _timestamp: 1652173868.0000 - _runtime: 209.0000
Epoch 138/200==========================] - 1s 13ms/step - loss: 10.1570 - val_loss: 10.0761 - _timestamp: 1652173868.0000 - _runtime: 209.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.3540 - val_loss: 10.6384 - _timestamp: 1652173872.0000 - _runtime: 213.0000
Epoch 141/200==========================] - 1s 12ms/step - loss: 10.3540 - val_loss: 10.6384 - _timestamp: 1652173872.0000 - _runtime: 213.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2450 - val_loss: 10.1573 - _timestamp: 1652173877.0000 - _runtime: 218.0000
Epoch 144/200==========================] - 1s 12ms/step - loss: 10.2450 - val_loss: 10.1573 - _timestamp: 1652173877.0000 - _runtime: 218.0000
 82/110 [=====================>........] - ETA: 0s - loss: 10.2071 - val_loss: 10.2071.1573 - _timestamp: 1652173877.0000 - _runtime: 218.0000
Epoch 147/200==========================] - 1s 12ms/step - loss: 10.2148 - val_loss: 10.1455 - _timestamp: 1652173881.0000 - _runtime: 222.0000
 75/110 [===================>..........] - ETA: 0s - loss: 9.3278 - val_loss: 9.3278  .1455 - _timestamp: 1652173881.0000 - _runtime: 222.0000
Epoch 150/200==========================] - 1s 13ms/step - loss: 10.2459 - val_loss: 10.1631 - _timestamp: 1652173885.0000 - _runtime: 226.0000
 69/110 [=================>............] - ETA: 0s - loss: 9.9698 - val_loss: 9.9698  .1631 - _timestamp: 1652173885.0000 - _runtime: 226.0000
Epoch 153/200==========================] - 1s 12ms/step - loss: 10.1330 - val_loss: 10.0725 - _timestamp: 1652173889.0000 - _runtime: 230.0000
 58/110 [==============>...............] - ETA: 0s - loss: 10.8837 - val_loss: 10.8837.0725 - _timestamp: 1652173889.0000 - _runtime: 230.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0811 - val_loss: 11.0367 - _timestamp: 1652173893.0000 - _runtime: 234.0000
 28/110 [======>.......................] - ETA: 1s - loss: 10.8286 - val_loss: 10.8286.0367 - _timestamp: 1652173893.0000 - _runtime: 234.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2013 - val_loss: 10.1628 - _timestamp: 1652173898.0000 - _runtime: 239.0000
Epoch 159/200==========================] - 2s 14ms/step - loss: 10.2013 - val_loss: 10.1628 - _timestamp: 1652173898.0000 - _runtime: 239.0000
110/110 [==============================] - 1s 14ms/step - loss: 10.1163 - val_loss: 10.0422 - _timestamp: 1652173902.0000 - _runtime: 243.0000
Epoch 162/200==========================] - 1s 14ms/step - loss: 10.1163 - val_loss: 10.0422 - _timestamp: 1652173902.0000 - _runtime: 243.0000
 73/110 [==================>...........] - ETA: 0s - loss: 10.3697 - val_loss: 10.3697.0422 - _timestamp: 1652173902.0000 - _runtime: 243.0000
Epoch 165/200==========================] - 1s 14ms/step - loss: 10.2256 - val_loss: 10.1602 - _timestamp: 1652173907.0000 - _runtime: 248.0000
 48/110 [============>.................] - ETA: 0s - loss: 10.6307 - val_loss: 10.6307.1602 - _timestamp: 1652173907.0000 - _runtime: 248.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.0760 - val_loss: 9.9882 - _timestamp: 1652173911.0000 - _runtime: 252.00000
 13/110 [==>...........................] - ETA: 1s - loss: 11.3464 - val_loss: 11.34649882 - _timestamp: 1652173911.0000 - _runtime: 252.00000
110/110 [==============================] - 1s 14ms/step - loss: 10.1363 - val_loss: 10.1446 - _timestamp: 1652173916.0000 - _runtime: 257.0000
Epoch 171/200==========================] - 1s 14ms/step - loss: 10.1363 - val_loss: 10.1446 - _timestamp: 1652173916.0000 - _runtime: 257.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2999 - val_loss: 10.2343 - _timestamp: 1652173921.0000 - _runtime: 262.0000
Epoch 174/200==========================] - 1s 13ms/step - loss: 10.2999 - val_loss: 10.2343 - _timestamp: 1652173921.0000 - _runtime: 262.0000
 88/110 [=======================>......] - ETA: 0s - loss: 9.5498 - val_loss: 9.5498  .2343 - _timestamp: 1652173921.0000 - _runtime: 262.0000
Epoch 177/200==========================] - 1s 13ms/step - loss: 10.2654 - val_loss: 10.2916 - _timestamp: 1652173925.0000 - _runtime: 266.0000
 52/110 [=============>................] - ETA: 0s - loss: 9.8746 - val_loss: 9.874610.2916 - _timestamp: 1652173925.0000 - _runtime: 266.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0938 - val_loss: 10.0936 - _timestamp: 1652173930.0000 - _runtime: 271.0000
 19/110 [====>.........................] - ETA: 1s - loss: 8.3470 - val_loss: 8.3470  .0936 - _timestamp: 1652173930.0000 - _runtime: 271.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0960 - val_loss: 10.0305 - _timestamp: 1652173934.0000 - _runtime: 275.0000
  5/110 [>.............................] - ETA: 1s - loss: 9.1061 - val_loss: 9.106110.0305 - _timestamp: 1652173934.0000 - _runtime: 275.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0715 - val_loss: 10.0075 - _timestamp: 1652173938.0000 - _runtime: 279.0000
Epoch 186/200==========================] - 1s 13ms/step - loss: 10.0715 - val_loss: 10.0075 - _timestamp: 1652173938.0000 - _runtime: 279.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.0181 - val_loss: 9.9537 - _timestamp: 1652173942.0000 - _runtime: 283.00000
Epoch 189/200==========================] - 1s 12ms/step - loss: 10.0181 - val_loss: 9.9537 - _timestamp: 1652173942.0000 - _runtime: 283.00000
110/110 [==============================] - 1s 12ms/step - loss: 10.1212 - val_loss: 10.8173 - _timestamp: 1652173946.0000 - _runtime: 287.0000
Epoch 192/200==========================] - 1s 12ms/step - loss: 10.1212 - val_loss: 10.8173 - _timestamp: 1652173946.0000 - _runtime: 287.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2201 - val_loss: 10.1465 - _timestamp: 1652173950.0000 - _runtime: 291.0000
Epoch 195/200==========================] - 1s 12ms/step - loss: 10.2201 - val_loss: 10.1465 - _timestamp: 1652173950.0000 - _runtime: 291.0000
 93/110 [========================>.....] - ETA: 0s - loss: 9.7477 - val_loss: 9.7477  .1465 - _timestamp: 1652173950.0000 - _runtime: 291.0000
Epoch 198/200==========================] - 1s 13ms/step - loss: 9.8584 - val_loss: 9.8906 - _timestamp: 1652173955.0000 - _runtime: 296.000000
 89/110 [=======================>......] - ETA: 0s - loss: 10.8432 - val_loss: 10.8432906 - _timestamp: 1652173955.0000 - _runtime: 296.000000
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()nverts>.predict_function at 0x4183b1430> and will run it as-is.
rmse: 31.26243633613088, requested ('self', 'step_function'), but source function had ()nverts>.predict_function at 0x4183b1430> and will run it as-is.
rmse: 31.26243633613088, requested ('self', 'step_function'), but source function had ()nverts>.predict_function at 0x4183b1430> and will run it as-is.
2022-05-10 17:12:40.854303: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.