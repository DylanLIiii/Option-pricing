==================== fold_0 Training ====================
Epoch 1/100
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d5f77c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d5f77c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 39/110 [=========>....................] - ETA: 1s - loss: 14.4768 - val_loss: 14.4768
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
110/110 [==============================] - ETA: 0s - loss: 13.4324 - val_loss: 13.3323WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x404dfc550> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x404dfc550> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 3s 18ms/step - loss: 13.4324 - val_loss: 11.2837 - val_val_loss: 11.2470 - _timestamp: 1652173467.0000 - _runtime: 6.0000
Epoch 2/100
 71/110 [==================>...........] - ETA: 0s - loss: 12.7019 - val_loss: 12.7019
110/110 [==============================] - 1s 11ms/step - loss: 12.1283 - val_loss: 12.0638 - _timestamp: 1652173469.0000 - _runtime: 8.0000
Epoch 3/100
110/110 [==============================] - 1s 11ms/step - loss: 11.4481 - val_loss: 11.5564 - _timestamp: 1652173470.0000 - _runtime: 9.0000
Epoch 4/100
110/110 [==============================] - 1s 11ms/step - loss: 11.1956 - val_loss: 11.1289 - _timestamp: 1652173471.0000 - _runtime: 10.0000
Epoch 5/100
110/110 [==============================] - 1s 11ms/step - loss: 11.0328 - val_loss: 11.3158 - _timestamp: 1652173472.0000 - _runtime: 11.0000
Epoch 6/100
110/110 [==============================] - 1s 11ms/step - loss: 10.7512 - val_loss: 12.9354 - _timestamp: 1652173473.0000 - _runtime: 12.0000
Epoch 7/100
110/110 [==============================] - 1s 11ms/step - loss: 10.6953 - val_loss: 11.2394 - _timestamp: 1652173475.0000 - _runtime: 14.0000
Epoch 8/100
110/110 [==============================] - 1s 11ms/step - loss: 10.6811 - val_loss: 10.6883 - _timestamp: 1652173476.0000 - _runtime: 15.0000
Epoch 9/100
110/110 [==============================] - 1s 11ms/step - loss: 10.8460 - val_loss: 10.7633 - _timestamp: 1652173477.0000 - _runtime: 16.0000
Epoch 10/100
110/110 [==============================] - 1s 12ms/step - loss: 10.4973 - val_loss: 10.4195 - _timestamp: 1652173478.0000 - _runtime: 17.0000
Epoch 11/100
110/110 [==============================] - 1s 11ms/step - loss: 10.5871 - val_loss: 10.6186 - _timestamp: 1652173480.0000 - _runtime: 19.0000
Epoch 12/100
110/110 [==============================] - 1s 11ms/step - loss: 10.6416 - val_loss: 10.5493 - _timestamp: 1652173481.0000 - _runtime: 20.0000
Epoch 13/100
110/110 [==============================] - 1s 11ms/step - loss: 10.6330 - val_loss: 10.5531 - _timestamp: 1652173482.0000 - _runtime: 21.0000
Epoch 14/100
110/110 [==============================] - 1s 11ms/step - loss: 10.5262 - val_loss: 10.7119 - _timestamp: 1652173483.0000 - _runtime: 22.0000
Epoch 15/100
110/110 [==============================] - 1s 10ms/step - loss: 10.5479 - val_loss: 10.4733 - _timestamp: 1652173484.0000 - _runtime: 23.0000
Epoch 16/100
110/110 [==============================] - 1s 11ms/step - loss: 10.5857 - val_loss: 10.4976 - _timestamp: 1652173486.0000 - _runtime: 25.0000
Epoch 17/100
110/110 [==============================] - 1s 10ms/step - loss: 10.5336 - val_loss: 10.4494 - _timestamp: 1652173487.0000 - _runtime: 26.0000
Epoch 18/100
110/110 [==============================] - 1s 12ms/step - loss: 10.5712 - val_loss: 10.4885 - _timestamp: 1652173488.0000 - _runtime: 27.0000
Epoch 19/100
110/110 [==============================] - 2s 14ms/step - loss: 10.4057 - val_loss: 10.7127 - _timestamp: 1652173490.0000 - _runtime: 29.0000
Epoch 20/100
110/110 [==============================] - 1s 12ms/step - loss: 10.7052 - val_loss: 10.7183 - _timestamp: 1652173491.0000 - _runtime: 30.0000
Epoch 21/100
110/110 [==============================] - 1s 11ms/step - loss: 10.5011 - val_loss: 10.4196 - _timestamp: 1652173492.0000 - _runtime: 31.0000
Epoch 22/100
110/110 [==============================] - 1s 11ms/step - loss: 10.3702 - val_loss: 10.2903 - _timestamp: 1652173493.0000 - _runtime: 32.0000
Epoch 23/100
110/110 [==============================] - 1s 11ms/step - loss: 10.4818 - val_loss: 10.4101 - _timestamp: 1652173494.0000 - _runtime: 33.0000
Epoch 24/100
110/110 [==============================] - 1s 12ms/step - loss: 10.5750 - val_loss: 10.4864 - _timestamp: 1652173496.0000 - _runtime: 35.0000
Epoch 25/100
110/110 [==============================] - 1s 11ms/step - loss: 10.4984 - val_loss: 10.4112 - _timestamp: 1652173497.0000 - _runtime: 36.0000
Epoch 26/100
110/110 [==============================] - 1s 11ms/step - loss: 10.3654 - val_loss: 10.2896 - _timestamp: 1652173498.0000 - _runtime: 37.0000
Epoch 27/100
110/110 [==============================] - 1s 12ms/step - loss: 10.3690 - val_loss: 10.2879 - _timestamp: 1652173499.0000 - _runtime: 38.0000
Epoch 28/100
110/110 [==============================] - 1s 11ms/step - loss: 10.4459 - val_loss: 10.3606 - _timestamp: 1652173501.0000 - _runtime: 40.0000
Epoch 29/100
110/110 [==============================] - 1s 11ms/step - loss: 10.5110 - val_loss: 10.4539 - _timestamp: 1652173502.0000 - _runtime: 41.0000
Epoch 30/100
110/110 [==============================] - 1s 12ms/step - loss: 10.4872 - val_loss: 10.4055 - _timestamp: 1652173503.0000 - _runtime: 42.0000
Epoch 31/100
110/110 [==============================] - 1s 11ms/step - loss: 10.3789 - val_loss: 10.4069 - _timestamp: 1652173504.0000 - _runtime: 43.0000
Epoch 32/100
110/110 [==============================] - 1s 11ms/step - loss: 10.3382 - val_loss: 10.2623 - _timestamp: 1652173506.0000 - _runtime: 45.0000
Epoch 33/100
110/110 [==============================] - 1s 11ms/step - loss: 10.5445 - val_loss: 10.5719 - _timestamp: 1652173507.0000 - _runtime: 46.0000
Epoch 34/100
110/110 [==============================] - 1s 11ms/step - loss: 10.3836 - val_loss: 10.9788 - _timestamp: 1652173508.0000 - _runtime: 47.0000
Epoch 35/100
110/110 [==============================] - 1s 11ms/step - loss: 10.4375 - val_loss: 10.3546 - _timestamp: 1652173509.0000 - _runtime: 48.0000
Epoch 36/100
110/110 [==============================] - 1s 12ms/step - loss: 10.4310 - val_loss: 10.3421 - _timestamp: 1652173511.0000 - _runtime: 50.0000
Epoch 37/100
110/110 [==============================] - 1s 13ms/step - loss: 10.3356 - val_loss: 10.2494 - _timestamp: 1652173512.0000 - _runtime: 51.0000
Epoch 38/100
110/110 [==============================] - 1s 10ms/step - loss: 10.3797 - val_loss: 10.2923 - _timestamp: 1652173513.0000 - _runtime: 52.0000
Epoch 39/100
110/110 [==============================] - 1s 12ms/step - loss: 10.3271 - val_loss: 10.2726 - _timestamp: 1652173514.0000 - _runtime: 53.0000
Epoch 40/100
110/110 [==============================] - 1s 12ms/step - loss: 10.4253 - val_loss: 10.3458 - _timestamp: 1652173516.0000 - _runtime: 55.0000
Epoch 41/100
110/110 [==============================] - 1s 12ms/step - loss: 10.3904 - val_loss: 10.3057 - _timestamp: 1652173517.0000 - _runtime: 56.0000
Epoch 42/100
110/110 [==============================] - 1s 12ms/step - loss: 10.2812 - val_loss: 10.3522 - _timestamp: 1652173518.0000 - _runtime: 57.0000
Epoch 43/100
110/110 [==============================] - 1s 11ms/step - loss: 10.3535 - val_loss: 10.2639 - _timestamp: 1652173520.0000 - _runtime: 59.0000
Epoch 44/100
 65/110 [================>.............] - ETA: 0s - loss: 10.5159 - val_loss: 10.5159
 21/110 [====>.........................] - ETA: 0s - loss: 10.8793 - val_loss: 10.8793.2639 - _timestamp: 1652173520.0000 - _runtime: 59.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.4044 - val_loss: 10.3600 - _timestamp: 1652173522.0000 - _runtime: 61.0000
Epoch 46/100
110/110 [==============================] - 1s 12ms/step - loss: 10.3414 - val_loss: 11.4664 - _timestamp: 1652173524.0000 - _runtime: 63.0000
Epoch 47/100
 83/110 [=====================>........] - ETA: 0s - loss: 10.2752 - val_loss: 10.2752
 40/110 [=========>....................] - ETA: 0s - loss: 9.6700 - val_loss: 9.6700  .4664 - _timestamp: 1652173524.0000 - _runtime: 63.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.4669 - val_loss: 10.4166 - _timestamp: 1652173526.0000 - _runtime: 65.0000
Epoch 49/100
110/110 [==============================] - 2s 14ms/step - loss: 10.4941 - val_loss: 10.4269 - _timestamp: 1652173528.0000 - _runtime: 67.0000
Epoch 50/100
 25/110 [=====>........................] - ETA: 1s - loss: 11.2730 - val_loss: 11.2730
Epoch 51/100===========================] - 2s 14ms/step - loss: 10.4941 - val_loss: 10.4269 - _timestamp: 1652173528.0000 - _runtime: 67.0000
 91/110 [=======================>......] - ETA: 0s - loss: 10.0762 - val_loss: 10.0762
110/110 [==============================] - 1s 11ms/step - loss: 10.3679 - val_loss: 11.1698 - _timestamp: 1652173532.0000 - _runtime: 71.0000
Epoch 53/100
 40/110 [=========>....................] - ETA: 0s - loss: 9.8205 - val_loss: 9.8205
Epoch 54/100===========================] - 1s 11ms/step - loss: 10.3679 - val_loss: 11.1698 - _timestamp: 1652173532.0000 - _runtime: 71.0000
102/110 [==========================>...] - ETA: 0s - loss: 10.4183 - val_loss: 10.4183
110/110 [==============================] - 1s 11ms/step - loss: 10.4215 - val_loss: 10.4399 - _timestamp: 1652173536.0000 - _runtime: 75.0000
Epoch 56/100
 61/110 [===============>..............] - ETA: 0s - loss: 10.1394 - val_loss: 10.1394
 33/110 [========>.....................] - ETA: 0s - loss: 10.8086 - val_loss: 10.8086.4399 - _timestamp: 1652173536.0000 - _runtime: 75.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.4012 - val_loss: 10.3254 - _timestamp: 1652173538.0000 - _runtime: 77.0000
Epoch 58/100
110/110 [==============================] - 1s 10ms/step - loss: 10.3989 - val_loss: 10.3216 - _timestamp: 1652173539.0000 - _runtime: 78.0000
Epoch 59/100
108/110 [============================>.] - ETA: 0s - loss: 10.3581 - val_loss: 10.3581
 56/110 [==============>...............] - ETA: 0s - loss: 9.9218 - val_loss: 9.9218  .3216 - _timestamp: 1652173539.0000 - _runtime: 78.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.4217 - val_loss: 10.5125 - _timestamp: 1652173542.0000 - _runtime: 81.0000
Epoch 61/100
Epoch 63/100===========================] - 1s 13ms/step - loss: 10.1741 - val_loss: 10.0922 - _timestamp: 1652173543.0000 - _runtime: 82.0000
Epoch 62/100
110/110 [==============================] - 1s 11ms/step - loss: 10.2618 - val_loss: 10.1871 - _timestamp: 1652173544.0000 - _runtime: 83.0000
Epoch 63/100===========================] - 1s 13ms/step - loss: 10.1741 - val_loss: 10.0922 - _timestamp: 1652173543.0000 - _runtime: 82.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2745 - val_loss: 10.2296 - _timestamp: 1652173546.0000 - _runtime: 85.0000
Epoch 64/100
 72/110 [==================>...........] - ETA: 0s - loss: 10.4749 - val_loss: 10.4749
110/110 [==============================] - 1s 12ms/step - loss: 10.2888 - val_loss: 10.2085 - _timestamp: 1652173548.0000 - _runtime: 87.0000
Epoch 66/100
 24/110 [=====>........................] - ETA: 1s - loss: 10.5418 - val_loss: 10.5418
 86/110 [======================>.......] - ETA: 0s - loss: 10.6352 - val_loss: 10.6352.2085 - _timestamp: 1652173548.0000 - _runtime: 87.0000
 41/110 [==========>...................] - ETA: 0s - loss: 11.4265 - val_loss: 11.4265.4584 - _timestamp: 1652173551.0000 - _runtime: 90.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3742 - val_loss: 10.2853 - _timestamp: 1652173553.0000 - _runtime: 92.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.4615 - val_loss: 10.4349 - _timestamp: 1652173556.0000 - _runtime: 95.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.4551 - val_loss: 10.3701 - _timestamp: 1652173558.0000 - _runtime: 97.0000
105/110 [===========================>..] - ETA: 0s - loss: 10.3200 - val_loss: 10.3200.3701 - _timestamp: 1652173558.0000 - _runtime: 97.0000
 66/110 [=================>............] - ETA: 0s - loss: 10.5938 - val_loss: 10.5938.2970 - _timestamp: 1652173561.0000 - _runtime: 100.0000
 47/110 [===========>..................] - ETA: 0s - loss: 10.6594 - val_loss: 10.6594.5400 - _timestamp: 1652173563.0000 - _runtime: 102.0000
 14/110 [==>...........................] - ETA: 1s - loss: 9.2265 - val_loss: 9.226510.3639 - _timestamp: 1652173565.0000 - _runtime: 104.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.4355 - val_loss: 10.3592 - _timestamp: 1652173568.0000 - _runtime: 107.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3325 - val_loss: 10.2546 - _timestamp: 1652173570.0000 - _runtime: 109.0000
 93/110 [========================>.....] - ETA: 0s - loss: 10.4224 - val_loss: 10.4224.2546 - _timestamp: 1652173570.0000 - _runtime: 109.0000
 52/110 [=============>................] - ETA: 0s - loss: 9.4613 - val_loss: 9.4613  .5905 - _timestamp: 1652173573.0000 - _runtime: 112.0000
 11/110 [==>...........................] - ETA: 1s - loss: 10.8415 - val_loss: 10.8415.1678 - _timestamp: 1652173575.0000 - _runtime: 114.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1988 - val_loss: 12.0859 - _timestamp: 1652173578.0000 - _runtime: 117.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1988 - val_loss: 12.0859 - _timestamp: 1652173578.0000 - _runtime: 117.0000
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert _timestamp: 1652173580.0000 - _runtime: 119.0000
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert _timestamp: 1652173580.0000 - _runtime: 119.0000