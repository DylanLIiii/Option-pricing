/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
2022-05-10 13:47:47.385100: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e1fc3280> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e1fc3280> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 99/110 [==========================>...] - ETA: 0s - loss: 13.1816 - val_loss: 13.1816
109/110 [============================>.] - ETA: 0s - loss: 13.0239 - val_loss: 13.0239WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2e1c4ed30> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2e1c4ed30> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 2s 14ms/step - loss: 13.0215 - val_loss: 10.0162 - val_val_loss: 9.9769 - _timestamp: 1652161669.0000 - _runtime: 7.0000
Epoch 2/200
110/110 [==============================] - 1s 9ms/step - loss: 11.8341 - val_loss: 11.8343 - _timestamp: 1652161670.0000 - _runtime: 8.0000
Epoch 3/200
110/110 [==============================] - 1s 9ms/step - loss: 11.6343 - val_loss: 11.6150 - _timestamp: 1652161671.0000 - _runtime: 9.0000
Epoch 4/200
110/110 [==============================] - 1s 10ms/step - loss: 11.7815 - val_loss: 11.7209 - _timestamp: 1652161672.0000 - _runtime: 10.0000
Epoch 5/200
110/110 [==============================] - 1s 8ms/step - loss: 11.7344 - val_loss: 11.7505 - _timestamp: 1652161673.0000 - _runtime: 11.0000
Epoch 6/200
110/110 [==============================] - 1s 9ms/step - loss: 10.9773 - val_loss: 10.9980 - _timestamp: 1652161674.0000 - _runtime: 12.0000
Epoch 7/200
110/110 [==============================] - 1s 8ms/step - loss: 10.9617 - val_loss: 10.8952 - _timestamp: 1652161674.0000 - _runtime: 12.0000
Epoch 8/200
110/110 [==============================] - 1s 9ms/step - loss: 11.2512 - val_loss: 11.3079 - _timestamp: 1652161675.0000 - _runtime: 13.0000
Epoch 9/200
110/110 [==============================] - 1s 8ms/step - loss: 11.1560 - val_loss: 11.1797 - _timestamp: 1652161676.0000 - _runtime: 14.0000
Epoch 10/200
110/110 [==============================] - 1s 8ms/step - loss: 11.2031 - val_loss: 11.1232 - _timestamp: 1652161677.0000 - _runtime: 15.0000
Epoch 11/200
110/110 [==============================] - 1s 11ms/step - loss: 11.1441 - val_loss: 11.0946 - _timestamp: 1652161678.0000 - _runtime: 16.0000
Epoch 12/200
110/110 [==============================] - 1s 9ms/step - loss: 10.9235 - val_loss: 10.9507 - _timestamp: 1652161679.0000 - _runtime: 17.0000
Epoch 13/200
110/110 [==============================] - 1s 9ms/step - loss: 11.2658 - val_loss: 11.2050 - _timestamp: 1652161680.0000 - _runtime: 18.0000
Epoch 14/200
110/110 [==============================] - 1s 8ms/step - loss: 10.9102 - val_loss: 11.3940 - _timestamp: 1652161681.0000 - _runtime: 19.0000
Epoch 15/200
110/110 [==============================] - 1s 9ms/step - loss: 11.1159 - val_loss: 11.1046 - _timestamp: 1652161682.0000 - _runtime: 20.0000
Epoch 16/200
110/110 [==============================] - 1s 8ms/step - loss: 11.3754 - val_loss: 11.3075 - _timestamp: 1652161683.0000 - _runtime: 21.0000
Epoch 17/200
110/110 [==============================] - 1s 8ms/step - loss: 11.1484 - val_loss: 11.0705 - _timestamp: 1652161684.0000 - _runtime: 22.0000
Epoch 18/200
110/110 [==============================] - 1s 9ms/step - loss: 10.9531 - val_loss: 10.9338 - _timestamp: 1652161685.0000 - _runtime: 23.0000
Epoch 19/200
110/110 [==============================] - 1s 9ms/step - loss: 10.9968 - val_loss: 11.6408 - _timestamp: 1652161686.0000 - _runtime: 24.0000
Epoch 20/200
110/110 [==============================] - 1s 10ms/step - loss: 10.9934 - val_loss: 11.1975 - _timestamp: 1652161687.0000 - _runtime: 25.0000
Epoch 21/200
110/110 [==============================] - 1s 10ms/step - loss: 11.3404 - val_loss: 11.3290 - _timestamp: 1652161688.0000 - _runtime: 26.0000
Epoch 22/200
110/110 [==============================] - 1s 9ms/step - loss: 11.4277 - val_loss: 11.4383 - _timestamp: 1652161689.0000 - _runtime: 27.0000
Epoch 23/200
110/110 [==============================] - 1s 8ms/step - loss: 11.1602 - val_loss: 11.1825 - _timestamp: 1652161690.0000 - _runtime: 28.0000
Epoch 24/200
110/110 [==============================] - 1s 9ms/step - loss: 11.0021 - val_loss: 10.9240 - _timestamp: 1652161691.0000 - _runtime: 29.0000
Epoch 25/200
110/110 [==============================] - 1s 9ms/step - loss: 11.1861 - val_loss: 11.7367 - _timestamp: 1652161692.0000 - _runtime: 30.0000
Epoch 26/200
110/110 [==============================] - 1s 9ms/step - loss: 11.1376 - val_loss: 11.2400 - _timestamp: 1652161693.0000 - _runtime: 31.0000
Epoch 27/200
110/110 [==============================] - 1s 8ms/step - loss: 10.8699 - val_loss: 10.8067 - _timestamp: 1652161694.0000 - _runtime: 32.0000
Epoch 28/200
110/110 [==============================] - 1s 9ms/step - loss: 10.9620 - val_loss: 13.1224 - _timestamp: 1652161695.0000 - _runtime: 33.0000
Epoch 29/200
110/110 [==============================] - 1s 9ms/step - loss: 10.9756 - val_loss: 10.9779 - _timestamp: 1652161696.0000 - _runtime: 34.0000
Epoch 30/200
110/110 [==============================] - 1s 9ms/step - loss: 11.0314 - val_loss: 11.0285 - _timestamp: 1652161697.0000 - _runtime: 35.0000
Epoch 31/200
110/110 [==============================] - 1s 8ms/step - loss: 11.1248 - val_loss: 11.0788 - _timestamp: 1652161698.0000 - _runtime: 36.0000
Epoch 32/200
110/110 [==============================] - 1s 8ms/step - loss: 11.1992 - val_loss: 11.1872 - _timestamp: 1652161699.0000 - _runtime: 37.0000
Epoch 33/200
110/110 [==============================] - 1s 8ms/step - loss: 11.1769 - val_loss: 11.1000 - _timestamp: 1652161699.0000 - _runtime: 37.0000
Epoch 34/200
110/110 [==============================] - 1s 8ms/step - loss: 10.9098 - val_loss: 10.8616 - _timestamp: 1652161700.0000 - _runtime: 38.0000
Epoch 35/200
110/110 [==============================] - 1s 8ms/step - loss: 10.9642 - val_loss: 10.9828 - _timestamp: 1652161701.0000 - _runtime: 39.0000
Epoch 36/200
110/110 [==============================] - 1s 8ms/step - loss: 11.2197 - val_loss: 11.1925 - _timestamp: 1652161702.0000 - _runtime: 40.0000
Epoch 37/200
110/110 [==============================] - 1s 9ms/step - loss: 11.0037 - val_loss: 10.9912 - _timestamp: 1652161703.0000 - _runtime: 41.0000
Epoch 38/200
110/110 [==============================] - 1s 9ms/step - loss: 11.2831 - val_loss: 11.2436 - _timestamp: 1652161704.0000 - _runtime: 42.0000
Epoch 39/200
110/110 [==============================] - 1s 9ms/step - loss: 11.0837 - val_loss: 11.0747 - _timestamp: 1652161705.0000 - _runtime: 43.0000
Epoch 40/200
110/110 [==============================] - 1s 9ms/step - loss: 11.1839 - val_loss: 11.1945 - _timestamp: 1652161706.0000 - _runtime: 44.0000
Epoch 41/200
110/110 [==============================] - 1s 9ms/step - loss: 11.3679 - val_loss: 11.9105 - _timestamp: 1652161707.0000 - _runtime: 45.0000
Epoch 42/200
110/110 [==============================] - 1s 9ms/step - loss: 10.9528 - val_loss: 10.9185 - _timestamp: 1652161708.0000 - _runtime: 46.0000
Epoch 43/200
Epoch 45/200===========================] - 1s 10ms/step - loss: 10.9479 - val_loss: 10.9028 - _timestamp: 1652161709.0000 - _runtime: 47.0000
Epoch 44/200
110/110 [==============================] - 1s 8ms/step - loss: 11.1468 - val_loss: 11.1214 - _timestamp: 1652161710.0000 - _runtime: 48.0000
Epoch 45/200===========================] - 1s 10ms/step - loss: 10.9479 - val_loss: 10.9028 - _timestamp: 1652161709.0000 - _runtime: 47.0000
 86/110 [======================>.......] - ETA: 0s - loss: 10.6828 - val_loss: 10.68282025 - _timestamp: 1652161711.0000 - _runtime: 49.0000
Epoch 46/200
110/110 [==============================] - 1s 9ms/step - loss: 11.3879 - val_loss: 11.3715 - _timestamp: 1652161712.0000 - _runtime: 50.0000
Epoch 47/200
110/110 [==============================] - 1s 8ms/step - loss: 11.0059 - val_loss: 10.9694 - _timestamp: 1652161715.0000 - _runtime: 53.0000
Epoch 48/200
110/110 [==============================] - 1s 8ms/step - loss: 11.3247 - val_loss: 11.2562 - _timestamp: 1652161714.0000 - _runtime: 52.0000
Epoch 49/200
110/110 [==============================] - 1s 8ms/step - loss: 11.0059 - val_loss: 10.9694 - _timestamp: 1652161715.0000 - _runtime: 53.0000
Epoch 50/200
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert_timestamp: 1652161716.0000 - _runtime: 54.0000
Epoch 51/200
110/110 [==============================] - 1s 9ms/step - loss: 11.1224 - val_loss: 11.1017 - _timestamp: 1652161716.0000 - _runtime: 54.0000
WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e1c74310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert_timestamp: 1652161716.0000 - _runtime: 54.0000
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e1c74310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
==================== fold_0 score ====================
rmse: 32.03529678041659
2022-05-10 13:48:37.164935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.