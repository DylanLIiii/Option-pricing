/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
2022-05-10 16:31:14.801478: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3db055dc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3db055dc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 35/110 [========>.....................] - ETA: 1s - loss: 12.7402 - val_loss: 12.7402
110/110 [==============================] - ETA: 0s - loss: 12.2385 - val_loss: 12.2195WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x3a1da03a0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x3a1da03a0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 3s 19ms/step - loss: 12.2385 - val_loss: 10.9891 - val_val_loss: 10.9575 - _timestamp: 1652171477.0000 - _runtime: 7.0000
Epoch 2/200
110/110 [==============================] - 1s 12ms/step - loss: 10.7833 - val_loss: 10.9133 - _timestamp: 1652171478.0000 - _runtime: 8.0000
Epoch 3/200
110/110 [==============================] - 1s 13ms/step - loss: 10.7749 - val_loss: 10.8639 - _timestamp: 1652171480.0000 - _runtime: 10.0000
Epoch 4/200
110/110 [==============================] - 1s 12ms/step - loss: 10.6435 - val_loss: 10.5739 - _timestamp: 1652171481.0000 - _runtime: 11.0000
Epoch 5/200
110/110 [==============================] - 1s 12ms/step - loss: 10.6183 - val_loss: 10.5294 - _timestamp: 1652171482.0000 - _runtime: 12.0000
Epoch 6/200
110/110 [==============================] - 1s 12ms/step - loss: 10.4830 - val_loss: 10.4005 - _timestamp: 1652171483.0000 - _runtime: 13.0000
Epoch 7/200
110/110 [==============================] - 1s 12ms/step - loss: 10.3551 - val_loss: 10.2801 - _timestamp: 1652171485.0000 - _runtime: 15.0000
Epoch 8/200
110/110 [==============================] - 1s 11ms/step - loss: 10.5371 - val_loss: 10.4601 - _timestamp: 1652171486.0000 - _runtime: 16.0000
Epoch 9/200
110/110 [==============================] - 1s 11ms/step - loss: 10.3055 - val_loss: 10.4052 - _timestamp: 1652171487.0000 - _runtime: 17.0000
Epoch 10/200
110/110 [==============================] - 1s 11ms/step - loss: 10.5119 - val_loss: 10.7403 - _timestamp: 1652171489.0000 - _runtime: 19.0000
Epoch 11/200
110/110 [==============================] - 1s 11ms/step - loss: 10.4649 - val_loss: 10.3978 - _timestamp: 1652171490.0000 - _runtime: 20.0000
Epoch 12/200
110/110 [==============================] - 1s 11ms/step - loss: 10.4313 - val_loss: 10.3555 - _timestamp: 1652171491.0000 - _runtime: 21.0000
Epoch 13/200
110/110 [==============================] - 1s 11ms/step - loss: 10.4669 - val_loss: 10.3941 - _timestamp: 1652171492.0000 - _runtime: 22.0000
Epoch 14/200
110/110 [==============================] - 1s 11ms/step - loss: 10.3833 - val_loss: 10.3414 - _timestamp: 1652171493.0000 - _runtime: 23.0000
Epoch 15/200
110/110 [==============================] - 1s 11ms/step - loss: 10.4833 - val_loss: 11.2272 - _timestamp: 1652171495.0000 - _runtime: 25.0000
Epoch 16/200
110/110 [==============================] - 1s 11ms/step - loss: 10.5100 - val_loss: 10.4397 - _timestamp: 1652171496.0000 - _runtime: 26.0000
Epoch 17/200
110/110 [==============================] - 1s 11ms/step - loss: 10.3774 - val_loss: 10.3429 - _timestamp: 1652171497.0000 - _runtime: 27.0000
Epoch 18/200
110/110 [==============================] - 1s 12ms/step - loss: 10.1934 - val_loss: 10.1056 - _timestamp: 1652171499.0000 - _runtime: 29.0000
Epoch 19/200
110/110 [==============================] - 1s 11ms/step - loss: 10.3895 - val_loss: 10.6234 - _timestamp: 1652171500.0000 - _runtime: 30.0000
Epoch 20/200
110/110 [==============================] - 1s 12ms/step - loss: 10.2680 - val_loss: 10.1918 - _timestamp: 1652171501.0000 - _runtime: 31.0000
Epoch 21/200
110/110 [==============================] - 1s 12ms/step - loss: 10.3001 - val_loss: 10.2301 - _timestamp: 1652171502.0000 - _runtime: 32.0000
Epoch 22/200
110/110 [==============================] - 1s 11ms/step - loss: 10.4737 - val_loss: 10.4025 - _timestamp: 1652171504.0000 - _runtime: 34.0000
Epoch 23/200
110/110 [==============================] - 1s 12ms/step - loss: 10.3276 - val_loss: 10.2433 - _timestamp: 1652171505.0000 - _runtime: 35.0000
Epoch 24/200
110/110 [==============================] - 1s 12ms/step - loss: 10.4846 - val_loss: 10.5685 - _timestamp: 1652171506.0000 - _runtime: 36.0000
Epoch 25/200
110/110 [==============================] - 1s 11ms/step - loss: 10.4594 - val_loss: 10.4838 - _timestamp: 1652171508.0000 - _runtime: 38.0000
Epoch 26/200
110/110 [==============================] - 1s 12ms/step - loss: 10.3540 - val_loss: 10.3852 - _timestamp: 1652171509.0000 - _runtime: 39.0000
Epoch 27/200
110/110 [==============================] - 1s 11ms/step - loss: 10.3783 - val_loss: 10.2873 - _timestamp: 1652171510.0000 - _runtime: 40.0000
Epoch 28/200
110/110 [==============================] - 1s 12ms/step - loss: 10.3335 - val_loss: 11.3384 - _timestamp: 1652171511.0000 - _runtime: 41.0000
Epoch 29/200
110/110 [==============================] - 1s 12ms/step - loss: 10.1130 - val_loss: 10.0548 - _timestamp: 1652171513.0000 - _runtime: 43.0000
Epoch 30/200
110/110 [==============================] - 1s 11ms/step - loss: 10.3757 - val_loss: 10.3816 - _timestamp: 1652171514.0000 - _runtime: 44.0000
Epoch 31/200
110/110 [==============================] - 1s 11ms/step - loss: 10.1733 - val_loss: 10.2357 - _timestamp: 1652171515.0000 - _runtime: 45.0000
Epoch 32/200
110/110 [==============================] - 1s 11ms/step - loss: 10.3259 - val_loss: 10.5781 - _timestamp: 1652171516.0000 - _runtime: 46.0000
Epoch 33/200
110/110 [==============================] - 1s 11ms/step - loss: 10.2796 - val_loss: 10.6512 - _timestamp: 1652171518.0000 - _runtime: 48.0000
Epoch 34/200
110/110 [==============================] - 1s 12ms/step - loss: 10.1900 - val_loss: 10.3145 - _timestamp: 1652171519.0000 - _runtime: 49.0000
Epoch 35/200
110/110 [==============================] - 1s 12ms/step - loss: 10.1498 - val_loss: 10.0921 - _timestamp: 1652171520.0000 - _runtime: 50.0000
Epoch 36/200
110/110 [==============================] - 1s 11ms/step - loss: 10.2955 - val_loss: 10.2995 - _timestamp: 1652171522.0000 - _runtime: 52.0000
Epoch 37/200
110/110 [==============================] - 1s 10ms/step - loss: 10.3197 - val_loss: 10.2466 - _timestamp: 1652171523.0000 - _runtime: 53.0000
Epoch 38/200
110/110 [==============================] - 1s 11ms/step - loss: 10.2633 - val_loss: 10.3240 - _timestamp: 1652171524.0000 - _runtime: 54.0000
Epoch 39/200
110/110 [==============================] - 1s 10ms/step - loss: 10.1908 - val_loss: 10.1041 - _timestamp: 1652171525.0000 - _runtime: 55.0000
Epoch 40/200
110/110 [==============================] - 1s 11ms/step - loss: 10.2523 - val_loss: 10.4624 - _timestamp: 1652171526.0000 - _runtime: 56.0000
Epoch 41/200
110/110 [==============================] - 1s 11ms/step - loss: 10.5393 - val_loss: 10.4559 - _timestamp: 1652171527.0000 - _runtime: 57.0000
Epoch 42/200
Epoch 43/200===========================] - 1s 11ms/step - loss: 10.2598 - val_loss: 11.2370 - _timestamp: 1652171529.0000 - _runtime: 59.0000
Epoch 43/200===========================] - 1s 11ms/step - loss: 10.2598 - val_loss: 11.2370 - _timestamp: 1652171529.0000 - _runtime: 59.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2306 - val_loss: 10.1484 - _timestamp: 1652171530.0000 - _runtime: 60.0000
Epoch 44/200
110/110 [==============================] - 1s 11ms/step - loss: 10.2696 - val_loss: 10.1955 - _timestamp: 1652171531.0000 - _runtime: 61.0000
Epoch 45/200
 82/110 [=====================>........] - ETA: 0s - loss: 10.4145 - val_loss: 10.4145
110/110 [==============================] - 1s 10ms/step - loss: 10.1851 - val_loss: 10.1212 - _timestamp: 1652171533.0000 - _runtime: 63.0000
Epoch 47/200
 57/110 [==============>...............] - ETA: 0s - loss: 9.6521 - val_loss: 9.6521
110/110 [==============================] - 1s 11ms/step - loss: 10.3699 - val_loss: 10.5054 - _timestamp: 1652171536.0000 - _runtime: 66.0000
Epoch 49/200
 15/110 [===>..........................] - ETA: 1s - loss: 9.4019 - val_loss: 9.4019
 36/110 [========>.....................] - ETA: 0s - loss: 10.7866 - val_loss: 10.7866.5054 - _timestamp: 1652171536.0000 - _runtime: 66.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3588 - val_loss: 10.4763 - _timestamp: 1652171538.0000 - _runtime: 68.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.2219 - val_loss: 10.2245 - _timestamp: 1652171541.0000 - _runtime: 71.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3611 - val_loss: 10.2854 - _timestamp: 1652171543.0000 - _runtime: 73.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.0679 - val_loss: 10.0019 - _timestamp: 1652171546.0000 - _runtime: 76.0000
 81/110 [=====================>........] - ETA: 0s - loss: 10.0570 - val_loss: 10.0570.0019 - _timestamp: 1652171546.0000 - _runtime: 76.0000
 46/110 [===========>..................] - ETA: 0s - loss: 11.2291 - val_loss: 11.2291.0055 - _timestamp: 1652171548.0000 - _runtime: 78.0000
 11/110 [==>...........................] - ETA: 1s - loss: 9.8087 - val_loss: 9.8087  .1829 - _timestamp: 1652171550.0000 - _runtime: 80.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.3598 - val_loss: 10.2876 - _timestamp: 1652171553.0000 - _runtime: 83.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.2148 - val_loss: 10.2781 - _timestamp: 1652171555.0000 - _runtime: 85.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3416 - val_loss: 10.2540 - _timestamp: 1652171557.0000 - _runtime: 87.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1863 - val_loss: 10.1049 - _timestamp: 1652171560.0000 - _runtime: 90.0000
 69/110 [=================>............] - ETA: 0s - loss: 10.1517 - val_loss: 10.1517.1049 - _timestamp: 1652171560.0000 - _runtime: 90.0000
 26/110 [======>.......................] - ETA: 0s - loss: 8.2915 - val_loss: 8.291510.1738 - _timestamp: 1652171562.0000 - _runtime: 92.0000
  1/110 [..............................] - ETA: 1s - loss: 25.1269 - val_loss: 25.1269.2529 - _timestamp: 1652171565.0000 - _runtime: 95.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.2430 - val_loss: 10.2804 - _timestamp: 1652171567.0000 - _runtime: 97.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.2799 - val_loss: 10.2225 - _timestamp: 1652171569.0000 - _runtime: 99.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2613 - val_loss: 10.3281 - _timestamp: 1652171571.0000 - _runtime: 101.0000
107/110 [============================>.] - ETA: 0s - loss: 10.1515 - val_loss: 10.1515.3281 - _timestamp: 1652171571.0000 - _runtime: 101.0000
 71/110 [==================>...........] - ETA: 0s - loss: 10.8955 - val_loss: 10.8955.1862 - _timestamp: 1652171574.0000 - _runtime: 104.0000
 43/110 [==========>...................] - ETA: 0s - loss: 9.2840 - val_loss: 9.2840  .1584 - _timestamp: 1652171576.0000 - _runtime: 106.0000
 21/110 [====>.........................] - ETA: 0s - loss: 9.3912 - val_loss: 9.3912  .0236 - _timestamp: 1652171579.0000 - _runtime: 109.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.4063 - val_loss: 10.4046 - _timestamp: 1652171581.0000 - _runtime: 111.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2573 - val_loss: 10.6254 - _timestamp: 1652171583.0000 - _runtime: 113.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.2965 - val_loss: 10.2062 - _timestamp: 1652171586.0000 - _runtime: 116.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2287 - val_loss: 10.3970 - _timestamp: 1652171588.0000 - _runtime: 118.0000
 92/110 [========================>.....] - ETA: 0s - loss: 9.6479 - val_loss: 9.6479  .3970 - _timestamp: 1652171588.0000 - _runtime: 118.0000
 64/110 [================>.............] - ETA: 0s - loss: 9.7220 - val_loss: 9.7220  .3679 - _timestamp: 1652171590.0000 - _runtime: 120.0000
 33/110 [========>.....................] - ETA: 0s - loss: 9.1539 - val_loss: 9.1539  .1921 - _timestamp: 1652171593.0000 - _runtime: 123.0000
  6/110 [>.............................] - ETA: 1s - loss: 10.2995 - val_loss: 10.2995.0918 - _timestamp: 1652171595.0000 - _runtime: 125.0000
110/110 [==============================] - 1s 11ms/step - loss: 9.9449 - val_loss: 9.8771 - _timestamp: 1652171597.0000 - _runtime: 127.000000
110/110 [==============================] - 1s 11ms/step - loss: 10.4523 - val_loss: 10.4240 - _timestamp: 1652171600.0000 - _runtime: 130.0000
 61/110 [===============>..............] - ETA: 0s - loss: 9.7920 - val_loss: 9.7920  .4240 - _timestamp: 1652171600.0000 - _runtime: 130.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1487 - val_loss: 10.1153 - _timestamp: 1652171602.0000 - _runtime: 132.0000
110/110 [==============================] - 2s 15ms/step - loss: 10.2395 - val_loss: 10.2048 - _timestamp: 1652171605.0000 - _runtime: 135.0000
 47/110 [===========>..................] - ETA: 0s - loss: 11.0292 - val_loss: 11.0292.2048 - _timestamp: 1652171605.0000 - _runtime: 135.0000
 11/110 [==>...........................] - ETA: 1s - loss: 10.6278 - val_loss: 10.6278.2201 - _timestamp: 1652171608.0000 - _runtime: 138.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.1909 - val_loss: 10.1398 - _timestamp: 1652171611.0000 - _runtime: 141.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2693 - val_loss: 10.2248 - _timestamp: 1652171613.0000 - _runtime: 143.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1254 - val_loss: 10.0368 - _timestamp: 1652171615.0000 - _runtime: 145.0000
104/110 [===========================>..] - ETA: 0s - loss: 10.2880 - val_loss: 10.2880.0368 - _timestamp: 1652171615.0000 - _runtime: 145.0000
 69/110 [=================>............] - ETA: 0s - loss: 9.8076 - val_loss: 9.807610.1553 - _timestamp: 1652171618.0000 - _runtime: 148.0000
 21/110 [====>.........................] - ETA: 0s - loss: 7.8136 - val_loss: 7.813610.1566 - _timestamp: 1652171620.0000 - _runtime: 150.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.2546 - val_loss: 10.2014 - _timestamp: 1652171623.0000 - _runtime: 153.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3944 - val_loss: 10.4849 - _timestamp: 1652171625.0000 - _runtime: 155.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1502 - val_loss: 10.0627 - _timestamp: 1652171628.0000 - _runtime: 158.0000
 66/110 [=================>............] - ETA: 0s - loss: 10.1672 - val_loss: 10.1672.0627 - _timestamp: 1652171628.0000 - _runtime: 158.0000
 21/110 [====>.........................] - ETA: 0s - loss: 11.6302 - val_loss: 11.6302.1512 - _timestamp: 1652171630.0000 - _runtime: 160.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3143 - val_loss: 11.1616 - _timestamp: 1652171633.0000 - _runtime: 163.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2363 - val_loss: 10.1529 - _timestamp: 1652171635.0000 - _runtime: 165.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1760 - val_loss: 10.1022 - _timestamp: 1652171638.0000 - _runtime: 168.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1451 - val_loss: 10.0858 - _timestamp: 1652171640.0000 - _runtime: 170.0000
 71/110 [==================>...........] - ETA: 0s - loss: 9.8262 - val_loss: 9.8262  .0858 - _timestamp: 1652171640.0000 - _runtime: 170.0000
 16/110 [===>..........................] - ETA: 1s - loss: 10.7419 - val_loss: 10.7419.0010 - _timestamp: 1652171643.0000 - _runtime: 173.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.3540 - val_loss: 10.2652 - _timestamp: 1652171645.0000 - _runtime: 175.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.3304 - val_loss: 10.2499 - _timestamp: 1652171648.0000 - _runtime: 178.0000
 81/110 [=====================>........] - ETA: 0s - loss: 10.4155 - val_loss: 10.4155.2499 - _timestamp: 1652171648.0000 - _runtime: 178.0000
 37/110 [=========>....................] - ETA: 0s - loss: 10.3343 - val_loss: 10.3343.3174 - _timestamp: 1652171650.0000 - _runtime: 180.0000
 12/110 [==>...........................] - ETA: 0s - loss: 11.1285 - val_loss: 11.1285.2333 - _timestamp: 1652171653.0000 - _runtime: 183.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.0120 - val_loss: 10.1071 - _timestamp: 1652171655.0000 - _runtime: 185.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1338 - val_loss: 10.0730 - _timestamp: 1652171658.0000 - _runtime: 188.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2626 - val_loss: 10.1981 - _timestamp: 1652171660.0000 - _runtime: 190.0000
 91/110 [=======================>......] - ETA: 0s - loss: 9.9027 - val_loss: 9.9027  .1981 - _timestamp: 1652171660.0000 - _runtime: 190.0000
  6/110 [>.............................] - ETA: 1s - loss: 13.5354 - val_loss: 13.5354.0348 - _timestamp: 1652171662.0000 - _runtime: 192.0000
110/110 [==============================] - 1s 10ms/step - loss: 10.2322 - val_loss: 10.2937 - _timestamp: 1652171665.0000 - _runtime: 195.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1170 - val_loss: 10.0460 - _timestamp: 1652171667.0000 - _runtime: 197.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.1494 - val_loss: 10.0873 - _timestamp: 1652171670.0000 - _runtime: 200.0000
 67/110 [=================>............] - ETA: 0s - loss: 10.3133 - val_loss: 10.3133.0873 - _timestamp: 1652171670.0000 - _runtime: 200.0000
 31/110 [=======>......................] - ETA: 0s - loss: 12.0405 - val_loss: 12.0405.1141 - _timestamp: 1652171672.0000 - _runtime: 202.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2292 - val_loss: 10.1623 - _timestamp: 1652171675.0000 - _runtime: 205.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.3795 - val_loss: 10.5644 - _timestamp: 1652171677.0000 - _runtime: 207.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1626 - val_loss: 10.0920 - _timestamp: 1652171679.0000 - _runtime: 209.0000
 91/110 [=======================>......] - ETA: 0s - loss: 9.8030 - val_loss: 9.8030  .0920 - _timestamp: 1652171679.0000 - _runtime: 209.0000
 40/110 [=========>....................] - ETA: 0s - loss: 12.0600 - val_loss: 12.0600.1312 - _timestamp: 1652171682.0000 - _runtime: 212.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2942 - val_loss: 10.2158 - _timestamp: 1652171685.0000 - _runtime: 215.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.1673 - val_loss: 10.4594 - _timestamp: 1652171687.0000 - _runtime: 217.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.2944 - val_loss: 10.2295 - _timestamp: 1652171690.0000 - _runtime: 220.0000
 90/110 [=======================>......] - ETA: 0s - loss: 10.2479 - val_loss: 10.2479.2295 - _timestamp: 1652171690.0000 - _runtime: 220.0000
 68/110 [=================>............] - ETA: 0s - loss: 10.1494 - val_loss: 10.14949982 - _timestamp: 1652171692.0000 - _runtime: 222.00000
 36/110 [========>.....................] - ETA: 0s - loss: 10.7622 - val_loss: 10.7622.2017 - _timestamp: 1652171695.0000 - _runtime: 225.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.1692 - val_loss: 10.7137 - _timestamp: 1652171697.0000 - _runtime: 227.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.1513 - val_loss: 10.0908 - _timestamp: 1652171699.0000 - _runtime: 229.0000
110/110 [==============================] - 1s 11ms/step - loss: 10.0736 - val_loss: 10.0101 - _timestamp: 1652171702.0000 - _runtime: 232.0000
 75/110 [===================>..........] - ETA: 0s - loss: 9.6434 - val_loss: 9.6434  .0101 - _timestamp: 1652171702.0000 - _runtime: 232.0000
 42/110 [==========>...................] - ETA: 0s - loss: 11.6036 - val_loss: 11.6036.1140 - _timestamp: 1652171705.0000 - _runtime: 235.0000
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
rmse: 31.415209787099908e TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
rmse: 31.415209787099908e TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
2022-05-10 16:35:08.706163: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.