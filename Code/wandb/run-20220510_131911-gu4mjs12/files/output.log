==================== fold_0 Training ====================
Epoch 1/100
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x30303adc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x30303adc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
  1/110 [..............................] - ETA: 2:29 - loss: 13.1464 - val_loss: 13.1464
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.

110/110 [==============================] - ETA: 0s - loss: 14.5232 - val_loss: 14.5508WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x3006ae820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x3006ae820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 4s 24ms/step - loss: 14.5232 - val_loss: 11.7220 - val_val_loss: 11.6978 - _timestamp: 1652159959.0000 - _runtime: 8.0000
Epoch 2/100
 72/110 [==================>...........] - ETA: 0s - loss: 12.8542 - val_loss: 12.8542
110/110 [==============================] - 2s 20ms/step - loss: 12.5593 - val_loss: 12.4726 - _timestamp: 1652159961.0000 - _runtime: 10.0000
Epoch 3/100
110/110 [==============================] - 2s 19ms/step - loss: 11.6222 - val_loss: 12.2022 - _timestamp: 1652159963.0000 - _runtime: 12.0000
Epoch 4/100
110/110 [==============================] - 2s 20ms/step - loss: 11.1970 - val_loss: 11.1186 - _timestamp: 1652159965.0000 - _runtime: 14.0000
Epoch 5/100
110/110 [==============================] - 2s 20ms/step - loss: 11.0555 - val_loss: 11.1892 - _timestamp: 1652159968.0000 - _runtime: 17.0000
Epoch 6/100
110/110 [==============================] - 2s 21ms/step - loss: 10.9639 - val_loss: 10.8829 - _timestamp: 1652159970.0000 - _runtime: 19.0000
Epoch 7/100
110/110 [==============================] - 2s 20ms/step - loss: 10.8276 - val_loss: 10.8977 - _timestamp: 1652159972.0000 - _runtime: 21.0000
Epoch 8/100
110/110 [==============================] - 2s 21ms/step - loss: 10.6708 - val_loss: 10.5817 - _timestamp: 1652159974.0000 - _runtime: 23.0000
Epoch 9/100

110/110 [==============================] - 2s 20ms/step - loss: 10.8356 - val_loss: 10.9611 - _timestamp: 1652159977.0000 - _runtime: 26.0000
Epoch 10/100
110/110 [==============================] - 2s 20ms/step - loss: 10.6915 - val_loss: 11.5706 - _timestamp: 1652159979.0000 - _runtime: 28.0000
Epoch 11/100
110/110 [==============================] - 2s 20ms/step - loss: 10.7872 - val_loss: 10.7000 - _timestamp: 1652159981.0000 - _runtime: 30.0000
Epoch 12/100
110/110 [==============================] - 2s 20ms/step - loss: 10.5885 - val_loss: 10.8076 - _timestamp: 1652159983.0000 - _runtime: 32.0000
Epoch 13/100
110/110 [==============================] - 2s 20ms/step - loss: 10.5600 - val_loss: 10.5004 - _timestamp: 1652159985.0000 - _runtime: 34.0000
Epoch 14/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4786 - val_loss: 11.0463 - _timestamp: 1652159987.0000 - _runtime: 36.0000
Epoch 15/100
110/110 [==============================] - 2s 21ms/step - loss: 10.4408 - val_loss: 10.3550 - _timestamp: 1652159990.0000 - _runtime: 39.0000
Epoch 16/100
110/110 [==============================] - 2s 20ms/step - loss: 10.5417 - val_loss: 10.4793 - _timestamp: 1652159992.0000 - _runtime: 41.0000
Epoch 17/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4826 - val_loss: 11.5595 - _timestamp: 1652159994.0000 - _runtime: 43.0000
Epoch 18/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4908 - val_loss: 10.7008 - _timestamp: 1652159996.0000 - _runtime: 45.0000
Epoch 19/100
110/110 [==============================] - 2s 20ms/step - loss: 10.4842 - val_loss: 10.4022 - _timestamp: 1652159999.0000 - _runtime: 48.0000
Epoch 20/100
110/110 [==============================] - 2s 19ms/step - loss: 10.3754 - val_loss: 10.4363 - _timestamp: 1652160001.0000 - _runtime: 50.0000
Epoch 21/100

110/110 [==============================] - 2s 19ms/step - loss: 10.4290 - val_loss: 10.3645 - _timestamp: 1652160003.0000 - _runtime: 52.0000
Epoch 22/100
110/110 [==============================] - 2s 20ms/step - loss: 10.6686 - val_loss: 10.6088 - _timestamp: 1652160005.0000 - _runtime: 54.0000
Epoch 23/100
110/110 [==============================] - 2s 19ms/step - loss: 10.3563 - val_loss: 10.3401 - _timestamp: 1652160007.0000 - _runtime: 56.0000
Epoch 24/100
110/110 [==============================] - 2s 18ms/step - loss: 10.5284 - val_loss: 10.4361 - _timestamp: 1652160009.0000 - _runtime: 58.0000
Epoch 25/100
110/110 [==============================] - 2s 18ms/step - loss: 10.4851 - val_loss: 11.1498 - _timestamp: 1652160011.0000 - _runtime: 60.0000
Epoch 26/100
110/110 [==============================] - 2s 18ms/step - loss: 10.4370 - val_loss: 11.2117 - _timestamp: 1652160013.0000 - _runtime: 62.0000
Epoch 27/100
110/110 [==============================] - 2s 18ms/step - loss: 10.5254 - val_loss: 10.4423 - _timestamp: 1652160015.0000 - _runtime: 64.0000
Epoch 28/100
110/110 [==============================] - 2s 18ms/step - loss: 10.3174 - val_loss: 10.2595 - _timestamp: 1652160017.0000 - _runtime: 66.0000
Epoch 29/100
110/110 [==============================] - 2s 19ms/step - loss: 10.2728 - val_loss: 10.2130 - _timestamp: 1652160019.0000 - _runtime: 68.0000
Epoch 30/100
110/110 [==============================] - 2s 19ms/step - loss: 10.3465 - val_loss: 10.2558 - _timestamp: 1652160021.0000 - _runtime: 70.0000
Epoch 31/100
110/110 [==============================] - 2s 20ms/step - loss: 10.3674 - val_loss: 10.2868 - _timestamp: 1652160024.0000 - _runtime: 73.0000
Epoch 32/100
110/110 [==============================] - 2s 18ms/step - loss: 10.3578 - val_loss: 10.2714 - _timestamp: 1652160026.0000 - _runtime: 75.0000
Epoch 33/100
110/110 [==============================] - 2s 18ms/step - loss: 10.3229 - val_loss: 10.2658 - _timestamp: 1652160028.0000 - _runtime: 77.0000
Epoch 34/100
110/110 [==============================] - 2s 18ms/step - loss: 10.5521 - val_loss: 10.4759 - _timestamp: 1652160030.0000 - _runtime: 79.0000
Epoch 35/100
110/110 [==============================] - 2s 19ms/step - loss: 10.3913 - val_loss: 10.3048 - _timestamp: 1652160032.0000 - _runtime: 81.0000
Epoch 36/100
110/110 [==============================] - 2s 19ms/step - loss: 10.4303 - val_loss: 10.5223 - _timestamp: 1652160034.0000 - _runtime: 83.0000
Epoch 37/100
110/110 [==============================] - 2s 19ms/step - loss: 10.2862 - val_loss: 10.3225 - _timestamp: 1652160036.0000 - _runtime: 85.0000
Epoch 38/100
110/110 [==============================] - 2s 18ms/step - loss: 10.2998 - val_loss: 10.6037 - _timestamp: 1652160038.0000 - _runtime: 87.0000
Epoch 39/100
110/110 [==============================] - 2s 18ms/step - loss: 10.2865 - val_loss: 10.5614 - _timestamp: 1652160040.0000 - _runtime: 89.0000
Epoch 40/100
110/110 [==============================] - 2s 18ms/step - loss: 10.2739 - val_loss: 10.2166 - _timestamp: 1652160042.0000 - _runtime: 91.0000
Epoch 41/100
110/110 [==============================] - 2s 18ms/step - loss: 10.3241 - val_loss: 10.2449 - _timestamp: 1652160044.0000 - _runtime: 93.0000
Epoch 42/100
Epoch 43/100===========================] - 2s 19ms/step - loss: 10.2050 - val_loss: 10.1175 - _timestamp: 1652160046.0000 - _runtime: 95.0000
Epoch 43/100===========================] - 2s 19ms/step - loss: 10.2050 - val_loss: 10.1175 - _timestamp: 1652160046.0000 - _runtime: 95.0000
 40/110 [=========>....................] - ETA: 1s - loss: 10.7546 - val_loss: 10.7546.8640 - _timestamp: 1652160048.0000 - _runtime: 97.0000
Epoch 44/100
 40/110 [=========>....................] - ETA: 1s - loss: 9.7841 - val_loss: 9.7841  .4651 - _timestamp: 1652160050.0000 - _runtime: 99.0000
Epoch 45/100
 34/110 [========>.....................] - ETA: 1s - loss: 10.7696 - val_loss: 10.7696.2367 - _timestamp: 1652160052.0000 - _runtime: 101.0000
Epoch 46/100
 28/110 [======>.......................] - ETA: 1s - loss: 10.3693 - val_loss: 10.3693.2197 - _timestamp: 1652160054.0000 - _runtime: 103.0000
Epoch 47/100
 28/110 [======>.......................] - ETA: 1s - loss: 10.3749 - val_loss: 10.3749.2349 - _timestamp: 1652160056.0000 - _runtime: 105.0000
Epoch 48/100
 28/110 [======>.......................] - ETA: 1s - loss: 9.5621 - val_loss: 9.5621  .3809 - _timestamp: 1652160058.0000 - _runtime: 107.0000
Epoch 49/100
 28/110 [======>.......................] - ETA: 1s - loss: 9.3846 - val_loss: 9.3846  .2091 - _timestamp: 1652160060.0000 - _runtime: 109.0000
Epoch 50/100
 28/110 [======>.......................] - ETA: 1s - loss: 10.7066 - val_loss: 10.7066.8147 - _timestamp: 1652160062.0000 - _runtime: 111.0000
Epoch 51/100
 22/110 [=====>........................] - ETA: 1s - loss: 11.6446 - val_loss: 11.6446.2837 - _timestamp: 1652160064.0000 - _runtime: 113.0000
Epoch 52/100
 22/110 [=====>........................] - ETA: 1s - loss: 8.5751 - val_loss: 8.575110.3804 - _timestamp: 1652160066.0000 - _runtime: 115.0000
Epoch 53/100
 19/110 [====>.........................] - ETA: 1s - loss: 14.0483 - val_loss: 14.0483.2031 - _timestamp: 1652160068.0000 - _runtime: 117.0000
Epoch 54/100
 16/110 [===>..........................] - ETA: 1s - loss: 9.1886 - val_loss: 9.1886  .3371 - _timestamp: 1652160070.0000 - _runtime: 119.0000
Epoch 55/100
 16/110 [===>..........................] - ETA: 1s - loss: 9.5364 - val_loss: 9.5364  .2753 - _timestamp: 1652160073.0000 - _runtime: 122.0000
Epoch 56/100
 13/110 [==>...........................] - ETA: 1s - loss: 9.6608 - val_loss: 9.6608  .3239 - _timestamp: 1652160075.0000 - _runtime: 124.0000
Epoch 57/100
 13/110 [==>...........................] - ETA: 1s - loss: 6.4068 - val_loss: 6.406810.2419 - _timestamp: 1652160077.0000 - _runtime: 126.0000
Epoch 58/100
 13/110 [==>...........................] - ETA: 1s - loss: 11.0568 - val_loss: 11.0568.4035 - _timestamp: 1652160079.0000 - _runtime: 128.0000
Epoch 59/100
  7/110 [>.............................] - ETA: 1s - loss: 10.5575 - val_loss: 10.5575.3778 - _timestamp: 1652160081.0000 - _runtime: 130.0000
Epoch 60/100
 88/110 [=======================>......] - ETA: 0s - loss: 10.1207 - val_loss: 10.1207.3778 - _timestamp: 1652160081.0000 - _runtime: 130.0000
 83/110 [=====================>........] - ETA: 0s - loss: 10.6224 - val_loss: 10.6224.5493 - _timestamp: 1652160083.0000 - _runtime: 132.0000
 79/110 [====================>.........] - ETA: 0s - loss: 9.8414 - val_loss: 9.8414  .3343 - _timestamp: 1652160085.0000 - _runtime: 134.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.0775 - val_loss: 10.0775.3650 - _timestamp: 1652160087.0000 - _runtime: 136.0000
 73/110 [==================>...........] - ETA: 0s - loss: 10.1340 - val_loss: 10.1340.3294 - _timestamp: 1652160089.0000 - _runtime: 138.0000
 64/110 [================>.............] - ETA: 0s - loss: 10.9356 - val_loss: 10.9356.3762 - _timestamp: 1652160091.0000 - _runtime: 140.0000
 61/110 [===============>..............] - ETA: 0s - loss: 9.8063 - val_loss: 9.8063  .2419 - _timestamp: 1652160093.0000 - _runtime: 142.0000
 58/110 [==============>...............] - ETA: 0s - loss: 10.6911 - val_loss: 10.6911.2360 - _timestamp: 1652160095.0000 - _runtime: 144.0000
 58/110 [==============>...............] - ETA: 0s - loss: 10.2121 - val_loss: 10.2121.5409 - _timestamp: 1652160098.0000 - _runtime: 147.0000
 55/110 [==============>...............] - ETA: 1s - loss: 10.5733 - val_loss: 10.5733.2036 - _timestamp: 1652160100.0000 - _runtime: 149.0000
 49/110 [============>.................] - ETA: 1s - loss: 9.9016 - val_loss: 9.9016  .5757 - _timestamp: 1652160102.0000 - _runtime: 151.0000
 49/110 [============>.................] - ETA: 1s - loss: 9.3967 - val_loss: 9.3967  .1764 - _timestamp: 1652160104.0000 - _runtime: 153.0000
 41/110 [==========>...................] - ETA: 1s - loss: 11.3352 - val_loss: 11.3352.3201 - _timestamp: 1652160106.0000 - _runtime: 155.0000
 34/110 [========>.....................] - ETA: 1s - loss: 10.7661 - val_loss: 10.7661.4072 - _timestamp: 1652160108.0000 - _runtime: 157.0000
 28/110 [======>.......................] - ETA: 1s - loss: 12.0258 - val_loss: 12.0258.3006 - _timestamp: 1652160110.0000 - _runtime: 159.0000
 22/110 [=====>........................] - ETA: 1s - loss: 9.0466 - val_loss: 9.0466  .4152 - _timestamp: 1652160112.0000 - _runtime: 161.0000
 16/110 [===>..........................] - ETA: 1s - loss: 11.8153 - val_loss: 11.8153.8729 - _timestamp: 1652160114.0000 - _runtime: 163.0000
 10/110 [=>............................] - ETA: 1s - loss: 7.7387 - val_loss: 7.738710.1779 - _timestamp: 1652160117.0000 - _runtime: 166.0000
  7/110 [>.............................] - ETA: 2s - loss: 8.0953 - val_loss: 8.0953  .1650 - _timestamp: 1652160119.0000 - _runtime: 168.0000
107/110 [============================>.] - ETA: 0s - loss: 10.3324 - val_loss: 10.3324.1650 - _timestamp: 1652160119.0000 - _runtime: 168.0000
103/110 [===========================>..] - ETA: 0s - loss: 10.4948 - val_loss: 10.4948.2889 - _timestamp: 1652160121.0000 - _runtime: 170.0000
 96/110 [=========================>....] - ETA: 0s - loss: 10.5134 - val_loss: 10.5134.2607 - _timestamp: 1652160123.0000 - _runtime: 172.0000
 93/110 [========================>.....] - ETA: 0s - loss: 10.2851 - val_loss: 10.2851.3418 - _timestamp: 1652160125.0000 - _runtime: 174.0000
 85/110 [======================>.......] - ETA: 0s - loss: 10.2792 - val_loss: 10.2792.1555 - _timestamp: 1652160127.0000 - _runtime: 176.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.3079 - val_loss: 10.3079.0131 - _timestamp: 1652160129.0000 - _runtime: 178.0000
 73/110 [==================>...........] - ETA: 0s - loss: 9.4891 - val_loss: 9.4891  .2898 - _timestamp: 1652160132.0000 - _runtime: 181.0000
 67/110 [=================>............] - ETA: 0s - loss: 10.4694 - val_loss: 10.4694.2164 - _timestamp: 1652160134.0000 - _runtime: 183.0000
 64/110 [================>.............] - ETA: 0s - loss: 10.3530 - val_loss: 10.3530.3065 - _timestamp: 1652160136.0000 - _runtime: 185.0000
 58/110 [==============>...............] - ETA: 1s - loss: 10.2714 - val_loss: 10.2714.3354 - _timestamp: 1652160138.0000 - _runtime: 187.0000
 55/110 [==============>...............] - ETA: 1s - loss: 9.8872 - val_loss: 9.8872  .2356 - _timestamp: 1652160140.0000 - _runtime: 189.0000
 43/110 [==========>...................] - ETA: 1s - loss: 10.8589 - val_loss: 10.8589.1968 - _timestamp: 1652160142.0000 - _runtime: 191.0000
 37/110 [=========>....................] - ETA: 1s - loss: 8.8024 - val_loss: 8.802410.1363 - _timestamp: 1652160144.0000 - _runtime: 193.0000
  7/110 [>.............................] - ETA: 2s - loss: 12.0977 - val_loss: 12.0977.0558 - _timestamp: 1652160146.0000 - _runtime: 195.0000
  4/110 [>.............................] - ETA: 2s - loss: 13.2484 - val_loss: 13.2484.2329 - _timestamp: 1652160148.0000 - _runtime: 197.0000
  4/110 [>.............................] - ETA: 2s - loss: 10.9447 - val_loss: 10.9447.2861 - _timestamp: 1652160151.0000 - _runtime: 200.0000
  1/110 [..............................] - ETA: 2s - loss: 15.9887 - val_loss: 15.9887.1413 - _timestamp: 1652160153.0000 - _runtime: 202.0000
110/110 [==============================] - 2s 18ms/step - loss: 10.1933 - val_loss: 10.1325 - _timestamp: 1652160155.0000 - _runtime: 204.0000
110/110 [==============================] - 2s 18ms/step - loss: 10.2642 - val_loss: 10.1816 - _timestamp: 1652160157.0000 - _runtime: 206.0000
106/110 [===========================>..] - ETA: 0s - loss: 10.1604 - val_loss: 10.1604.1816 - _timestamp: 1652160157.0000 - _runtime: 206.0000
102/110 [==========================>...] - ETA: 0s - loss: 10.1835 - val_loss: 10.1835.4763 - _timestamp: 1652160159.0000 - _runtime: 208.0000
 97/110 [=========================>....] - ETA: 0s - loss: 10.2063 - val_loss: 10.2063.0494 - _timestamp: 1652160161.0000 - _runtime: 210.0000
 94/110 [========================>.....] - ETA: 0s - loss: 10.6759 - val_loss: 10.6759.2278 - _timestamp: 1652160163.0000 - _runtime: 212.0000
 94/110 [========================>.....] - ETA: 0s - loss: 10.3447 - val_loss: 10.3447.4604 - _timestamp: 1652160165.0000 - _runtime: 214.0000
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertinux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertinux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
==================== fold_0 score ====================
rmse: 31.30113017593871
2022-05-10 13:22:47.738293: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.