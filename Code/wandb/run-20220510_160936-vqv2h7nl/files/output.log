/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
2022-05-10 16:09:40.490125: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d6e43e50> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d6e43e50> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 27/110 [======>.......................] - ETA: 2s - loss: 15.9353 - val_loss: 15.9353
110/110 [==============================] - ETA: 0s - loss: 13.2959 - val_loss: 13.3099WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d7349c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d7349c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 3s 20ms/step - loss: 13.2959 - val_loss: 12.0742 - val_val_loss: 12.0161 - _timestamp: 1652170183.0000 - _runtime: 7.0000
Epoch 2/200
110/110 [==============================] - 1s 12ms/step - loss: 11.4952 - val_loss: 11.5960 - _timestamp: 1652170184.0000 - _runtime: 8.0000
Epoch 3/200
110/110 [==============================] - 1s 12ms/step - loss: 10.9986 - val_loss: 10.9300 - _timestamp: 1652170185.0000 - _runtime: 9.0000
Epoch 4/200
110/110 [==============================] - 1s 13ms/step - loss: 10.7332 - val_loss: 10.6485 - _timestamp: 1652170187.0000 - _runtime: 11.0000
Epoch 5/200
110/110 [==============================] - 1s 13ms/step - loss: 10.6531 - val_loss: 10.5803 - _timestamp: 1652170188.0000 - _runtime: 12.0000
Epoch 6/200
110/110 [==============================] - 2s 14ms/step - loss: 10.6943 - val_loss: 10.6228 - _timestamp: 1652170190.0000 - _runtime: 14.0000
Epoch 7/200
110/110 [==============================] - 1s 13ms/step - loss: 10.5776 - val_loss: 10.8346 - _timestamp: 1652170191.0000 - _runtime: 15.0000
Epoch 8/200
110/110 [==============================] - 1s 13ms/step - loss: 10.5337 - val_loss: 11.4235 - _timestamp: 1652170193.0000 - _runtime: 17.0000
Epoch 9/200
110/110 [==============================] - 2s 16ms/step - loss: 10.6150 - val_loss: 10.5470 - _timestamp: 1652170194.0000 - _runtime: 18.0000
Epoch 10/200
110/110 [==============================] - 2s 15ms/step - loss: 10.5580 - val_loss: 10.4867 - _timestamp: 1652170196.0000 - _runtime: 20.0000
Epoch 11/200
110/110 [==============================] - 1s 13ms/step - loss: 10.5717 - val_loss: 10.4965 - _timestamp: 1652170198.0000 - _runtime: 22.0000
Epoch 12/200
110/110 [==============================] - 2s 14ms/step - loss: 10.5202 - val_loss: 10.4491 - _timestamp: 1652170199.0000 - _runtime: 23.0000
Epoch 13/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3717 - val_loss: 10.3129 - _timestamp: 1652170201.0000 - _runtime: 25.0000
Epoch 14/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3852 - val_loss: 10.8005 - _timestamp: 1652170202.0000 - _runtime: 26.0000
Epoch 15/200
110/110 [==============================] - 2s 15ms/step - loss: 10.2430 - val_loss: 10.1554 - _timestamp: 1652170204.0000 - _runtime: 28.0000
Epoch 16/200
110/110 [==============================] - 1s 14ms/step - loss: 10.3162 - val_loss: 10.2464 - _timestamp: 1652170205.0000 - _runtime: 29.0000
Epoch 17/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4612 - val_loss: 10.3886 - _timestamp: 1652170207.0000 - _runtime: 31.0000
Epoch 18/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4453 - val_loss: 10.3715 - _timestamp: 1652170208.0000 - _runtime: 32.0000
Epoch 19/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4981 - val_loss: 10.4215 - _timestamp: 1652170210.0000 - _runtime: 34.0000
Epoch 20/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4493 - val_loss: 10.4719 - _timestamp: 1652170211.0000 - _runtime: 35.0000
Epoch 21/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4083 - val_loss: 10.3440 - _timestamp: 1652170213.0000 - _runtime: 37.0000
Epoch 22/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3880 - val_loss: 10.6223 - _timestamp: 1652170214.0000 - _runtime: 38.0000
Epoch 23/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4284 - val_loss: 10.4563 - _timestamp: 1652170216.0000 - _runtime: 40.0000
Epoch 24/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3847 - val_loss: 11.0943 - _timestamp: 1652170217.0000 - _runtime: 41.0000
Epoch 25/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4650 - val_loss: 11.2383 - _timestamp: 1652170219.0000 - _runtime: 43.0000
Epoch 26/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3049 - val_loss: 10.2207 - _timestamp: 1652170220.0000 - _runtime: 44.0000
Epoch 27/200
110/110 [==============================] - 1s 12ms/step - loss: 10.3310 - val_loss: 10.3051 - _timestamp: 1652170222.0000 - _runtime: 46.0000
Epoch 28/200
110/110 [==============================] - 1s 13ms/step - loss: 10.5145 - val_loss: 10.4435 - _timestamp: 1652170223.0000 - _runtime: 47.0000
Epoch 29/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4472 - val_loss: 10.3647 - _timestamp: 1652170224.0000 - _runtime: 48.0000
Epoch 30/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4206 - val_loss: 10.3410 - _timestamp: 1652170226.0000 - _runtime: 50.0000
Epoch 31/200
110/110 [==============================] - 2s 15ms/step - loss: 10.2423 - val_loss: 10.1751 - _timestamp: 1652170228.0000 - _runtime: 52.0000
Epoch 32/200
110/110 [==============================] - 2s 16ms/step - loss: 10.2875 - val_loss: 10.2153 - _timestamp: 1652170229.0000 - _runtime: 53.0000
Epoch 33/200
110/110 [==============================] - 2s 16ms/step - loss: 10.3270 - val_loss: 10.3979 - _timestamp: 1652170231.0000 - _runtime: 55.0000
Epoch 34/200
110/110 [==============================] - 2s 16ms/step - loss: 10.1744 - val_loss: 10.1064 - _timestamp: 1652170233.0000 - _runtime: 57.0000
Epoch 35/200
110/110 [==============================] - 2s 14ms/step - loss: 10.1798 - val_loss: 10.1957 - _timestamp: 1652170234.0000 - _runtime: 58.0000
Epoch 36/200
110/110 [==============================] - 2s 16ms/step - loss: 10.2564 - val_loss: 10.1825 - _timestamp: 1652170236.0000 - _runtime: 60.0000
Epoch 37/200
110/110 [==============================] - 2s 17ms/step - loss: 10.0568 - val_loss: 10.2173 - _timestamp: 1652170238.0000 - _runtime: 62.0000
Epoch 38/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2303 - val_loss: 10.2748 - _timestamp: 1652170240.0000 - _runtime: 64.0000
Epoch 39/200
110/110 [==============================] - 2s 14ms/step - loss: 10.2568 - val_loss: 10.1865 - _timestamp: 1652170241.0000 - _runtime: 65.0000
Epoch 40/200
110/110 [==============================] - 1s 13ms/step - loss: 10.2084 - val_loss: 10.5056 - _timestamp: 1652170243.0000 - _runtime: 67.0000
Epoch 41/200
110/110 [==============================] - 2s 18ms/step - loss: 10.3463 - val_loss: 10.2707 - _timestamp: 1652170245.0000 - _runtime: 69.0000
Epoch 42/200
Epoch 43/200===========================] - 2s 15ms/step - loss: 10.4180 - val_loss: 10.3273 - _timestamp: 1652170246.0000 - _runtime: 70.0000
Epoch 43/200===========================] - 2s 15ms/step - loss: 10.4180 - val_loss: 10.3273 - _timestamp: 1652170246.0000 - _runtime: 70.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2519 - val_loss: 10.1713 - _timestamp: 1652170249.0000 - _runtime: 73.0000
Epoch 44/200
110/110 [==============================] - 2s 14ms/step - loss: 10.2519 - val_loss: 10.1713 - _timestamp: 1652170249.0000 - _runtime: 73.0000
Epoch 45/200
110/110 [==============================] - 1s 13ms/step - loss: 10.3355 - val_loss: 10.2558 - _timestamp: 1652170251.0000 - _runtime: 75.0000
Epoch 46/200
 58/110 [==============>...............] - ETA: 0s - loss: 10.2471 - val_loss: 10.2471
106/110 [===========================>..] - ETA: 0s - loss: 10.4664 - val_loss: 10.4664.2558 - _timestamp: 1652170251.0000 - _runtime: 75.0000
 21/110 [====>.........................] - ETA: 1s - loss: 9.3216 - val_loss: 9.3216  .3699 - _timestamp: 1652170253.0000 - _runtime: 77.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3051 - val_loss: 10.2352 - _timestamp: 1652170257.0000 - _runtime: 81.0000
 79/110 [====================>.........] - ETA: 0s - loss: 10.2177 - val_loss: 10.2177.2352 - _timestamp: 1652170257.0000 - _runtime: 81.0000
  1/110 [..............................] - ETA: 1s - loss: 3.3141 - val_loss: 3.314110.4487 - _timestamp: 1652170260.0000 - _runtime: 84.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2799 - val_loss: 10.2824 - _timestamp: 1652170263.0000 - _runtime: 87.0000
 79/110 [====================>.........] - ETA: 0s - loss: 10.0131 - val_loss: 10.0131.2824 - _timestamp: 1652170263.0000 - _runtime: 87.0000
  1/110 [..............................] - ETA: 1s - loss: 8.3502 - val_loss: 8.350210.3543 - _timestamp: 1652170266.0000 - _runtime: 90.0000
110/110 [==============================] - 2s 15ms/step - loss: 10.0858 - val_loss: 9.9975 - _timestamp: 1652170269.0000 - _runtime: 93.00000
 58/110 [==============>...............] - ETA: 0s - loss: 8.7612 - val_loss: 8.76129.9975 - _timestamp: 1652170269.0000 - _runtime: 93.00000
110/110 [==============================] - 2s 14ms/step - loss: 10.1900 - val_loss: 10.1155 - _timestamp: 1652170272.0000 - _runtime: 96.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2654 - val_loss: 10.2247 - _timestamp: 1652170275.0000 - _runtime: 99.0000
 53/110 [=============>................] - ETA: 0s - loss: 11.1077 - val_loss: 11.1077.2247 - _timestamp: 1652170275.0000 - _runtime: 99.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2736 - val_loss: 10.1963 - _timestamp: 1652170278.0000 - _runtime: 102.0000
110/110 [==============================] - 2s 15ms/step - loss: 10.5373 - val_loss: 10.4533 - _timestamp: 1652170282.0000 - _runtime: 106.0000
 41/110 [==========>...................] - ETA: 0s - loss: 9.4952 - val_loss: 9.4952  .4533 - _timestamp: 1652170282.0000 - _runtime: 106.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3393 - val_loss: 10.2543 - _timestamp: 1652170285.0000 - _runtime: 109.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2182 - val_loss: 10.1286 - _timestamp: 1652170287.0000 - _runtime: 111.0000
 50/110 [============>.................] - ETA: 0s - loss: 9.7241 - val_loss: 9.724110.1286 - _timestamp: 1652170287.0000 - _runtime: 111.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1870 - val_loss: 10.1144 - _timestamp: 1652170291.0000 - _runtime: 115.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3247 - val_loss: 10.2597 - _timestamp: 1652170294.0000 - _runtime: 118.0000
 38/110 [=========>....................] - ETA: 0s - loss: 8.8400 - val_loss: 8.8400  .2597 - _timestamp: 1652170294.0000 - _runtime: 118.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3050 - val_loss: 10.2198 - _timestamp: 1652170297.0000 - _runtime: 121.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3191 - val_loss: 10.2734 - _timestamp: 1652170300.0000 - _runtime: 124.0000
 41/110 [==========>...................] - ETA: 0s - loss: 10.6333 - val_loss: 10.6333.2734 - _timestamp: 1652170300.0000 - _runtime: 124.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1794 - val_loss: 10.1254 - _timestamp: 1652170303.0000 - _runtime: 127.0000
 77/110 [====================>.........] - ETA: 0s - loss: 10.8327 - val_loss: 10.8327.1254 - _timestamp: 1652170303.0000 - _runtime: 127.0000
 10/110 [=>............................] - ETA: 1s - loss: 9.1172 - val_loss: 9.117210.1915 - _timestamp: 1652170306.0000 - _runtime: 130.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2376 - val_loss: 10.1637 - _timestamp: 1652170309.0000 - _runtime: 133.0000
 88/110 [=======================>......] - ETA: 0s - loss: 9.8618 - val_loss: 9.8618  .1637 - _timestamp: 1652170309.0000 - _runtime: 133.0000
 17/110 [===>..........................] - ETA: 1s - loss: 9.2233 - val_loss: 9.2233  .1161 - _timestamp: 1652170312.0000 - _runtime: 136.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2036 - val_loss: 10.1609 - _timestamp: 1652170315.0000 - _runtime: 139.0000
 92/110 [========================>.....] - ETA: 0s - loss: 10.3518 - val_loss: 10.3518.1609 - _timestamp: 1652170315.0000 - _runtime: 139.0000
 27/110 [======>.......................] - ETA: 1s - loss: 10.3249 - val_loss: 10.3249.2274 - _timestamp: 1652170318.0000 - _runtime: 142.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2951 - val_loss: 10.2081 - _timestamp: 1652170321.0000 - _runtime: 145.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1690 - val_loss: 10.0956 - _timestamp: 1652170323.0000 - _runtime: 147.0000
 54/110 [=============>................] - ETA: 0s - loss: 11.0757 - val_loss: 11.0757.0956 - _timestamp: 1652170323.0000 - _runtime: 147.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2464 - val_loss: 10.1719 - _timestamp: 1652170326.0000 - _runtime: 150.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1872 - val_loss: 10.3288 - _timestamp: 1652170329.0000 - _runtime: 153.0000
 64/110 [================>.............] - ETA: 0s - loss: 10.1685 - val_loss: 10.1685.3288 - _timestamp: 1652170329.0000 - _runtime: 153.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2198 - val_loss: 10.1361 - _timestamp: 1652170332.0000 - _runtime: 156.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1467 - val_loss: 10.0695 - _timestamp: 1652170335.0000 - _runtime: 159.0000
 67/110 [=================>............] - ETA: 0s - loss: 10.5360 - val_loss: 10.5360.0695 - _timestamp: 1652170335.0000 - _runtime: 159.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1887 - val_loss: 10.1122 - _timestamp: 1652170338.0000 - _runtime: 162.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1788 - val_loss: 10.2269 - _timestamp: 1652170341.0000 - _runtime: 165.0000
 62/110 [===============>..............] - ETA: 0s - loss: 10.3829 - val_loss: 10.3829.2269 - _timestamp: 1652170341.0000 - _runtime: 165.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1806 - val_loss: 10.1135 - _timestamp: 1652170344.0000 - _runtime: 168.0000
110/110 [==============================] - 1s 14ms/step - loss: 10.0996 - val_loss: 10.0210 - _timestamp: 1652170347.0000 - _runtime: 171.0000
 67/110 [=================>............] - ETA: 0s - loss: 9.8800 - val_loss: 9.8800  .0210 - _timestamp: 1652170347.0000 - _runtime: 171.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.0985 - val_loss: 10.0243 - _timestamp: 1652170350.0000 - _runtime: 174.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2114 - val_loss: 10.1946 - _timestamp: 1652170353.0000 - _runtime: 177.0000
 65/110 [================>.............] - ETA: 0s - loss: 10.3556 - val_loss: 10.3556.1946 - _timestamp: 1652170353.0000 - _runtime: 177.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2322 - val_loss: 10.1568 - _timestamp: 1652170356.0000 - _runtime: 180.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3561 - val_loss: 10.9432 - _timestamp: 1652170359.0000 - _runtime: 183.0000
 65/110 [================>.............] - ETA: 0s - loss: 9.7870 - val_loss: 9.7870  .9432 - _timestamp: 1652170359.0000 - _runtime: 183.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.0436 - val_loss: 9.9783 - _timestamp: 1652170362.0000 - _runtime: 186.00000
110/110 [==============================] - 1s 13ms/step - loss: 10.2454 - val_loss: 10.1875 - _timestamp: 1652170365.0000 - _runtime: 189.0000
 44/110 [===========>..................] - ETA: 0s - loss: 9.9217 - val_loss: 9.9217  .1875 - _timestamp: 1652170365.0000 - _runtime: 189.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.1244 - val_loss: 10.0543 - _timestamp: 1652170368.0000 - _runtime: 192.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1090 - val_loss: 10.0380 - _timestamp: 1652170371.0000 - _runtime: 195.0000
 73/110 [==================>...........] - ETA: 0s - loss: 10.1747 - val_loss: 10.1747.0380 - _timestamp: 1652170371.0000 - _runtime: 195.0000
  5/110 [>.............................] - ETA: 1s - loss: 11.0644 - val_loss: 11.0644.3221 - _timestamp: 1652170374.0000 - _runtime: 198.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.2206 - val_loss: 10.1484 - _timestamp: 1652170377.0000 - _runtime: 201.0000
110/110 [==============================] - 1s 12ms/step - loss: 10.3738 - val_loss: 10.3210 - _timestamp: 1652170380.0000 - _runtime: 204.0000
 49/110 [============>.................] - ETA: 0s - loss: 9.8989 - val_loss: 9.8989  .3210 - _timestamp: 1652170380.0000 - _runtime: 204.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2095 - val_loss: 10.1272 - _timestamp: 1652170382.0000 - _runtime: 206.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2520 - val_loss: 10.1653 - _timestamp: 1652170385.0000 - _runtime: 209.0000
 49/110 [============>.................] - ETA: 0s - loss: 10.1783 - val_loss: 10.1783.1653 - _timestamp: 1652170385.0000 - _runtime: 209.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1816 - val_loss: 10.1076 - _timestamp: 1652170388.0000 - _runtime: 212.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2249 - val_loss: 10.3682 - _timestamp: 1652170391.0000 - _runtime: 215.0000
 54/110 [=============>................] - ETA: 0s - loss: 9.7861 - val_loss: 9.7861  .3682 - _timestamp: 1652170391.0000 - _runtime: 215.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.1257 - val_loss: 10.0445 - _timestamp: 1652170394.0000 - _runtime: 218.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1730 - val_loss: 10.0929 - _timestamp: 1652170397.0000 - _runtime: 221.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1730 - val_loss: 10.0929 - _timestamp: 1652170397.0000 - _runtime: 221.0000
rmse: 31.46529396996892he TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
rmse: 31.46529396996892he TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.