==================== fold_0 Training ====================
Epoch 1/200
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x33cc1cf70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x33cc1cf70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
 31/110 [=======>......................] - ETA: 1s - loss: 15.5490 - val_loss: 15.5490
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
110/110 [==============================] - ETA: 0s - loss: 13.5835 - val_loss: 13.4966WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d62a9820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2d62a9820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 3s 21ms/step - loss: 13.5835 - val_loss: 11.3687 - val_val_loss: 11.3243 - _timestamp: 1652167025.0000 - _runtime: 7.0000
Epoch 2/200
 25/110 [=====>........................] - ETA: 1s - loss: 12.5632 - val_loss: 12.5632
110/110 [==============================] - 2s 15ms/step - loss: 11.6111 - val_loss: 11.5089 - _timestamp: 1652167026.0000 - _runtime: 8.0000
Epoch 3/200
110/110 [==============================] - 2s 14ms/step - loss: 11.1191 - val_loss: 11.1132 - _timestamp: 1652167028.0000 - _runtime: 10.0000
Epoch 4/200
110/110 [==============================] - 2s 15ms/step - loss: 10.9282 - val_loss: 10.8568 - _timestamp: 1652167029.0000 - _runtime: 11.0000
Epoch 5/200
110/110 [==============================] - 2s 14ms/step - loss: 10.7831 - val_loss: 10.7055 - _timestamp: 1652167031.0000 - _runtime: 13.0000
Epoch 6/200
110/110 [==============================] - 2s 15ms/step - loss: 10.6735 - val_loss: 10.5934 - _timestamp: 1652167033.0000 - _runtime: 15.0000
Epoch 7/200
110/110 [==============================] - 2s 15ms/step - loss: 10.6454 - val_loss: 10.5573 - _timestamp: 1652167034.0000 - _runtime: 16.0000
Epoch 8/200
110/110 [==============================] - 2s 14ms/step - loss: 10.6315 - val_loss: 10.5467 - _timestamp: 1652167036.0000 - _runtime: 18.0000
Epoch 9/200
110/110 [==============================] - 2s 15ms/step - loss: 10.5501 - val_loss: 10.4951 - _timestamp: 1652167038.0000 - _runtime: 20.0000
Epoch 10/200
110/110 [==============================] - 2s 15ms/step - loss: 10.5675 - val_loss: 10.4910 - _timestamp: 1652167039.0000 - _runtime: 21.0000
Epoch 11/200
110/110 [==============================] - 2s 15ms/step - loss: 10.5254 - val_loss: 10.4617 - _timestamp: 1652167041.0000 - _runtime: 23.0000
Epoch 12/200
110/110 [==============================] - 2s 15ms/step - loss: 10.4644 - val_loss: 10.3816 - _timestamp: 1652167042.0000 - _runtime: 24.0000
Epoch 13/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4367 - val_loss: 10.4678 - _timestamp: 1652167044.0000 - _runtime: 26.0000
Epoch 14/200
110/110 [==============================] - 2s 14ms/step - loss: 10.7384 - val_loss: 10.6641 - _timestamp: 1652167046.0000 - _runtime: 28.0000
Epoch 15/200
110/110 [==============================] - 2s 15ms/step - loss: 10.4439 - val_loss: 10.3792 - _timestamp: 1652167047.0000 - _runtime: 29.0000
Epoch 16/200
110/110 [==============================] - 2s 14ms/step - loss: 10.5756 - val_loss: 10.4983 - _timestamp: 1652167049.0000 - _runtime: 31.0000
Epoch 17/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4023 - val_loss: 10.3201 - _timestamp: 1652167050.0000 - _runtime: 32.0000
Epoch 18/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4583 - val_loss: 12.0006 - _timestamp: 1652167052.0000 - _runtime: 34.0000
Epoch 19/200
110/110 [==============================] - 2s 15ms/step - loss: 10.2287 - val_loss: 10.1579 - _timestamp: 1652167054.0000 - _runtime: 36.0000
Epoch 20/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3791 - val_loss: 10.2925 - _timestamp: 1652167055.0000 - _runtime: 37.0000
Epoch 21/200
110/110 [==============================] - 2s 14ms/step - loss: 10.5200 - val_loss: 10.5860 - _timestamp: 1652167057.0000 - _runtime: 39.0000
Epoch 22/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3118 - val_loss: 10.2465 - _timestamp: 1652167058.0000 - _runtime: 40.0000
Epoch 23/200
110/110 [==============================] - 2s 15ms/step - loss: 10.4238 - val_loss: 10.3531 - _timestamp: 1652167060.0000 - _runtime: 42.0000
Epoch 24/200
110/110 [==============================] - 2s 14ms/step - loss: 10.2827 - val_loss: 10.2123 - _timestamp: 1652167061.0000 - _runtime: 43.0000
Epoch 25/200
110/110 [==============================] - 2s 15ms/step - loss: 10.4806 - val_loss: 10.3942 - _timestamp: 1652167063.0000 - _runtime: 45.0000
Epoch 26/200
110/110 [==============================] - 2s 14ms/step - loss: 10.5641 - val_loss: 10.4969 - _timestamp: 1652167065.0000 - _runtime: 47.0000
Epoch 27/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4280 - val_loss: 10.3630 - _timestamp: 1652167066.0000 - _runtime: 48.0000
Epoch 28/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3512 - val_loss: 10.2951 - _timestamp: 1652167068.0000 - _runtime: 50.0000
Epoch 29/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4896 - val_loss: 10.4605 - _timestamp: 1652167069.0000 - _runtime: 51.0000
Epoch 30/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4123 - val_loss: 10.7811 - _timestamp: 1652167071.0000 - _runtime: 53.0000
Epoch 31/200
110/110 [==============================] - 2s 15ms/step - loss: 10.2073 - val_loss: 10.1378 - _timestamp: 1652167072.0000 - _runtime: 54.0000
Epoch 32/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4834 - val_loss: 10.4182 - _timestamp: 1652167074.0000 - _runtime: 56.0000
Epoch 33/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4376 - val_loss: 10.3718 - _timestamp: 1652167075.0000 - _runtime: 57.0000
Epoch 34/200
110/110 [==============================] - 2s 14ms/step - loss: 10.2974 - val_loss: 10.2176 - _timestamp: 1652167077.0000 - _runtime: 59.0000
Epoch 35/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3657 - val_loss: 10.2935 - _timestamp: 1652167079.0000 - _runtime: 61.0000
Epoch 36/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4721 - val_loss: 10.5378 - _timestamp: 1652167080.0000 - _runtime: 62.0000
Epoch 37/200
110/110 [==============================] - 2s 14ms/step - loss: 10.4711 - val_loss: 10.4013 - _timestamp: 1652167082.0000 - _runtime: 64.0000
Epoch 38/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3025 - val_loss: 10.2267 - _timestamp: 1652167083.0000 - _runtime: 65.0000
Epoch 39/200
110/110 [==============================] - 2s 14ms/step - loss: 10.2287 - val_loss: 10.1462 - _timestamp: 1652167085.0000 - _runtime: 67.0000
Epoch 40/200
110/110 [==============================] - 1s 14ms/step - loss: 10.3998 - val_loss: 10.5981 - _timestamp: 1652167086.0000 - _runtime: 68.0000
Epoch 41/200
110/110 [==============================] - 2s 14ms/step - loss: 10.5802 - val_loss: 10.5623 - _timestamp: 1652167088.0000 - _runtime: 70.0000
Epoch 42/200
110/110 [==============================] - 1s 13ms/step - loss: 10.4031 - val_loss: 10.3396 - _timestamp: 1652167089.0000 - _runtime: 71.0000
Epoch 43/200
110/110 [==============================] - 2s 14ms/step - loss: 10.3441 - val_loss: 10.2763 - _timestamp: 1652167091.0000 - _runtime: 73.0000
Epoch 44/200
 41/110 [==========>...................] - ETA: 0s - loss: 10.3574 - val_loss: 10.3574
Epoch 45/200===========================] - 2s 14ms/step - loss: 10.3441 - val_loss: 10.2763 - _timestamp: 1652167091.0000 - _runtime: 73.0000
 76/110 [===================>..........] - ETA: 0s - loss: 10.2712 - val_loss: 10.2712
110/110 [==============================] - 2s 14ms/step - loss: 10.3558 - val_loss: 10.2737 - _timestamp: 1652167095.0000 - _runtime: 77.0000
Epoch 47/200
  1/110 [..............................] - ETA: 1s - loss: 6.5705 - val_loss: 6.5705
Epoch 48/200===========================] - 2s 14ms/step - loss: 10.3558 - val_loss: 10.2737 - _timestamp: 1652167095.0000 - _runtime: 77.0000
 41/110 [==========>...................] - ETA: 0s - loss: 10.8794 - val_loss: 10.8794
 76/110 [===================>..........] - ETA: 0s - loss: 11.3520 - val_loss: 11.3520.2737 - _timestamp: 1652167095.0000 - _runtime: 77.0000
Epoch 51/200===========================] - 2s 14ms/step - loss: 10.3879 - val_loss: 10.3054 - _timestamp: 1652167100.0000 - _runtime: 82.0000
  1/110 [..............................] - ETA: 1s - loss: 7.9140 - val_loss: 7.9140
 37/110 [=========>....................] - ETA: 0s - loss: 9.4404 - val_loss: 9.4404  .3054 - _timestamp: 1652167100.0000 - _runtime: 82.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3052 - val_loss: 10.2191 - _timestamp: 1652167104.0000 - _runtime: 86.0000
  1/110 [..............................] - ETA: 1s - loss: 5.8692 - val_loss: 5.869210.2191 - _timestamp: 1652167104.0000 - _runtime: 86.0000
110/110 [==============================] - 1s 13ms/step - loss: 10.2405 - val_loss: 10.2856 - _timestamp: 1652167109.0000 - _runtime: 91.0000
Epoch 57/200===========================] - 1s 13ms/step - loss: 10.2405 - val_loss: 10.2856 - _timestamp: 1652167109.0000 - _runtime: 91.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.4007 - val_loss: 10.3317 - _timestamp: 1652167113.0000 - _runtime: 95.0000
Epoch 60/200===========================] - 2s 14ms/step - loss: 10.4007 - val_loss: 10.3317 - _timestamp: 1652167113.0000 - _runtime: 95.0000
 77/110 [====================>.........] - ETA: 0s - loss: 10.6883 - val_loss: 10.6883.3317 - _timestamp: 1652167113.0000 - _runtime: 95.0000
Epoch 63/200===========================] - 1s 13ms/step - loss: 10.2636 - val_loss: 10.2333 - _timestamp: 1652167118.0000 - _runtime: 100.0000
 45/110 [===========>..................] - ETA: 0s - loss: 10.2049 - val_loss: 10.2049.2333 - _timestamp: 1652167118.0000 - _runtime: 100.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3105 - val_loss: 10.2412 - _timestamp: 1652167122.0000 - _runtime: 104.0000
  9/110 [=>............................] - ETA: 1s - loss: 9.9524 - val_loss: 9.9524  .2412 - _timestamp: 1652167122.0000 - _runtime: 104.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2300 - val_loss: 10.1514 - _timestamp: 1652167127.0000 - _runtime: 109.0000
Epoch 69/200===========================] - 2s 14ms/step - loss: 10.2300 - val_loss: 10.1514 - _timestamp: 1652167127.0000 - _runtime: 109.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2711 - val_loss: 10.3536 - _timestamp: 1652167131.0000 - _runtime: 113.0000
Epoch 72/200===========================] - 2s 14ms/step - loss: 10.2711 - val_loss: 10.3536 - _timestamp: 1652167131.0000 - _runtime: 113.0000
 77/110 [====================>.........] - ETA: 0s - loss: 9.8701 - val_loss: 9.8701  .3536 - _timestamp: 1652167131.0000 - _runtime: 113.0000
Epoch 75/200===========================] - 2s 14ms/step - loss: 10.3069 - val_loss: 10.2296 - _timestamp: 1652167136.0000 - _runtime: 118.0000
 41/110 [==========>...................] - ETA: 0s - loss: 11.5968 - val_loss: 11.5968.2296 - _timestamp: 1652167136.0000 - _runtime: 118.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.4167 - val_loss: 10.3551 - _timestamp: 1652167141.0000 - _runtime: 123.0000
Epoch 78/200===========================] - 2s 14ms/step - loss: 10.4167 - val_loss: 10.3551 - _timestamp: 1652167141.0000 - _runtime: 123.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3397 - val_loss: 10.2694 - _timestamp: 1652167145.0000 - _runtime: 127.0000
Epoch 81/200===========================] - 2s 14ms/step - loss: 10.3397 - val_loss: 10.2694 - _timestamp: 1652167145.0000 - _runtime: 127.0000
 71/110 [==================>...........] - ETA: 0s - loss: 10.6861 - val_loss: 10.6861.2694 - _timestamp: 1652167145.0000 - _runtime: 127.0000
Epoch 84/200===========================] - 2s 14ms/step - loss: 10.2326 - val_loss: 10.1602 - _timestamp: 1652167150.0000 - _runtime: 132.0000
 37/110 [=========>....................] - ETA: 0s - loss: 11.1919 - val_loss: 11.1919.1602 - _timestamp: 1652167150.0000 - _runtime: 132.0000
110/110 [==============================] - 1s 14ms/step - loss: 10.1919 - val_loss: 10.3474 - _timestamp: 1652167154.0000 - _runtime: 136.0000
  1/110 [..............................] - ETA: 1s - loss: 5.5307 - val_loss: 5.530710.3474 - _timestamp: 1652167154.0000 - _runtime: 136.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3528 - val_loss: 10.2798 - _timestamp: 1652167159.0000 - _runtime: 141.0000
Epoch 90/200===========================] - 2s 14ms/step - loss: 10.3528 - val_loss: 10.2798 - _timestamp: 1652167159.0000 - _runtime: 141.0000
101/110 [==========================>...] - ETA: 0s - loss: 10.1579 - val_loss: 10.1579.2798 - _timestamp: 1652167159.0000 - _runtime: 141.0000
Epoch 93/200===========================] - 2s 14ms/step - loss: 10.2422 - val_loss: 10.1657 - _timestamp: 1652167164.0000 - _runtime: 146.0000
 65/110 [================>.............] - ETA: 0s - loss: 9.7425 - val_loss: 9.7425  .1657 - _timestamp: 1652167164.0000 - _runtime: 146.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1985 - val_loss: 10.1242 - _timestamp: 1652167168.0000 - _runtime: 150.0000
 25/110 [=====>........................] - ETA: 1s - loss: 10.7386 - val_loss: 10.7386.1242 - _timestamp: 1652167168.0000 - _runtime: 150.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2176 - val_loss: 10.1830 - _timestamp: 1652167173.0000 - _runtime: 155.0000
Epoch 99/200===========================] - 2s 14ms/step - loss: 10.2176 - val_loss: 10.1830 - _timestamp: 1652167173.0000 - _runtime: 155.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3114 - val_loss: 10.4700 - _timestamp: 1652167177.0000 - _runtime: 159.0000
Epoch 102/200==========================] - 2s 14ms/step - loss: 10.3114 - val_loss: 10.4700 - _timestamp: 1652167177.0000 - _runtime: 159.0000
 97/110 [=========================>....] - ETA: 0s - loss: 10.2925 - val_loss: 10.2925.4700 - _timestamp: 1652167177.0000 - _runtime: 159.0000
Epoch 105/200==========================] - 1s 13ms/step - loss: 10.1245 - val_loss: 10.0425 - _timestamp: 1652167182.0000 - _runtime: 164.0000
 57/110 [==============>...............] - ETA: 0s - loss: 9.4497 - val_loss: 9.449710.0425 - _timestamp: 1652167182.0000 - _runtime: 164.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2521 - val_loss: 10.3645 - _timestamp: 1652167186.0000 - _runtime: 168.0000
 13/110 [==>...........................] - ETA: 1s - loss: 7.8383 - val_loss: 7.838310.3645 - _timestamp: 1652167186.0000 - _runtime: 168.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2293 - val_loss: 10.1535 - _timestamp: 1652167191.0000 - _runtime: 173.0000
Epoch 111/200==========================] - 2s 14ms/step - loss: 10.2293 - val_loss: 10.1535 - _timestamp: 1652167191.0000 - _runtime: 173.0000
101/110 [==========================>...] - ETA: 0s - loss: 10.1887 - val_loss: 10.1887.1535 - _timestamp: 1652167191.0000 - _runtime: 173.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3181 - val_loss: 10.2320 - _timestamp: 1652167196.0000 - _runtime: 178.0000
 21/110 [====>.........................] - ETA: 1s - loss: 8.8465 - val_loss: 8.846510.2320 - _timestamp: 1652167196.0000 - _runtime: 178.0000
110/110 [==============================] - 2s 15ms/step - loss: 10.1270 - val_loss: 10.0548 - _timestamp: 1652167201.0000 - _runtime: 183.0000
Epoch 117/200==========================] - 2s 15ms/step - loss: 10.1270 - val_loss: 10.0548 - _timestamp: 1652167201.0000 - _runtime: 183.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1660 - val_loss: 10.1535 - _timestamp: 1652167205.0000 - _runtime: 187.0000
Epoch 120/200==========================] - 2s 14ms/step - loss: 10.1660 - val_loss: 10.1535 - _timestamp: 1652167205.0000 - _runtime: 187.0000
 81/110 [=====================>........] - ETA: 0s - loss: 10.3401 - val_loss: 10.3401.1535 - _timestamp: 1652167205.0000 - _runtime: 187.0000
Epoch 123/200==========================] - 1s 13ms/step - loss: 10.3992 - val_loss: 10.4865 - _timestamp: 1652167210.0000 - _runtime: 192.0000
 45/110 [===========>..................] - ETA: 0s - loss: 9.8045 - val_loss: 9.8045  .4865 - _timestamp: 1652167210.0000 - _runtime: 192.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3100 - val_loss: 11.9814 - _timestamp: 1652167214.0000 - _runtime: 196.0000
  9/110 [=>............................] - ETA: 1s - loss: 6.9497 - val_loss: 6.9497  .9814 - _timestamp: 1652167214.0000 - _runtime: 196.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2031 - val_loss: 10.1317 - _timestamp: 1652167219.0000 - _runtime: 201.0000
Epoch 129/200==========================] - 2s 14ms/step - loss: 10.2031 - val_loss: 10.1317 - _timestamp: 1652167219.0000 - _runtime: 201.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.2347 - val_loss: 10.2323 - _timestamp: 1652167223.0000 - _runtime: 205.0000
Epoch 132/200==========================] - 2s 14ms/step - loss: 10.2347 - val_loss: 10.2323 - _timestamp: 1652167223.0000 - _runtime: 205.0000
 69/110 [=================>............] - ETA: 0s - loss: 9.6916 - val_loss: 9.6916  .2323 - _timestamp: 1652167223.0000 - _runtime: 205.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.1711 - val_loss: 10.1010 - _timestamp: 1652167228.0000 - _runtime: 210.0000
 29/110 [======>.......................] - ETA: 1s - loss: 11.4166 - val_loss: 11.4166.1010 - _timestamp: 1652167228.0000 - _runtime: 210.0000
110/110 [==============================] - 1s 14ms/step - loss: 10.3361 - val_loss: 10.2511 - _timestamp: 1652167233.0000 - _runtime: 215.0000
Epoch 138/200==========================] - 1s 14ms/step - loss: 10.3361 - val_loss: 10.2511 - _timestamp: 1652167233.0000 - _runtime: 215.0000
110/110 [==============================] - 2s 14ms/step - loss: 10.3398 - val_loss: 10.3774 - _timestamp: 1652167237.0000 - _runtime: 219.0000
rmse: 31.256846080087147 requested ('self', 'step_function'), but source function had ()nverts>.predict_function at 0x35c0728b0> and will run it as-is.
rmse: 31.256846080087147 requested ('self', 'step_function'), but source function had ()nverts>.predict_function at 0x35c0728b0> and will run it as-is.
2022-05-10 15:20:39.615269: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.