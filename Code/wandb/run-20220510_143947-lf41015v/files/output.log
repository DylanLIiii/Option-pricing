==================== fold_0 Training ====================
Epoch 1/50
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cfe07040> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cfe07040> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
  6/110 [>.............................] - ETA: 6s - loss: 16.7198 - val_loss: 16.7198
/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.

110/110 [==============================] - ETA: 0s - loss: 12.2503 - val_loss: 12.2127WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cfe07820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cfe07820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 4s 27ms/step - loss: 12.2503 - val_loss: 11.6371 - val_val_loss: 11.5958 - _timestamp: 1652164794.0000 - _runtime: 7.0000
Epoch 2/50
 70/110 [==================>...........] - ETA: 0s - loss: 10.7255 - val_loss: 10.7255
110/110 [==============================] - 2s 21ms/step - loss: 10.8643 - val_loss: 10.7703 - _timestamp: 1652164796.0000 - _runtime: 9.0000
Epoch 3/50
110/110 [==============================] - 2s 21ms/step - loss: 10.6497 - val_loss: 10.5724 - _timestamp: 1652164798.0000 - _runtime: 11.0000
Epoch 4/50
110/110 [==============================] - 2s 20ms/step - loss: 10.6679 - val_loss: 10.5956 - _timestamp: 1652164801.0000 - _runtime: 14.0000
Epoch 5/50
110/110 [==============================] - 2s 21ms/step - loss: 10.4358 - val_loss: 10.3699 - _timestamp: 1652164803.0000 - _runtime: 16.0000
Epoch 6/50
110/110 [==============================] - 2s 19ms/step - loss: 10.4759 - val_loss: 10.4023 - _timestamp: 1652164805.0000 - _runtime: 18.0000
Epoch 7/50
110/110 [==============================] - 2s 19ms/step - loss: 10.5062 - val_loss: 10.5715 - _timestamp: 1652164807.0000 - _runtime: 20.0000
Epoch 8/50
110/110 [==============================] - 2s 19ms/step - loss: 10.5883 - val_loss: 10.5294 - _timestamp: 1652164809.0000 - _runtime: 22.0000
Epoch 9/50
110/110 [==============================] - 2s 19ms/step - loss: 10.4255 - val_loss: 10.3530 - _timestamp: 1652164811.0000 - _runtime: 24.0000
Epoch 10/50
110/110 [==============================] - 2s 20ms/step - loss: 10.5570 - val_loss: 11.9072 - _timestamp: 1652164814.0000 - _runtime: 27.0000
Epoch 11/50

110/110 [==============================] - 2s 20ms/step - loss: 10.2811 - val_loss: 10.1992 - _timestamp: 1652164816.0000 - _runtime: 29.0000
Epoch 12/50
110/110 [==============================] - 2s 20ms/step - loss: 10.4039 - val_loss: 10.3287 - _timestamp: 1652164818.0000 - _runtime: 31.0000
Epoch 13/50
110/110 [==============================] - 2s 19ms/step - loss: 10.4536 - val_loss: 10.4391 - _timestamp: 1652164820.0000 - _runtime: 33.0000
Epoch 14/50
110/110 [==============================] - 2s 20ms/step - loss: 10.2420 - val_loss: 10.1647 - _timestamp: 1652164822.0000 - _runtime: 35.0000
Epoch 15/50
110/110 [==============================] - 2s 19ms/step - loss: 10.2708 - val_loss: 10.1814 - _timestamp: 1652164824.0000 - _runtime: 37.0000
Epoch 16/50
110/110 [==============================] - 2s 19ms/step - loss: 10.4496 - val_loss: 10.7511 - _timestamp: 1652164826.0000 - _runtime: 39.0000
Epoch 17/50
110/110 [==============================] - 2s 20ms/step - loss: 10.3750 - val_loss: 10.4984 - _timestamp: 1652164828.0000 - _runtime: 41.0000
Epoch 18/50
110/110 [==============================] - 2s 19ms/step - loss: 10.4690 - val_loss: 10.3784 - _timestamp: 1652164831.0000 - _runtime: 44.0000
Epoch 19/50
110/110 [==============================] - 2s 20ms/step - loss: 10.2863 - val_loss: 10.3196 - _timestamp: 1652164833.0000 - _runtime: 46.0000
Epoch 20/50
110/110 [==============================] - 2s 21ms/step - loss: 10.4196 - val_loss: 10.3686 - _timestamp: 1652164835.0000 - _runtime: 48.0000
Epoch 21/50
110/110 [==============================] - 2s 19ms/step - loss: 10.2835 - val_loss: 10.2090 - _timestamp: 1652164837.0000 - _runtime: 50.0000
Epoch 22/50
110/110 [==============================] - 2s 19ms/step - loss: 10.4944 - val_loss: 10.4134 - _timestamp: 1652164839.0000 - _runtime: 52.0000
Epoch 23/50
110/110 [==============================] - 2s 20ms/step - loss: 10.4252 - val_loss: 10.8342 - _timestamp: 1652164841.0000 - _runtime: 54.0000
Epoch 24/50
110/110 [==============================] - 2s 19ms/step - loss: 10.2498 - val_loss: 10.1679 - _timestamp: 1652164843.0000 - _runtime: 56.0000
Epoch 25/50
110/110 [==============================] - 2s 19ms/step - loss: 10.3786 - val_loss: 10.5701 - _timestamp: 1652164846.0000 - _runtime: 59.0000
Epoch 26/50
110/110 [==============================] - 2s 19ms/step - loss: 10.2439 - val_loss: 10.1802 - _timestamp: 1652164848.0000 - _runtime: 61.0000
WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a234bc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a234bc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
==================== fold_0 score ====================
rmse: 31.455592518459238
2022-05-10 14:40:48.355231: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.