/Users/dylan/miniforge3/envs/machine_learning/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning:
The `lr` argument is deprecated, use `learning_rate` instead.
2022-05-10 13:48:58.502127: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
==================== fold_0 Training ====================
Epoch 1/50
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fc010ee0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fc010ee0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

 91/110 [=======================>......] - ETA: 0s - loss: 11.7465 - val_loss: 11.7465
110/110 [==============================] - ETA: 0s - loss: 12.1616 - val_loss: 12.0868WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2da8d6af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2da8d6af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
110/110 [==============================] - 4s 28ms/step - loss: 12.1616 - val_loss: 12.8873 - val_val_loss: 12.8553 - _timestamp: 1652161742.0000 - _runtime: 9.0000
Epoch 2/50
110/110 [==============================] - 2s 21ms/step - loss: 11.1537 - val_loss: 11.0578 - _timestamp: 1652161744.0000 - _runtime: 11.0000
Epoch 3/50
110/110 [==============================] - 2s 21ms/step - loss: 10.7369 - val_loss: 10.6743 - _timestamp: 1652161746.0000 - _runtime: 13.0000
Epoch 4/50
110/110 [==============================] - 2s 21ms/step - loss: 10.8413 - val_loss: 10.7684 - _timestamp: 1652161749.0000 - _runtime: 16.0000
Epoch 5/50
110/110 [==============================] - 2s 20ms/step - loss: 10.7374 - val_loss: 10.9914 - _timestamp: 1652161751.0000 - _runtime: 18.0000
Epoch 6/50

110/110 [==============================] - 2s 21ms/step - loss: 10.7472 - val_loss: 10.7496 - _timestamp: 1652161753.0000 - _runtime: 20.0000
Epoch 7/50
110/110 [==============================] - 3s 23ms/step - loss: 10.6961 - val_loss: 10.6139 - _timestamp: 1652161756.0000 - _runtime: 23.0000
Epoch 8/50
110/110 [==============================] - 2s 21ms/step - loss: 10.5691 - val_loss: 10.4906 - _timestamp: 1652161758.0000 - _runtime: 25.0000
Epoch 9/50
110/110 [==============================] - 2s 20ms/step - loss: 10.6170 - val_loss: 10.5456 - _timestamp: 1652161760.0000 - _runtime: 27.0000
Epoch 10/50
110/110 [==============================] - 2s 21ms/step - loss: 10.5279 - val_loss: 10.4556 - _timestamp: 1652161762.0000 - _runtime: 29.0000
Epoch 11/50
110/110 [==============================] - 2s 22ms/step - loss: 10.5124 - val_loss: 10.4294 - _timestamp: 1652161765.0000 - _runtime: 32.0000
Epoch 12/50

110/110 [==============================] - 3s 23ms/step - loss: 10.5042 - val_loss: 10.4197 - _timestamp: 1652161767.0000 - _runtime: 34.0000
Epoch 13/50
110/110 [==============================] - 2s 22ms/step - loss: 10.3777 - val_loss: 10.3690 - _timestamp: 1652161770.0000 - _runtime: 37.0000
Epoch 14/50
110/110 [==============================] - 2s 20ms/step - loss: 10.5258 - val_loss: 10.4561 - _timestamp: 1652161772.0000 - _runtime: 39.0000
Epoch 15/50
110/110 [==============================] - 2s 22ms/step - loss: 10.5263 - val_loss: 10.4532 - _timestamp: 1652161775.0000 - _runtime: 42.0000
Epoch 16/50
110/110 [==============================] - 2s 22ms/step - loss: 10.4168 - val_loss: 10.3770 - _timestamp: 1652161777.0000 - _runtime: 44.0000
Epoch 17/50

110/110 [==============================] - 3s 23ms/step - loss: 10.5384 - val_loss: 10.4780 - _timestamp: 1652161780.0000 - _runtime: 47.0000
Epoch 18/50
110/110 [==============================] - 2s 22ms/step - loss: 10.5298 - val_loss: 10.4499 - _timestamp: 1652161782.0000 - _runtime: 49.0000
Epoch 19/50
110/110 [==============================] - 3s 23ms/step - loss: 10.3950 - val_loss: 10.3080 - _timestamp: 1652161785.0000 - _runtime: 52.0000
Epoch 20/50
110/110 [==============================] - 2s 21ms/step - loss: 10.4010 - val_loss: 10.3395 - _timestamp: 1652161787.0000 - _runtime: 54.0000
Epoch 21/50
110/110 [==============================] - 2s 21ms/step - loss: 10.4539 - val_loss: 10.3805 - _timestamp: 1652161789.0000 - _runtime: 56.0000
Epoch 22/50

110/110 [==============================] - 2s 21ms/step - loss: 10.4166 - val_loss: 10.3420 - _timestamp: 1652161792.0000 - _runtime: 59.0000
Epoch 23/50
110/110 [==============================] - 2s 20ms/step - loss: 10.3879 - val_loss: 10.8517 - _timestamp: 1652161794.0000 - _runtime: 61.0000
Epoch 24/50
110/110 [==============================] - 2s 21ms/step - loss: 10.6244 - val_loss: 10.6678 - _timestamp: 1652161796.0000 - _runtime: 63.0000
Epoch 25/50
110/110 [==============================] - 3s 24ms/step - loss: 10.4190 - val_loss: 10.3494 - _timestamp: 1652161799.0000 - _runtime: 66.0000
Epoch 26/50
110/110 [==============================] - 2s 19ms/step - loss: 10.4033 - val_loss: 10.4302 - _timestamp: 1652161801.0000 - _runtime: 68.0000
Epoch 27/50
110/110 [==============================] - 2s 20ms/step - loss: 10.4413 - val_loss: 10.4015 - _timestamp: 1652161803.0000 - _runtime: 70.0000
Epoch 28/50

110/110 [==============================] - 2s 20ms/step - loss: 10.4957 - val_loss: 10.4167 - _timestamp: 1652161805.0000 - _runtime: 72.0000
Epoch 29/50
110/110 [==============================] - 2s 21ms/step - loss: 10.4606 - val_loss: 10.5650 - _timestamp: 1652161807.0000 - _runtime: 74.0000
Epoch 30/50
110/110 [==============================] - 2s 20ms/step - loss: 10.5161 - val_loss: 10.4357 - _timestamp: 1652161810.0000 - _runtime: 77.0000
Epoch 31/50
110/110 [==============================] - 2s 22ms/step - loss: 10.2437 - val_loss: 10.2047 - _timestamp: 1652161812.0000 - _runtime: 79.0000
Epoch 32/50
110/110 [==============================] - 2s 20ms/step - loss: 10.4575 - val_loss: 10.4469 - _timestamp: 1652161814.0000 - _runtime: 81.0000
Epoch 33/50
110/110 [==============================] - 2s 21ms/step - loss: 10.6031 - val_loss: 10.5479 - _timestamp: 1652161817.0000 - _runtime: 84.0000
Epoch 34/50
110/110 [==============================] - 2s 20ms/step - loss: 10.4483 - val_loss: 10.3669 - _timestamp: 1652161819.0000 - _runtime: 86.0000
Epoch 35/50
110/110 [==============================] - 2s 20ms/step - loss: 10.5455 - val_loss: 10.4741 - _timestamp: 1652161821.0000 - _runtime: 88.0000
Epoch 36/50
110/110 [==============================] - 2s 20ms/step - loss: 10.3448 - val_loss: 10.2867 - _timestamp: 1652161823.0000 - _runtime: 90.0000
Epoch 37/50

110/110 [==============================] - 2s 20ms/step - loss: 10.3309 - val_loss: 10.2701 - _timestamp: 1652161825.0000 - _runtime: 92.0000
Epoch 38/50
110/110 [==============================] - 2s 21ms/step - loss: 10.2921 - val_loss: 10.2374 - _timestamp: 1652161828.0000 - _runtime: 95.0000
Epoch 39/50
110/110 [==============================] - 2s 20ms/step - loss: 10.4144 - val_loss: 10.3456 - _timestamp: 1652161830.0000 - _runtime: 97.0000
Epoch 40/50
110/110 [==============================] - 2s 20ms/step - loss: 10.4197 - val_loss: 10.3512 - _timestamp: 1652161832.0000 - _runtime: 99.0000
Epoch 41/50
110/110 [==============================] - 2s 20ms/step - loss: 10.2823 - val_loss: 10.2773 - _timestamp: 1652161834.0000 - _runtime: 101.0000
Epoch 42/50
Epoch 43/50============================] - 2s 21ms/step - loss: 10.4587 - val_loss: 10.3865 - _timestamp: 1652161837.0000 - _runtime: 104.0000
Epoch 43/50============================] - 2s 21ms/step - loss: 10.4587 - val_loss: 10.3865 - _timestamp: 1652161837.0000 - _runtime: 104.0000
 35/110 [========>.....................] - ETA: 1s - loss: 9.3939 - val_loss: 9.3939
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2daa0fb80> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2daa0fb80> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
==================== fold_0 score ====================
rmse: 31.26863706287804